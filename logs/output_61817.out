Parameters for testing:
Batch size: 32
Tensor size: (3,8,112,112)
Skip Length: 2
Precrop: True
Total Epochs: 1
FullNetwork(
  (vgg): VGG(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU(inplace)
      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU(inplace)
      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (6): ReLU(inplace)
      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (8): ReLU(inplace)
      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (11): ReLU(inplace)
      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (13): ReLU(inplace)
      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (15): ReLU(inplace)
      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (18): ReLU(inplace)
      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (20): ReLU(inplace)
      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (22): ReLU(inplace)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (25): ReLU(inplace)
      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (27): ReLU(inplace)
      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (29): ReLU(inplace)
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
    (classifier): Sequential(
      (0): Linear(in_features=25088, out_features=4096, bias=True)
      (1): ReLU(inplace)
      (2): Dropout(p=0.5)
      (3): Linear(in_features=4096, out_features=4096, bias=True)
      (4): ReLU(inplace)
      (5): Dropout(p=0.5)
      (6): Linear(in_features=4096, out_features=1000, bias=True)
    )
  )
  (i3d): InceptionI3d(
    (logits): Unit3D(
      (conv3d): Conv3d(1024, 157, kernel_size=[1, 1, 1], stride=(1, 1, 1))
    )
    (Conv3d_1a_7x7): Unit3D(
      (conv3d): Conv3d(3, 64, kernel_size=[7, 7, 7], stride=(2, 2, 2), bias=False)
      (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
    )
    (MaxPool3d_2a_3x3): MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)
    (Conv3d_2b_1x1): Unit3D(
      (conv3d): Conv3d(64, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
      (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
    )
    (Conv3d_2c_3x3): Unit3D(
      (conv3d): Conv3d(64, 192, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
      (bn): BatchNorm3d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
    )
    (MaxPool3d_3a_3x3): MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)
    (Mixed_3b): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(192, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(192, 96, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(96, 128, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(192, 16, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(16, 32, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(192, 32, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (Mixed_3c): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(256, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(256, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(128, 192, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(256, 32, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(32, 96, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(256, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (MaxPool3d_4a_3x3): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)
    (Mixed_4b): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(480, 192, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(480, 96, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(96, 208, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(208, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(480, 16, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(16, 48, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(480, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (Mixed_4c): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(512, 160, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(512, 112, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(112, 224, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(224, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(512, 24, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(24, 64, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(512, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (Mixed_4d): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(512, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(512, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(128, 256, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(512, 24, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(24, 64, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(512, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (Mixed_4e): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(512, 112, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(512, 144, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(144, 288, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(512, 32, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(32, 64, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(512, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (Mixed_4f): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(528, 256, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(528, 160, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(160, 320, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(320, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(528, 32, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(32, 128, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(528, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (MaxPool3d_5a_1x1): MaxPool3dSamePadding(kernel_size=[2, 2, 2], stride=(2, 1, 1), padding=0, dilation=1, ceil_mode=False)
    (Mixed_5b): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(832, 256, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(832, 160, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(160, 320, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(320, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(832, 32, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(32, 128, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(832, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (Mixed_5c): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(832, 384, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(832, 192, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(192, 384, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(832, 48, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(48, 128, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(832, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
  )
  (gen): Generator(
    (conv2d): Conv2d(1536, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (upsamp1): Upsample(scale_factor=2, mode=nearest)
    (conv3d_1a): Conv3d(1024, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (conv3d_1b): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (upsamp2): Upsample(scale_factor=2, mode=nearest)
    (conv3d_2a): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (conv3d_2b): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (upsamp3): Upsample(scale_factor=2, mode=nearest)
    (conv3d_3a): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (conv3d_3b): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (upsamp4): Upsample(scale_factor=2, mode=nearest)
    (conv3d_4): Conv3d(64, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
  )
)
	Batch 1/173 Loss:0.14760 con:0.00004 recon1:0.07153 recon2:0.07603
	Batch 2/173 Loss:0.14143 con:0.00004 recon1:0.07073 recon2:0.07067
	Batch 3/173 Loss:0.14741 con:0.00004 recon1:0.07475 recon2:0.07261
	Batch 4/173 Loss:0.14347 con:0.00004 recon1:0.07135 recon2:0.07208
	Batch 5/173 Loss:0.14814 con:0.00004 recon1:0.07396 recon2:0.07414
	Batch 6/173 Loss:0.14660 con:0.00004 recon1:0.07345 recon2:0.07312
	Batch 7/173 Loss:0.14542 con:0.00004 recon1:0.07269 recon2:0.07269
	Batch 8/173 Loss:0.14529 con:0.00004 recon1:0.07364 recon2:0.07160
	Batch 9/173 Loss:0.14860 con:0.00004 recon1:0.07421 recon2:0.07435
	Batch 10/173 Loss:0.15016 con:0.00004 recon1:0.07469 recon2:0.07543
	Batch 11/173 Loss:0.14780 con:0.00004 recon1:0.07327 recon2:0.07449
	Batch 12/173 Loss:0.14974 con:0.00004 recon1:0.07596 recon2:0.07374
	Batch 13/173 Loss:0.14579 con:0.00004 recon1:0.07325 recon2:0.07249
	Batch 14/173 Loss:0.14398 con:0.00004 recon1:0.07140 recon2:0.07255
	Batch 15/173 Loss:0.14838 con:0.00004 recon1:0.07327 recon2:0.07507
	Batch 16/173 Loss:0.14657 con:0.00004 recon1:0.07189 recon2:0.07463
	Batch 17/173 Loss:0.15045 con:0.00004 recon1:0.07551 recon2:0.07490
	Batch 18/173 Loss:0.14376 con:0.00004 recon1:0.07117 recon2:0.07255
	Batch 19/173 Loss:0.14985 con:0.00004 recon1:0.07576 recon2:0.07405
	Batch 20/173 Loss:0.14629 con:0.00004 recon1:0.07456 recon2:0.07169
	Batch 21/173 Loss:0.14513 con:0.00004 recon1:0.07381 recon2:0.07127
	Batch 22/173 Loss:0.14907 con:0.00004 recon1:0.07328 recon2:0.07575
	Batch 23/173 Loss:0.14897 con:0.00004 recon1:0.07509 recon2:0.07383
	Batch 24/173 Loss:0.14596 con:0.00004 recon1:0.07309 recon2:0.07283
	Batch 25/173 Loss:0.14837 con:0.00004 recon1:0.07231 recon2:0.07602
	Batch 26/173 Loss:0.14725 con:0.00004 recon1:0.07447 recon2:0.07274
	Batch 27/173 Loss:0.14068 con:0.00004 recon1:0.06875 recon2:0.07189
	Batch 28/173 Loss:0.14517 con:0.00004 recon1:0.07409 recon2:0.07104
	Batch 29/173 Loss:0.14511 con:0.00004 recon1:0.07178 recon2:0.07329
	Batch 30/173 Loss:0.15118 con:0.00004 recon1:0.07474 recon2:0.07640
	Batch 31/173 Loss:0.14535 con:0.00005 recon1:0.07350 recon2:0.07181
	Batch 32/173 Loss:0.14615 con:0.00004 recon1:0.07229 recon2:0.07382
	Batch 33/173 Loss:0.14852 con:0.00004 recon1:0.07425 recon2:0.07424
	Batch 34/173 Loss:0.14763 con:0.00004 recon1:0.07376 recon2:0.07383
	Batch 35/173 Loss:0.14052 con:0.00004 recon1:0.07004 recon2:0.07044
	Batch 36/173 Loss:0.14705 con:0.00004 recon1:0.07411 recon2:0.07290
	Batch 37/173 Loss:0.14650 con:0.00004 recon1:0.07152 recon2:0.07494
	Batch 38/173 Loss:0.15092 con:0.00004 recon1:0.07499 recon2:0.07589
	Batch 39/173 Loss:0.14586 con:0.00004 recon1:0.07332 recon2:0.07250
	Batch 40/173 Loss:0.14705 con:0.00004 recon1:0.07299 recon2:0.07402
	Batch 41/173 Loss:0.14645 con:0.00004 recon1:0.07207 recon2:0.07434
	Batch 42/173 Loss:0.14302 con:0.00004 recon1:0.07292 recon2:0.07005
	Batch 43/173 Loss:0.14550 con:0.00004 recon1:0.07248 recon2:0.07298
	Batch 44/173 Loss:0.14914 con:0.00004 recon1:0.07650 recon2:0.07260
	Batch 45/173 Loss:0.14500 con:0.00005 recon1:0.07336 recon2:0.07159
	Batch 46/173 Loss:0.14481 con:0.00004 recon1:0.07185 recon2:0.07292
	Batch 47/173 Loss:0.14996 con:0.00004 recon1:0.07545 recon2:0.07446
	Batch 48/173 Loss:0.14915 con:0.00004 recon1:0.07335 recon2:0.07576
	Batch 49/173 Loss:0.15251 con:0.00004 recon1:0.07742 recon2:0.07505
	Batch 50/173 Loss:0.14119 con:0.00004 recon1:0.07017 recon2:0.07098
	Batch 51/173 Loss:0.15125 con:0.00004 recon1:0.07702 recon2:0.07419
	Batch 52/173 Loss:0.14895 con:0.00004 recon1:0.07466 recon2:0.07424
	Batch 53/173 Loss:0.14846 con:0.00004 recon1:0.07479 recon2:0.07363
	Batch 54/173 Loss:0.14582 con:0.00004 recon1:0.07327 recon2:0.07250
	Batch 55/173 Loss:0.14383 con:0.00004 recon1:0.07058 recon2:0.07321
	Batch 56/173 Loss:0.15268 con:0.00004 recon1:0.07629 recon2:0.07635
	Batch 57/173 Loss:0.14636 con:0.00004 recon1:0.07345 recon2:0.07288
	Batch 58/173 Loss:0.14071 con:0.00005 recon1:0.07037 recon2:0.07029
	Batch 59/173 Loss:0.14576 con:0.00004 recon1:0.07298 recon2:0.07275
	Batch 60/173 Loss:0.14315 con:0.00004 recon1:0.07325 recon2:0.06987
	Batch 61/173 Loss:0.14537 con:0.00004 recon1:0.07122 recon2:0.07410
	Batch 62/173 Loss:0.14590 con:0.00004 recon1:0.07283 recon2:0.07303
	Batch 63/173 Loss:0.14342 con:0.00004 recon1:0.07117 recon2:0.07221
	Batch 64/173 Loss:0.14675 con:0.00004 recon1:0.07235 recon2:0.07437
	Batch 65/173 Loss:0.14182 con:0.00004 recon1:0.06936 recon2:0.07242
	Batch 66/173 Loss:0.14416 con:0.00004 recon1:0.07258 recon2:0.07155
	Batch 67/173 Loss:0.14565 con:0.00004 recon1:0.07386 recon2:0.07175
	Batch 68/173 Loss:0.14407 con:0.00004 recon1:0.07379 recon2:0.07024
	Batch 69/173 Loss:0.14329 con:0.00004 recon1:0.07152 recon2:0.07172
	Batch 70/173 Loss:0.14406 con:0.00004 recon1:0.07145 recon2:0.07256
	Batch 71/173 Loss:0.14686 con:0.00004 recon1:0.07270 recon2:0.07413
	Batch 72/173 Loss:0.14622 con:0.00004 recon1:0.07231 recon2:0.07387
	Batch 73/173 Loss:0.14871 con:0.00005 recon1:0.07418 recon2:0.07448
	Batch 74/173 Loss:0.14609 con:0.00004 recon1:0.07506 recon2:0.07099
	Batch 75/173 Loss:0.14542 con:0.00004 recon1:0.07167 recon2:0.07371
	Batch 76/173 Loss:0.14348 con:0.00004 recon1:0.07112 recon2:0.07232
	Batch 77/173 Loss:0.14950 con:0.00004 recon1:0.07606 recon2:0.07340
	Batch 78/173 Loss:0.14740 con:0.00004 recon1:0.07205 recon2:0.07532
	Batch 79/173 Loss:0.14716 con:0.00004 recon1:0.07139 recon2:0.07573
	Batch 80/173 Loss:0.14853 con:0.00004 recon1:0.07428 recon2:0.07421
	Batch 81/173 Loss:0.15127 con:0.00004 recon1:0.07463 recon2:0.07659
	Batch 82/173 Loss:0.14368 con:0.00004 recon1:0.07071 recon2:0.07293
	Batch 83/173 Loss:0.14518 con:0.00004 recon1:0.07362 recon2:0.07152
	Batch 84/173 Loss:0.14404 con:0.00005 recon1:0.06967 recon2:0.07432
	Batch 85/173 Loss:0.14712 con:0.00004 recon1:0.07381 recon2:0.07327
	Batch 86/173 Loss:0.14665 con:0.00004 recon1:0.07159 recon2:0.07503
	Batch 87/173 Loss:0.14919 con:0.00004 recon1:0.07454 recon2:0.07461
	Batch 88/173 Loss:0.14937 con:0.00004 recon1:0.07297 recon2:0.07637
	Batch 89/173 Loss:0.15088 con:0.00004 recon1:0.07683 recon2:0.07401
	Batch 90/173 Loss:0.14814 con:0.00004 recon1:0.07331 recon2:0.07479
	Batch 91/173 Loss:0.14114 con:0.00004 recon1:0.06959 recon2:0.07151
	Batch 92/173 Loss:0.14552 con:0.00004 recon1:0.07265 recon2:0.07283
	Batch 93/173 Loss:0.14914 con:0.00004 recon1:0.07403 recon2:0.07508
	Batch 94/173 Loss:0.15693 con:0.00005 recon1:0.07703 recon2:0.07985
	Batch 95/173 Loss:0.14817 con:0.00004 recon1:0.07400 recon2:0.07413
	Batch 96/173 Loss:0.14140 con:0.00004 recon1:0.07006 recon2:0.07131
	Batch 97/173 Loss:0.15045 con:0.00004 recon1:0.07658 recon2:0.07383
	Batch 98/173 Loss:0.15205 con:0.00004 recon1:0.07655 recon2:0.07546
	Batch 99/173 Loss:0.14403 con:0.00004 recon1:0.07156 recon2:0.07242
	Batch 100/173 Loss:0.14571 con:0.00004 recon1:0.07387 recon2:0.07179
	Batch 101/173 Loss:0.14585 con:0.00004 recon1:0.07455 recon2:0.07126
	Batch 102/173 Loss:0.14200 con:0.00004 recon1:0.07016 recon2:0.07180
	Batch 103/173 Loss:0.14463 con:0.00005 recon1:0.07254 recon2:0.07205
	Batch 104/173 Loss:0.14519 con:0.00004 recon1:0.07421 recon2:0.07094
	Batch 105/173 Loss:0.14325 con:0.00004 recon1:0.07003 recon2:0.07317
	Batch 106/173 Loss:0.14547 con:0.00004 recon1:0.07414 recon2:0.07129
	Batch 107/173 Loss:0.14189 con:0.00004 recon1:0.07323 recon2:0.06862
	Batch 108/173 Loss:0.14285 con:0.00004 recon1:0.07132 recon2:0.07149
	Batch 109/173 Loss:0.14378 con:0.00004 recon1:0.07135 recon2:0.07239
	Batch 110/173 Loss:0.13884 con:0.00004 recon1:0.06783 recon2:0.07097
	Batch 111/173 Loss:0.14904 con:0.00004 recon1:0.07378 recon2:0.07522
	Batch 112/173 Loss:0.14508 con:0.00004 recon1:0.07252 recon2:0.07252
	Batch 113/173 Loss:0.14529 con:0.00004 recon1:0.07256 recon2:0.07269
	Batch 114/173 Loss:0.14983 con:0.00004 recon1:0.07505 recon2:0.07474
	Batch 115/173 Loss:0.14904 con:0.00004 recon1:0.07302 recon2:0.07598
	Batch 116/173 Loss:0.15015 con:0.00004 recon1:0.07490 recon2:0.07521
	Batch 117/173 Loss:0.14678 con:0.00004 recon1:0.07197 recon2:0.07477
	Batch 118/173 Loss:0.15023 con:0.00004 recon1:0.07517 recon2:0.07502
	Batch 119/173 Loss:0.14643 con:0.00004 recon1:0.07396 recon2:0.07244
	Batch 120/173 Loss:0.15032 con:0.00004 recon1:0.07616 recon2:0.07411
	Batch 121/173 Loss:0.14632 con:0.00004 recon1:0.07297 recon2:0.07331
	Batch 122/173 Loss:0.14937 con:0.00004 recon1:0.07581 recon2:0.07352
	Batch 123/173 Loss:0.14344 con:0.00004 recon1:0.07030 recon2:0.07310
	Batch 124/173 Loss:0.14756 con:0.00004 recon1:0.07370 recon2:0.07382
	Batch 125/173 Loss:0.14399 con:0.00004 recon1:0.07176 recon2:0.07219
	Batch 126/173 Loss:0.14504 con:0.00004 recon1:0.07264 recon2:0.07235
	Batch 127/173 Loss:0.14796 con:0.00004 recon1:0.07234 recon2:0.07558
	Batch 128/173 Loss:0.14658 con:0.00004 recon1:0.07477 recon2:0.07178
	Batch 129/173 Loss:0.14876 con:0.00004 recon1:0.07504 recon2:0.07368
	Batch 130/173 Loss:0.14960 con:0.00004 recon1:0.07221 recon2:0.07736
	Batch 131/173 Loss:0.14980 con:0.00004 recon1:0.07587 recon2:0.07389
	Batch 132/173 Loss:0.14721 con:0.00004 recon1:0.07408 recon2:0.07309
	Batch 133/173 Loss:0.14455 con:0.00004 recon1:0.07327 recon2:0.07124
	Batch 134/173 Loss:0.14780 con:0.00004 recon1:0.07394 recon2:0.07382
	Batch 135/173 Loss:0.14630 con:0.00004 recon1:0.07277 recon2:0.07349
	Batch 136/173 Loss:0.14151 con:0.00004 recon1:0.06999 recon2:0.07147
	Batch 137/173 Loss:0.14927 con:0.00004 recon1:0.07470 recon2:0.07454
	Batch 138/173 Loss:0.14228 con:0.00004 recon1:0.07020 recon2:0.07204
	Batch 139/173 Loss:0.14555 con:0.00004 recon1:0.07383 recon2:0.07168
	Batch 140/173 Loss:0.14948 con:0.00004 recon1:0.07363 recon2:0.07581
	Batch 141/173 Loss:0.14937 con:0.00004 recon1:0.07342 recon2:0.07591
	Batch 142/173 Loss:0.14754 con:0.00004 recon1:0.07434 recon2:0.07317
	Batch 143/173 Loss:0.14992 con:0.00004 recon1:0.07445 recon2:0.07543
	Batch 144/173 Loss:0.14721 con:0.00004 recon1:0.07325 recon2:0.07392
	Batch 145/173 Loss:0.14154 con:0.00004 recon1:0.06998 recon2:0.07152
	Batch 146/173 Loss:0.14358 con:0.00004 recon1:0.07254 recon2:0.07100
	Batch 147/173 Loss:0.14822 con:0.00004 recon1:0.07318 recon2:0.07500
	Batch 148/173 Loss:0.14213 con:0.00004 recon1:0.07209 recon2:0.07000
	Batch 149/173 Loss:0.14592 con:0.00004 recon1:0.06985 recon2:0.07602
	Batch 150/173 Loss:0.14809 con:0.00004 recon1:0.07212 recon2:0.07593
	Batch 151/173 Loss:0.14275 con:0.00004 recon1:0.07213 recon2:0.07058
	Batch 152/173 Loss:0.15007 con:0.00004 recon1:0.07484 recon2:0.07518
	Batch 153/173 Loss:0.14487 con:0.00004 recon1:0.07083 recon2:0.07401
	Batch 154/173 Loss:0.14336 con:0.00004 recon1:0.07153 recon2:0.07178
	Batch 155/173 Loss:0.15051 con:0.00004 recon1:0.07607 recon2:0.07441
	Batch 156/173 Loss:0.14721 con:0.00004 recon1:0.07508 recon2:0.07209
	Batch 157/173 Loss:0.14582 con:0.00004 recon1:0.07363 recon2:0.07215
	Batch 158/173 Loss:0.14654 con:0.00004 recon1:0.07364 recon2:0.07286
	Batch 159/173 Loss:0.14863 con:0.00004 recon1:0.07533 recon2:0.07326
	Batch 160/173 Loss:0.14151 con:0.00004 recon1:0.07046 recon2:0.07101
	Batch 161/173 Loss:0.14601 con:0.00004 recon1:0.07031 recon2:0.07566
	Batch 162/173 Loss:0.14432 con:0.00004 recon1:0.07137 recon2:0.07291
	Batch 163/173 Loss:0.14424 con:0.00004 recon1:0.07285 recon2:0.07135
	Batch 164/173 Loss:0.14386 con:0.00004 recon1:0.07436 recon2:0.06947
	Batch 165/173 Loss:0.14748 con:0.00004 recon1:0.07242 recon2:0.07501
	Batch 166/173 Loss:0.14396 con:0.00004 recon1:0.07190 recon2:0.07202
	Batch 167/173 Loss:0.14480 con:0.00005 recon1:0.07333 recon2:0.07142
	Batch 168/173 Loss:0.14258 con:0.00004 recon1:0.07091 recon2:0.07163
	Batch 169/173 Loss:0.14485 con:0.00004 recon1:0.07188 recon2:0.07292
	Batch 170/173 Loss:0.14629 con:0.00004 recon1:0.07204 recon2:0.07422
	Batch 171/173 Loss:0.14188 con:0.00004 recon1:0.07173 recon2:0.07011
	Batch 172/173 Loss:0.14828 con:0.00004 recon1:0.07281 recon2:0.07543
	Batch 173/173 Loss:0.14719 con:0.00004 recon1:0.07346 recon2:0.07369
Testing Epoch 1/1 Loss:0.14639 con:0.00004 recon1:0.07307 recon2:0.07328
Time: 691.95356798172
