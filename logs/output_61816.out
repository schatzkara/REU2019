Parameters for testing:
Batch size: 32
Tensor size: (3,8,112,112)
Skip Length: 2
Precrop: True
Total Epochs: 1
FullNetwork(
  (vgg): VGG(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU(inplace)
      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU(inplace)
      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (6): ReLU(inplace)
      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (8): ReLU(inplace)
      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (11): ReLU(inplace)
      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (13): ReLU(inplace)
      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (15): ReLU(inplace)
      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (18): ReLU(inplace)
      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (20): ReLU(inplace)
      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (22): ReLU(inplace)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (25): ReLU(inplace)
      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (27): ReLU(inplace)
      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (29): ReLU(inplace)
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
    (classifier): Sequential(
      (0): Linear(in_features=25088, out_features=4096, bias=True)
      (1): ReLU(inplace)
      (2): Dropout(p=0.5)
      (3): Linear(in_features=4096, out_features=4096, bias=True)
      (4): ReLU(inplace)
      (5): Dropout(p=0.5)
      (6): Linear(in_features=4096, out_features=1000, bias=True)
    )
  )
  (i3d): InceptionI3d(
    (logits): Unit3D(
      (conv3d): Conv3d(1024, 157, kernel_size=[1, 1, 1], stride=(1, 1, 1))
    )
    (Conv3d_1a_7x7): Unit3D(
      (conv3d): Conv3d(3, 64, kernel_size=[7, 7, 7], stride=(2, 2, 2), bias=False)
      (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
    )
    (MaxPool3d_2a_3x3): MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)
    (Conv3d_2b_1x1): Unit3D(
      (conv3d): Conv3d(64, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
      (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
    )
    (Conv3d_2c_3x3): Unit3D(
      (conv3d): Conv3d(64, 192, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
      (bn): BatchNorm3d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
    )
    (MaxPool3d_3a_3x3): MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)
    (Mixed_3b): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(192, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(192, 96, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(96, 128, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(192, 16, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(16, 32, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(192, 32, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (Mixed_3c): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(256, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(256, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(128, 192, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(256, 32, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(32, 96, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(256, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (MaxPool3d_4a_3x3): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)
    (Mixed_4b): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(480, 192, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(480, 96, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(96, 208, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(208, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(480, 16, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(16, 48, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(480, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (Mixed_4c): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(512, 160, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(512, 112, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(112, 224, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(224, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(512, 24, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(24, 64, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(512, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (Mixed_4d): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(512, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(512, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(128, 256, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(512, 24, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(24, 64, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(512, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (Mixed_4e): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(512, 112, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(512, 144, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(144, 288, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(512, 32, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(32, 64, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(512, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (Mixed_4f): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(528, 256, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(528, 160, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(160, 320, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(320, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(528, 32, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(32, 128, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(528, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (MaxPool3d_5a_1x1): MaxPool3dSamePadding(kernel_size=[2, 2, 2], stride=(2, 1, 1), padding=0, dilation=1, ceil_mode=False)
    (Mixed_5b): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(832, 256, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(832, 160, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(160, 320, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(320, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(832, 32, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(32, 128, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(832, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (Mixed_5c): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(832, 384, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(832, 192, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(192, 384, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(832, 48, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(48, 128, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(832, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
  )
  (gen): Generator(
    (conv2d): Conv2d(1536, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (upsamp1): Upsample(scale_factor=2, mode=nearest)
    (conv3d_1a): Conv3d(1024, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (conv3d_1b): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (upsamp2): Upsample(scale_factor=2, mode=nearest)
    (conv3d_2a): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (conv3d_2b): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (upsamp3): Upsample(scale_factor=2, mode=nearest)
    (conv3d_3a): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (conv3d_3b): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (upsamp4): Upsample(scale_factor=2, mode=nearest)
    (conv3d_4): Conv3d(64, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
  )
)
	Batch 1/173 Loss:0.04875 con:0.00016 recon1:0.02428 recon2:0.02430
	Batch 2/173 Loss:0.04986 con:0.00016 recon1:0.02627 recon2:0.02343
	Batch 3/173 Loss:0.04818 con:0.00017 recon1:0.02367 recon2:0.02433
	Batch 4/173 Loss:0.04738 con:0.00014 recon1:0.02382 recon2:0.02342
	Batch 5/173 Loss:0.05197 con:0.00016 recon1:0.02547 recon2:0.02634
	Batch 6/173 Loss:0.04993 con:0.00016 recon1:0.02572 recon2:0.02406
	Batch 7/173 Loss:0.04685 con:0.00014 recon1:0.02273 recon2:0.02397
	Batch 8/173 Loss:0.05244 con:0.00015 recon1:0.02665 recon2:0.02564
	Batch 9/173 Loss:0.05270 con:0.00014 recon1:0.02602 recon2:0.02654
	Batch 10/173 Loss:0.06229 con:0.00014 recon1:0.03031 recon2:0.03184
	Batch 11/173 Loss:0.06033 con:0.00014 recon1:0.02983 recon2:0.03036
	Batch 12/173 Loss:0.05467 con:0.00015 recon1:0.02758 recon2:0.02694
	Batch 13/173 Loss:0.04971 con:0.00016 recon1:0.02508 recon2:0.02447
	Batch 14/173 Loss:0.05063 con:0.00015 recon1:0.02592 recon2:0.02456
	Batch 15/173 Loss:0.05190 con:0.00015 recon1:0.02748 recon2:0.02427
	Batch 16/173 Loss:0.05418 con:0.00015 recon1:0.02671 recon2:0.02732
	Batch 17/173 Loss:0.05138 con:0.00015 recon1:0.02666 recon2:0.02457
	Batch 18/173 Loss:0.04855 con:0.00014 recon1:0.02620 recon2:0.02221
	Batch 19/173 Loss:0.05390 con:0.00016 recon1:0.02584 recon2:0.02790
	Batch 20/173 Loss:0.05193 con:0.00015 recon1:0.02566 recon2:0.02613
	Batch 21/173 Loss:0.05212 con:0.00016 recon1:0.02570 recon2:0.02626
	Batch 22/173 Loss:0.05109 con:0.00016 recon1:0.02511 recon2:0.02582
	Batch 23/173 Loss:0.05150 con:0.00015 recon1:0.02562 recon2:0.02573
	Batch 24/173 Loss:0.05119 con:0.00016 recon1:0.02513 recon2:0.02591
	Batch 25/173 Loss:0.04941 con:0.00015 recon1:0.02428 recon2:0.02498
	Batch 26/173 Loss:0.05208 con:0.00015 recon1:0.02525 recon2:0.02668
	Batch 27/173 Loss:0.04839 con:0.00015 recon1:0.02362 recon2:0.02462
	Batch 28/173 Loss:0.05129 con:0.00016 recon1:0.02643 recon2:0.02470
	Batch 29/173 Loss:0.04601 con:0.00016 recon1:0.02249 recon2:0.02336
	Batch 30/173 Loss:0.05125 con:0.00015 recon1:0.02494 recon2:0.02615
	Batch 31/173 Loss:0.05631 con:0.00016 recon1:0.02825 recon2:0.02790
	Batch 32/173 Loss:0.04929 con:0.00015 recon1:0.02498 recon2:0.02416
	Batch 33/173 Loss:0.04913 con:0.00016 recon1:0.02353 recon2:0.02544
	Batch 34/173 Loss:0.04637 con:0.00015 recon1:0.02325 recon2:0.02297
	Batch 35/173 Loss:0.05305 con:0.00015 recon1:0.02804 recon2:0.02486
	Batch 36/173 Loss:0.04707 con:0.00015 recon1:0.02277 recon2:0.02415
	Batch 37/173 Loss:0.04833 con:0.00015 recon1:0.02392 recon2:0.02426
	Batch 38/173 Loss:0.05608 con:0.00015 recon1:0.02731 recon2:0.02862
	Batch 39/173 Loss:0.06430 con:0.00016 recon1:0.03160 recon2:0.03254
	Batch 40/173 Loss:0.06232 con:0.00015 recon1:0.03076 recon2:0.03141
	Batch 41/173 Loss:0.05220 con:0.00013 recon1:0.02589 recon2:0.02618
	Batch 42/173 Loss:0.05460 con:0.00016 recon1:0.02715 recon2:0.02728
	Batch 43/173 Loss:0.05094 con:0.00014 recon1:0.02640 recon2:0.02440
	Batch 44/173 Loss:0.04937 con:0.00014 recon1:0.02478 recon2:0.02444
	Batch 45/173 Loss:0.05351 con:0.00018 recon1:0.02519 recon2:0.02814
	Batch 46/173 Loss:0.05016 con:0.00016 recon1:0.02498 recon2:0.02502
	Batch 47/173 Loss:0.04791 con:0.00017 recon1:0.02300 recon2:0.02475
	Batch 48/173 Loss:0.05089 con:0.00016 recon1:0.02568 recon2:0.02505
	Batch 49/173 Loss:0.05079 con:0.00016 recon1:0.02556 recon2:0.02507
	Batch 50/173 Loss:0.06265 con:0.00015 recon1:0.03191 recon2:0.03059
	Batch 51/173 Loss:0.06037 con:0.00013 recon1:0.02941 recon2:0.03083
	Batch 52/173 Loss:0.06105 con:0.00015 recon1:0.02920 recon2:0.03171
	Batch 53/173 Loss:0.04957 con:0.00015 recon1:0.02445 recon2:0.02497
	Batch 54/173 Loss:0.05086 con:0.00015 recon1:0.02467 recon2:0.02604
	Batch 55/173 Loss:0.04965 con:0.00016 recon1:0.02418 recon2:0.02531
	Batch 56/173 Loss:0.05105 con:0.00016 recon1:0.02529 recon2:0.02561
	Batch 57/173 Loss:0.04856 con:0.00015 recon1:0.02452 recon2:0.02390
	Batch 58/173 Loss:0.05543 con:0.00016 recon1:0.02711 recon2:0.02816
	Batch 59/173 Loss:0.04975 con:0.00014 recon1:0.02391 recon2:0.02570
	Batch 60/173 Loss:0.05024 con:0.00014 recon1:0.02510 recon2:0.02499
	Batch 61/173 Loss:0.05214 con:0.00014 recon1:0.02655 recon2:0.02545
	Batch 62/173 Loss:0.05009 con:0.00013 recon1:0.02557 recon2:0.02439
	Batch 63/173 Loss:0.05227 con:0.00016 recon1:0.02613 recon2:0.02598
	Batch 64/173 Loss:0.04785 con:0.00014 recon1:0.02452 recon2:0.02318
	Batch 65/173 Loss:0.05060 con:0.00015 recon1:0.02423 recon2:0.02621
	Batch 66/173 Loss:0.05095 con:0.00017 recon1:0.02535 recon2:0.02543
	Batch 67/173 Loss:0.04555 con:0.00014 recon1:0.02323 recon2:0.02218
	Batch 68/173 Loss:0.05131 con:0.00016 recon1:0.02407 recon2:0.02708
	Batch 69/173 Loss:0.05009 con:0.00015 recon1:0.02401 recon2:0.02592
	Batch 70/173 Loss:0.05404 con:0.00016 recon1:0.02641 recon2:0.02747
	Batch 71/173 Loss:0.04695 con:0.00015 recon1:0.02255 recon2:0.02425
	Batch 72/173 Loss:0.05287 con:0.00017 recon1:0.02729 recon2:0.02542
	Batch 73/173 Loss:0.05431 con:0.00016 recon1:0.02631 recon2:0.02784
	Batch 74/173 Loss:0.05258 con:0.00015 recon1:0.02637 recon2:0.02606
	Batch 75/173 Loss:0.05149 con:0.00015 recon1:0.02488 recon2:0.02646
	Batch 76/173 Loss:0.06097 con:0.00014 recon1:0.02929 recon2:0.03154
	Batch 77/173 Loss:0.06163 con:0.00015 recon1:0.03031 recon2:0.03116
	Batch 78/173 Loss:0.05613 con:0.00015 recon1:0.02917 recon2:0.02681
	Batch 79/173 Loss:0.06302 con:0.00015 recon1:0.03069 recon2:0.03218
	Batch 80/173 Loss:0.06335 con:0.00014 recon1:0.03038 recon2:0.03282
	Batch 81/173 Loss:0.05268 con:0.00014 recon1:0.02750 recon2:0.02504
	Batch 82/173 Loss:0.04765 con:0.00014 recon1:0.02331 recon2:0.02420
	Batch 83/173 Loss:0.04613 con:0.00015 recon1:0.02221 recon2:0.02378
	Batch 84/173 Loss:0.05203 con:0.00017 recon1:0.02625 recon2:0.02561
	Batch 85/173 Loss:0.05074 con:0.00015 recon1:0.02543 recon2:0.02516
	Batch 86/173 Loss:0.05158 con:0.00015 recon1:0.02535 recon2:0.02608
	Batch 87/173 Loss:0.05979 con:0.00015 recon1:0.02995 recon2:0.02968
	Batch 88/173 Loss:0.05940 con:0.00013 recon1:0.02922 recon2:0.03004
	Batch 89/173 Loss:0.06857 con:0.00015 recon1:0.03278 recon2:0.03564
	Batch 90/173 Loss:0.05089 con:0.00015 recon1:0.02582 recon2:0.02492
	Batch 91/173 Loss:0.05682 con:0.00015 recon1:0.02831 recon2:0.02836
	Batch 92/173 Loss:0.05295 con:0.00015 recon1:0.02567 recon2:0.02713
	Batch 93/173 Loss:0.04806 con:0.00014 recon1:0.02403 recon2:0.02389
	Batch 94/173 Loss:0.05281 con:0.00017 recon1:0.02668 recon2:0.02596
	Batch 95/173 Loss:0.05070 con:0.00016 recon1:0.02572 recon2:0.02483
	Batch 96/173 Loss:0.04758 con:0.00015 recon1:0.02365 recon2:0.02377
	Batch 97/173 Loss:0.05032 con:0.00015 recon1:0.02589 recon2:0.02427
	Batch 98/173 Loss:0.05204 con:0.00017 recon1:0.02495 recon2:0.02692
	Batch 99/173 Loss:0.04928 con:0.00015 recon1:0.02436 recon2:0.02478
	Batch 100/173 Loss:0.05378 con:0.00015 recon1:0.02592 recon2:0.02771
	Batch 101/173 Loss:0.05180 con:0.00016 recon1:0.02654 recon2:0.02510
	Batch 102/173 Loss:0.04715 con:0.00015 recon1:0.02424 recon2:0.02276
	Batch 103/173 Loss:0.04877 con:0.00015 recon1:0.02445 recon2:0.02417
	Batch 104/173 Loss:0.05172 con:0.00016 recon1:0.02571 recon2:0.02585
	Batch 105/173 Loss:0.04702 con:0.00017 recon1:0.02373 recon2:0.02313
	Batch 106/173 Loss:0.04793 con:0.00016 recon1:0.02403 recon2:0.02374
	Batch 107/173 Loss:0.05369 con:0.00016 recon1:0.02682 recon2:0.02672
	Batch 108/173 Loss:0.04886 con:0.00014 recon1:0.02362 recon2:0.02510
	Batch 109/173 Loss:0.05496 con:0.00015 recon1:0.02837 recon2:0.02644
	Batch 110/173 Loss:0.05152 con:0.00015 recon1:0.02593 recon2:0.02544
	Batch 111/173 Loss:0.05265 con:0.00015 recon1:0.02597 recon2:0.02653
	Batch 112/173 Loss:0.04908 con:0.00015 recon1:0.02404 recon2:0.02490
	Batch 113/173 Loss:0.05079 con:0.00017 recon1:0.02483 recon2:0.02579
	Batch 114/173 Loss:0.04569 con:0.00014 recon1:0.02304 recon2:0.02250
	Batch 115/173 Loss:0.05433 con:0.00017 recon1:0.02651 recon2:0.02765
	Batch 116/173 Loss:0.05006 con:0.00015 recon1:0.02523 recon2:0.02468
	Batch 117/173 Loss:0.04850 con:0.00015 recon1:0.02467 recon2:0.02368
	Batch 118/173 Loss:0.05184 con:0.00015 recon1:0.02646 recon2:0.02522
	Batch 119/173 Loss:0.04800 con:0.00016 recon1:0.02410 recon2:0.02374
	Batch 120/173 Loss:0.05222 con:0.00016 recon1:0.02658 recon2:0.02548
	Batch 121/173 Loss:0.04611 con:0.00014 recon1:0.02258 recon2:0.02339
	Batch 122/173 Loss:0.05100 con:0.00015 recon1:0.02540 recon2:0.02545
	Batch 123/173 Loss:0.05210 con:0.00016 recon1:0.02622 recon2:0.02572
	Batch 124/173 Loss:0.05154 con:0.00015 recon1:0.02572 recon2:0.02568
	Batch 125/173 Loss:0.06056 con:0.00014 recon1:0.03097 recon2:0.02945
	Batch 126/173 Loss:0.06315 con:0.00014 recon1:0.03240 recon2:0.03061
	Batch 127/173 Loss:0.05866 con:0.00015 recon1:0.02930 recon2:0.02921
	Batch 128/173 Loss:0.04785 con:0.00015 recon1:0.02447 recon2:0.02323
	Batch 129/173 Loss:0.05022 con:0.00016 recon1:0.02628 recon2:0.02378
	Batch 130/173 Loss:0.05573 con:0.00015 recon1:0.02788 recon2:0.02770
	Batch 131/173 Loss:0.06504 con:0.00015 recon1:0.03216 recon2:0.03273
	Batch 132/173 Loss:0.06250 con:0.00014 recon1:0.03034 recon2:0.03201
	Batch 133/173 Loss:0.06328 con:0.00015 recon1:0.03126 recon2:0.03188
	Batch 134/173 Loss:0.05807 con:0.00013 recon1:0.02944 recon2:0.02850
	Batch 135/173 Loss:0.06060 con:0.00015 recon1:0.02998 recon2:0.03047
	Batch 136/173 Loss:0.05313 con:0.00016 recon1:0.02642 recon2:0.02655
	Batch 137/173 Loss:0.04599 con:0.00015 recon1:0.02213 recon2:0.02371
	Batch 138/173 Loss:0.05342 con:0.00016 recon1:0.02638 recon2:0.02687
	Batch 139/173 Loss:0.04910 con:0.00015 recon1:0.02586 recon2:0.02309
	Batch 140/173 Loss:0.04907 con:0.00015 recon1:0.02547 recon2:0.02346
	Batch 141/173 Loss:0.04798 con:0.00014 recon1:0.02472 recon2:0.02313
	Batch 142/173 Loss:0.06158 con:0.00014 recon1:0.03038 recon2:0.03107
	Batch 143/173 Loss:0.06608 con:0.00015 recon1:0.03181 recon2:0.03412
	Batch 144/173 Loss:0.06021 con:0.00016 recon1:0.03074 recon2:0.02930
	Batch 145/173 Loss:0.04877 con:0.00015 recon1:0.02563 recon2:0.02299
	Batch 146/173 Loss:0.05084 con:0.00014 recon1:0.02594 recon2:0.02476
	Batch 147/173 Loss:0.05309 con:0.00015 recon1:0.02561 recon2:0.02732
	Batch 148/173 Loss:0.05101 con:0.00016 recon1:0.02460 recon2:0.02625
	Batch 149/173 Loss:0.04977 con:0.00014 recon1:0.02481 recon2:0.02482
	Batch 150/173 Loss:0.05208 con:0.00014 recon1:0.02512 recon2:0.02681
	Batch 151/173 Loss:0.04780 con:0.00014 recon1:0.02350 recon2:0.02416
	Batch 152/173 Loss:0.05072 con:0.00017 recon1:0.02504 recon2:0.02551
	Batch 153/173 Loss:0.04804 con:0.00015 recon1:0.02273 recon2:0.02515
	Batch 154/173 Loss:0.05397 con:0.00018 recon1:0.02670 recon2:0.02709
	Batch 155/173 Loss:0.05241 con:0.00017 recon1:0.02525 recon2:0.02699
	Batch 156/173 Loss:0.05038 con:0.00015 recon1:0.02575 recon2:0.02449
	Batch 157/173 Loss:0.04955 con:0.00014 recon1:0.02572 recon2:0.02369
	Batch 158/173 Loss:0.04941 con:0.00015 recon1:0.02603 recon2:0.02323
	Batch 159/173 Loss:0.05030 con:0.00015 recon1:0.02493 recon2:0.02522
	Batch 160/173 Loss:0.05447 con:0.00015 recon1:0.02690 recon2:0.02743
	Batch 161/173 Loss:0.04959 con:0.00015 recon1:0.02492 recon2:0.02452
	Batch 162/173 Loss:0.05137 con:0.00016 recon1:0.02622 recon2:0.02499
	Batch 163/173 Loss:0.04982 con:0.00015 recon1:0.02423 recon2:0.02544
	Batch 164/173 Loss:0.04881 con:0.00016 recon1:0.02505 recon2:0.02361
	Batch 165/173 Loss:0.05367 con:0.00015 recon1:0.02665 recon2:0.02688
	Batch 166/173 Loss:0.04543 con:0.00013 recon1:0.02242 recon2:0.02287
	Batch 167/173 Loss:0.05372 con:0.00016 recon1:0.02586 recon2:0.02770
	Batch 168/173 Loss:0.04852 con:0.00014 recon1:0.02390 recon2:0.02448
	Batch 169/173 Loss:0.05154 con:0.00014 recon1:0.02712 recon2:0.02428
	Batch 170/173 Loss:0.05328 con:0.00016 recon1:0.02621 recon2:0.02691
	Batch 171/173 Loss:0.05625 con:0.00014 recon1:0.02754 recon2:0.02857
	Batch 172/173 Loss:0.06086 con:0.00014 recon1:0.02930 recon2:0.03141
	Batch 173/173 Loss:0.06184 con:0.00014 recon1:0.03005 recon2:0.03166
Testing Epoch 1/1 Loss:0.05250 con:0.00015 recon1:0.02613 recon2:0.02622
Time: 715.2426881790161
