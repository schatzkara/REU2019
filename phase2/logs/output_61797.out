Parameters for training:
Batch Size: 20
Tensor Size: (3,16,112,112)
Skip Length: 2
Precrop: True
Total Epochs: 1000
Learning Rate: 0.0001
FullNetwork(
  (vgg): VGG(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU(inplace)
      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU(inplace)
      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (6): ReLU(inplace)
      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (8): ReLU(inplace)
      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (11): ReLU(inplace)
      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (13): ReLU(inplace)
      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (15): ReLU(inplace)
      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (18): ReLU(inplace)
      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (20): ReLU(inplace)
      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (22): ReLU(inplace)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (25): ReLU(inplace)
      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (27): ReLU(inplace)
      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (29): ReLU(inplace)
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
    (classifier): Sequential(
      (0): Linear(in_features=25088, out_features=4096, bias=True)
      (1): ReLU(inplace)
      (2): Dropout(p=0.5)
      (3): Linear(in_features=4096, out_features=4096, bias=True)
      (4): ReLU(inplace)
      (5): Dropout(p=0.5)
      (6): Linear(in_features=4096, out_features=1000, bias=True)
    )
    (final_layer): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (final_relu): ReLU(inplace)
  )
  (i3d): InceptionI3d(
    (Conv3d_1a_7x7): Unit3D(
      (conv3d): Conv3d(3, 64, kernel_size=[7, 7, 7], stride=(2, 2, 2), bias=False)
      (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
    )
    (MaxPool3d_2a_3x3): MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)
    (Conv3d_2b_1x1): Unit3D(
      (conv3d): Conv3d(64, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
      (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
    )
    (Conv3d_2c_3x3): Unit3D(
      (conv3d): Conv3d(64, 192, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
      (bn): BatchNorm3d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
    )
    (MaxPool3d_3a_3x3): MaxPool3dSamePadding(kernel_size=[2, 3, 3], stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)
    (Mixed_3b): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(192, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(192, 96, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(96, 128, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(192, 16, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(16, 32, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(192, 32, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (Mixed_3c): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(256, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(256, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(128, 192, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(256, 32, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(32, 96, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(256, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (MaxPool3d_4a_3x3): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)
    (Mixed_4b): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(480, 192, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(480, 96, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(96, 208, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(208, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(480, 16, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(16, 48, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(480, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (Mixed_4c): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(512, 160, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(512, 112, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(112, 224, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(224, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(512, 24, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(24, 64, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(512, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (Mixed_4d): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(512, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(512, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(128, 256, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(512, 24, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(24, 64, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(512, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (Mixed_4e): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(512, 112, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(512, 144, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(144, 288, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(512, 32, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(32, 64, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(512, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (Mixed_4f): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(528, 256, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(528, 160, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(160, 320, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(320, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(528, 32, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(32, 128, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(528, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (MaxPool3d_5a_1x1): MaxPool3dSamePadding(kernel_size=[2, 2, 2], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
    (Mixed_5b): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(832, 256, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(832, 160, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(160, 320, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(320, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(832, 32, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(32, 128, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(832, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (final_layer): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(832, 96, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(832, 48, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(48, 96, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(832, 12, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(12, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(12, 32, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(832, 32, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
  )
  (deconv): Deconv(
    (conv3d_1a): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (conv3d_1b): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (conv3d_2a): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (conv3d_2b): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
  )
  (exp): Expander(
    (conv3d_1a): Conv3d(1, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (conv3d_1b): Conv3d(4, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
  )
  (trans): Transformer(
    (conv3d_1a): Conv3d(33, 33, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (conv3d_1b): Conv3d(33, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (conv3d_1c): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
  )
  (gen): Generator(
    (conv3d_1a): Conv3d(64, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (conv3d_1b): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (conv3d_2a): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (conv3d_2b): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (conv3d_3a): Conv3d(32, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (conv3d_3b): Conv3d(16, 3, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
  )
)
Training...
	Batch 10/672 Loss:0.11991 con1:0.00089 con2:0.00086 recon1:0.05822 recon2:0.05994
	Batch 20/672 Loss:0.09149 con1:0.00062 con2:0.00063 recon1:0.04518 recon2:0.04506
	Batch 30/672 Loss:0.06848 con1:0.00062 con2:0.00061 recon1:0.03356 recon2:0.03369
	Batch 40/672 Loss:0.05645 con1:0.00055 con2:0.00056 recon1:0.02678 recon2:0.02856
	Batch 50/672 Loss:0.04291 con1:0.00031 con2:0.00031 recon1:0.02165 recon2:0.02064
	Batch 60/672 Loss:0.03770 con1:0.00024 con2:0.00024 recon1:0.01803 recon2:0.01919
	Batch 70/672 Loss:0.03857 con1:0.00020 con2:0.00020 recon1:0.01900 recon2:0.01918
	Batch 80/672 Loss:0.03270 con1:0.00016 con2:0.00016 recon1:0.01542 recon2:0.01696
	Batch 90/672 Loss:0.03203 con1:0.00016 con2:0.00017 recon1:0.01582 recon2:0.01589
	Batch 100/672 Loss:0.03028 con1:0.00014 con2:0.00015 recon1:0.01486 recon2:0.01513
	Batch 110/672 Loss:0.03237 con1:0.00013 con2:0.00013 recon1:0.01610 recon2:0.01600
	Batch 120/672 Loss:0.03006 con1:0.00011 con2:0.00011 recon1:0.01504 recon2:0.01479
	Batch 130/672 Loss:0.02433 con1:0.00011 con2:0.00011 recon1:0.01172 recon2:0.01239
	Batch 140/672 Loss:0.02334 con1:0.00009 con2:0.00009 recon1:0.01147 recon2:0.01168
	Batch 150/672 Loss:0.02548 con1:0.00008 con2:0.00009 recon1:0.01257 recon2:0.01274
	Batch 160/672 Loss:0.02772 con1:0.00008 con2:0.00008 recon1:0.01381 recon2:0.01374
	Batch 170/672 Loss:0.02845 con1:0.00008 con2:0.00008 recon1:0.01394 recon2:0.01436
	Batch 180/672 Loss:0.02560 con1:0.00007 con2:0.00007 recon1:0.01291 recon2:0.01255
	Batch 190/672 Loss:0.02228 con1:0.00007 con2:0.00007 recon1:0.01105 recon2:0.01110
	Batch 200/672 Loss:0.02413 con1:0.00006 con2:0.00006 recon1:0.01191 recon2:0.01210
	Batch 210/672 Loss:0.02339 con1:0.00006 con2:0.00006 recon1:0.01198 recon2:0.01128
	Batch 220/672 Loss:0.02728 con1:0.00006 con2:0.00005 recon1:0.01296 recon2:0.01421
	Batch 230/672 Loss:0.02431 con1:0.00005 con2:0.00005 recon1:0.01192 recon2:0.01228
	Batch 240/672 Loss:0.02165 con1:0.00005 con2:0.00005 recon1:0.01073 recon2:0.01083
	Batch 250/672 Loss:0.02216 con1:0.00005 con2:0.00005 recon1:0.01092 recon2:0.01114
	Batch 260/672 Loss:0.02439 con1:0.00004 con2:0.00004 recon1:0.01186 recon2:0.01244
	Batch 270/672 Loss:0.02506 con1:0.00004 con2:0.00004 recon1:0.01206 recon2:0.01291
	Batch 280/672 Loss:0.02042 con1:0.00004 con2:0.00004 recon1:0.01040 recon2:0.00995
	Batch 290/672 Loss:0.01945 con1:0.00004 con2:0.00004 recon1:0.00947 recon2:0.00990
	Batch 300/672 Loss:0.02181 con1:0.00004 con2:0.00004 recon1:0.01070 recon2:0.01104
	Batch 310/672 Loss:0.01952 con1:0.00004 con2:0.00004 recon1:0.00970 recon2:0.00975
	Batch 320/672 Loss:0.01953 con1:0.00004 con2:0.00003 recon1:0.01014 recon2:0.00933
	Batch 330/672 Loss:0.02004 con1:0.00003 con2:0.00003 recon1:0.00995 recon2:0.01003
	Batch 340/672 Loss:0.01865 con1:0.00003 con2:0.00003 recon1:0.00959 recon2:0.00900
	Batch 350/672 Loss:0.02330 con1:0.00003 con2:0.00003 recon1:0.01156 recon2:0.01167
	Batch 360/672 Loss:0.02000 con1:0.00003 con2:0.00003 recon1:0.01006 recon2:0.00988
	Batch 370/672 Loss:0.01728 con1:0.00003 con2:0.00003 recon1:0.00866 recon2:0.00856
	Batch 380/672 Loss:0.02061 con1:0.00003 con2:0.00003 recon1:0.00971 recon2:0.01085
	Batch 390/672 Loss:0.01952 con1:0.00003 con2:0.00003 recon1:0.00953 recon2:0.00993
	Batch 400/672 Loss:0.02033 con1:0.00002 con2:0.00002 recon1:0.00939 recon2:0.01090
	Batch 410/672 Loss:0.02003 con1:0.00002 con2:0.00002 recon1:0.01015 recon2:0.00984
	Batch 420/672 Loss:0.02014 con1:0.00002 con2:0.00002 recon1:0.00999 recon2:0.01010
	Batch 430/672 Loss:0.02385 con1:0.00002 con2:0.00002 recon1:0.01187 recon2:0.01193
	Batch 440/672 Loss:0.02011 con1:0.00002 con2:0.00002 recon1:0.01058 recon2:0.00949
	Batch 450/672 Loss:0.02371 con1:0.00002 con2:0.00002 recon1:0.01100 recon2:0.01266
	Batch 460/672 Loss:0.02500 con1:0.00002 con2:0.00002 recon1:0.01295 recon2:0.01201
	Batch 470/672 Loss:0.02195 con1:0.00002 con2:0.00002 recon1:0.01052 recon2:0.01138
	Batch 480/672 Loss:0.01618 con1:0.00002 con2:0.00002 recon1:0.00776 recon2:0.00838
	Batch 490/672 Loss:0.01751 con1:0.00002 con2:0.00002 recon1:0.00935 recon2:0.00812
	Batch 500/672 Loss:0.01664 con1:0.00002 con2:0.00002 recon1:0.00844 recon2:0.00816
	Batch 510/672 Loss:0.01890 con1:0.00002 con2:0.00002 recon1:0.00932 recon2:0.00955
	Batch 520/672 Loss:0.02110 con1:0.00002 con2:0.00002 recon1:0.00983 recon2:0.01123
	Batch 530/672 Loss:0.01584 con1:0.00002 con2:0.00002 recon1:0.00768 recon2:0.00813
	Batch 540/672 Loss:0.01751 con1:0.00002 con2:0.00002 recon1:0.00868 recon2:0.00880
	Batch 550/672 Loss:0.01811 con1:0.00002 con2:0.00002 recon1:0.00873 recon2:0.00934
	Batch 560/672 Loss:0.01793 con1:0.00002 con2:0.00002 recon1:0.00907 recon2:0.00883
	Batch 570/672 Loss:0.01620 con1:0.00002 con2:0.00002 recon1:0.00791 recon2:0.00826
	Batch 580/672 Loss:0.01714 con1:0.00002 con2:0.00002 recon1:0.00833 recon2:0.00878
	Batch 590/672 Loss:0.02439 con1:0.00002 con2:0.00002 recon1:0.01204 recon2:0.01232
	Batch 600/672 Loss:0.01878 con1:0.00002 con2:0.00002 recon1:0.00942 recon2:0.00933
	Batch 610/672 Loss:0.01715 con1:0.00001 con2:0.00001 recon1:0.00916 recon2:0.00796
	Batch 620/672 Loss:0.01541 con1:0.00001 con2:0.00001 recon1:0.00769 recon2:0.00769
	Batch 630/672 Loss:0.01616 con1:0.00001 con2:0.00002 recon1:0.00756 recon2:0.00857
	Batch 640/672 Loss:0.01773 con1:0.00001 con2:0.00001 recon1:0.00939 recon2:0.00831
	Batch 650/672 Loss:0.01746 con1:0.00001 con2:0.00001 recon1:0.00834 recon2:0.00909
	Batch 660/672 Loss:0.01821 con1:0.00001 con2:0.00001 recon1:0.00895 recon2:0.00923
	Batch 670/672 Loss:0.02111 con1:0.00001 con2:0.00001 recon1:0.01062 recon2:0.01046
Training Epoch 1/1000 Loss:0.02902 con1:0.00011 con2:0.00011 recon1:0.01440 recon2:0.01440
Validation...
	Batch 10/276 Loss:0.01371 con1:0.00001 con2: 0.00001 recon1:0.00677 recon2:0.00691
	Batch 20/276 Loss:0.01258 con1:0.00001 con2: 0.00001 recon1:0.00627 recon2:0.00628
	Batch 30/276 Loss:0.01394 con1:0.00001 con2: 0.00001 recon1:0.00706 recon2:0.00685
	Batch 40/276 Loss:0.02233 con1:0.00002 con2: 0.00002 recon1:0.01143 recon2:0.01087
	Batch 50/276 Loss:0.01484 con1:0.00001 con2: 0.00002 recon1:0.00765 recon2:0.00716
	Batch 60/276 Loss:0.01452 con1:0.00001 con2: 0.00001 recon1:0.00714 recon2:0.00735
	Batch 70/276 Loss:0.01585 con1:0.00001 con2: 0.00001 recon1:0.00747 recon2:0.00835
	Batch 80/276 Loss:0.01703 con1:0.00002 con2: 0.00002 recon1:0.00814 recon2:0.00886
	Batch 90/276 Loss:0.01090 con1:0.00001 con2: 0.00001 recon1:0.00537 recon2:0.00551
	Batch 100/276 Loss:0.01272 con1:0.00001 con2: 0.00001 recon1:0.00625 recon2:0.00644
	Batch 110/276 Loss:0.01126 con1:0.00002 con2: 0.00001 recon1:0.00574 recon2:0.00549
	Batch 120/276 Loss:0.01799 con1:0.00001 con2: 0.00001 recon1:0.00833 recon2:0.00963
	Batch 130/276 Loss:0.01405 con1:0.00002 con2: 0.00002 recon1:0.00691 recon2:0.00711
	Batch 140/276 Loss:0.02052 con1:0.00001 con2: 0.00001 recon1:0.01015 recon2:0.01034
	Batch 150/276 Loss:0.01589 con1:0.00002 con2: 0.00002 recon1:0.00810 recon2:0.00776
	Batch 160/276 Loss:0.02264 con1:0.00001 con2: 0.00001 recon1:0.01135 recon2:0.01126
	Batch 170/276 Loss:0.01865 con1:0.00002 con2: 0.00001 recon1:0.00903 recon2:0.00959
	Batch 180/276 Loss:0.01413 con1:0.00001 con2: 0.00001 recon1:0.00694 recon2:0.00715
	Batch 190/276 Loss:0.01232 con1:0.00001 con2: 0.00001 recon1:0.00599 recon2:0.00631
	Batch 200/276 Loss:0.01813 con1:0.00001 con2: 0.00002 recon1:0.00937 recon2:0.00874
	Batch 210/276 Loss:0.02124 con1:0.00001 con2: 0.00001 recon1:0.01067 recon2:0.01054
	Batch 220/276 Loss:0.01366 con1:0.00001 con2: 0.00001 recon1:0.00700 recon2:0.00663
	Batch 230/276 Loss:0.04496 con1:0.00001 con2: 0.00002 recon1:0.02070 recon2:0.02423
	Batch 240/276 Loss:0.01230 con1:0.00001 con2: 0.00001 recon1:0.00608 recon2:0.00619
	Batch 250/276 Loss:0.01246 con1:0.00001 con2: 0.00001 recon1:0.00618 recon2:0.00625
	Batch 260/276 Loss:0.01226 con1:0.00001 con2: 0.00001 recon1:0.00600 recon2:0.00624
	Batch 270/276 Loss:0.01552 con1:0.00001 con2: 0.00001 recon1:0.00761 recon2:0.00788
Validation Epoch 1/1000 Loss:0.01657 con1:0.00001 con2:0.00001 recon1:0.00823 recon2:0.00831
Training...
	Batch 10/672 Loss:0.02024 con1:0.00001 con2:0.00001 recon1:0.00954 recon2:0.01068
	Batch 20/672 Loss:0.01688 con1:0.00001 con2:0.00001 recon1:0.00828 recon2:0.00857
	Batch 30/672 Loss:0.01719 con1:0.00001 con2:0.00001 recon1:0.00899 recon2:0.00818
	Batch 40/672 Loss:0.01859 con1:0.00001 con2:0.00001 recon1:0.00926 recon2:0.00931
	Batch 50/672 Loss:0.01554 con1:0.00001 con2:0.00001 recon1:0.00790 recon2:0.00762
	Batch 60/672 Loss:0.01999 con1:0.00001 con2:0.00001 recon1:0.01016 recon2:0.00980
	Batch 70/672 Loss:0.01716 con1:0.00001 con2:0.00001 recon1:0.00853 recon2:0.00860
	Batch 80/672 Loss:0.01577 con1:0.00001 con2:0.00001 recon1:0.00795 recon2:0.00779
	Batch 90/672 Loss:0.01902 con1:0.00001 con2:0.00001 recon1:0.00946 recon2:0.00954
	Batch 100/672 Loss:0.01688 con1:0.00001 con2:0.00001 recon1:0.00848 recon2:0.00838
	Batch 110/672 Loss:0.01803 con1:0.00001 con2:0.00001 recon1:0.00787 recon2:0.01014
	Batch 120/672 Loss:0.01649 con1:0.00001 con2:0.00001 recon1:0.00795 recon2:0.00852
	Batch 130/672 Loss:0.01760 con1:0.00001 con2:0.00001 recon1:0.00891 recon2:0.00867
	Batch 140/672 Loss:0.01984 con1:0.00001 con2:0.00001 recon1:0.00944 recon2:0.01038
	Batch 150/672 Loss:0.01857 con1:0.00001 con2:0.00001 recon1:0.00939 recon2:0.00916
	Batch 160/672 Loss:0.01780 con1:0.00001 con2:0.00001 recon1:0.00879 recon2:0.00899
	Batch 170/672 Loss:0.01982 con1:0.00001 con2:0.00001 recon1:0.00955 recon2:0.01025
	Batch 180/672 Loss:0.01574 con1:0.00001 con2:0.00001 recon1:0.00856 recon2:0.00717
	Batch 190/672 Loss:0.01633 con1:0.00001 con2:0.00001 recon1:0.00777 recon2:0.00854
	Batch 200/672 Loss:0.01714 con1:0.00001 con2:0.00001 recon1:0.00859 recon2:0.00852
	Batch 210/672 Loss:0.02005 con1:0.00001 con2:0.00001 recon1:0.01061 recon2:0.00942
	Batch 220/672 Loss:0.01881 con1:0.00001 con2:0.00001 recon1:0.00939 recon2:0.00940
	Batch 230/672 Loss:0.01618 con1:0.00001 con2:0.00001 recon1:0.00824 recon2:0.00792
	Batch 240/672 Loss:0.01891 con1:0.00001 con2:0.00001 recon1:0.00954 recon2:0.00935
	Batch 250/672 Loss:0.01628 con1:0.00001 con2:0.00001 recon1:0.00788 recon2:0.00839
	Batch 260/672 Loss:0.01950 con1:0.00001 con2:0.00001 recon1:0.00971 recon2:0.00977
	Batch 270/672 Loss:0.01464 con1:0.00001 con2:0.00001 recon1:0.00753 recon2:0.00709
	Batch 280/672 Loss:0.01931 con1:0.00001 con2:0.00001 recon1:0.00973 recon2:0.00957
	Batch 290/672 Loss:0.02048 con1:0.00001 con2:0.00001 recon1:0.01024 recon2:0.01022
	Batch 300/672 Loss:0.01695 con1:0.00001 con2:0.00001 recon1:0.00831 recon2:0.00862
	Batch 310/672 Loss:0.01817 con1:0.00001 con2:0.00001 recon1:0.00954 recon2:0.00862
	Batch 320/672 Loss:0.01550 con1:0.00001 con2:0.00001 recon1:0.00790 recon2:0.00758
	Batch 330/672 Loss:0.01730 con1:0.00001 con2:0.00001 recon1:0.00876 recon2:0.00853
	Batch 340/672 Loss:0.01386 con1:0.00001 con2:0.00001 recon1:0.00682 recon2:0.00703
	Batch 350/672 Loss:0.01696 con1:0.00001 con2:0.00001 recon1:0.00863 recon2:0.00831
	Batch 360/672 Loss:0.01497 con1:0.00001 con2:0.00001 recon1:0.00753 recon2:0.00743
	Batch 370/672 Loss:0.01462 con1:0.00001 con2:0.00001 recon1:0.00715 recon2:0.00745
	Batch 380/672 Loss:0.01679 con1:0.00001 con2:0.00001 recon1:0.00875 recon2:0.00802
	Batch 390/672 Loss:0.01731 con1:0.00001 con2:0.00001 recon1:0.00925 recon2:0.00805
	Batch 400/672 Loss:0.02190 con1:0.00001 con2:0.00001 recon1:0.01028 recon2:0.01160
	Batch 410/672 Loss:0.01448 con1:0.00001 con2:0.00001 recon1:0.00731 recon2:0.00715
	Batch 420/672 Loss:0.01614 con1:0.00001 con2:0.00001 recon1:0.00772 recon2:0.00840
	Batch 430/672 Loss:0.01493 con1:0.00001 con2:0.00001 recon1:0.00736 recon2:0.00756
	Batch 440/672 Loss:0.01782 con1:0.00001 con2:0.00001 recon1:0.00867 recon2:0.00912
	Batch 450/672 Loss:0.01907 con1:0.00001 con2:0.00001 recon1:0.00994 recon2:0.00911
	Batch 460/672 Loss:0.01756 con1:0.00001 con2:0.00001 recon1:0.00826 recon2:0.00929
	Batch 470/672 Loss:0.01700 con1:0.00001 con2:0.00001 recon1:0.00869 recon2:0.00829
	Batch 480/672 Loss:0.01616 con1:0.00001 con2:0.00001 recon1:0.00875 recon2:0.00739
	Batch 490/672 Loss:0.01556 con1:0.00001 con2:0.00001 recon1:0.00856 recon2:0.00698
	Batch 500/672 Loss:0.01331 con1:0.00001 con2:0.00001 recon1:0.00644 recon2:0.00685
	Batch 510/672 Loss:0.01477 con1:0.00001 con2:0.00001 recon1:0.00761 recon2:0.00714
	Batch 520/672 Loss:0.01901 con1:0.00001 con2:0.00001 recon1:0.00920 recon2:0.00979
	Batch 530/672 Loss:0.01580 con1:0.00001 con2:0.00001 recon1:0.00825 recon2:0.00753
	Batch 540/672 Loss:0.01392 con1:0.00001 con2:0.00001 recon1:0.00678 recon2:0.00712
	Batch 550/672 Loss:0.01385 con1:0.00001 con2:0.00001 recon1:0.00695 recon2:0.00688
	Batch 560/672 Loss:0.01773 con1:0.00001 con2:0.00001 recon1:0.00796 recon2:0.00975
	Batch 570/672 Loss:0.01949 con1:0.00001 con2:0.00001 recon1:0.00941 recon2:0.01007
	Batch 580/672 Loss:0.01787 con1:0.00001 con2:0.00001 recon1:0.00861 recon2:0.00925
	Batch 590/672 Loss:0.01484 con1:0.00001 con2:0.00001 recon1:0.00762 recon2:0.00721
	Batch 600/672 Loss:0.01795 con1:0.00001 con2:0.00001 recon1:0.00894 recon2:0.00900
	Batch 610/672 Loss:0.01476 con1:0.00001 con2:0.00001 recon1:0.00730 recon2:0.00744
	Batch 620/672 Loss:0.01340 con1:0.00001 con2:0.00001 recon1:0.00656 recon2:0.00682
	Batch 630/672 Loss:0.01928 con1:0.00001 con2:0.00001 recon1:0.00932 recon2:0.00995
	Batch 640/672 Loss:0.02107 con1:0.00001 con2:0.00001 recon1:0.01066 recon2:0.01039
	Batch 650/672 Loss:0.01967 con1:0.00002 con2:0.00002 recon1:0.00948 recon2:0.01016
	Batch 660/672 Loss:0.01906 con1:0.00001 con2:0.00001 recon1:0.00942 recon2:0.00963
	Batch 670/672 Loss:0.01989 con1:0.00001 con2:0.00001 recon1:0.01061 recon2:0.00927
Training Epoch 2/1000 Loss:0.01664 con1:0.00001 con2:0.00001 recon1:0.00830 recon2:0.00832
Validation...
	Batch 10/276 Loss:0.01175 con1:0.00001 con2: 0.00001 recon1:0.00582 recon2:0.00592
	Batch 20/276 Loss:0.01092 con1:0.00001 con2: 0.00001 recon1:0.00548 recon2:0.00542
	Batch 30/276 Loss:0.01185 con1:0.00001 con2: 0.00001 recon1:0.00603 recon2:0.00580
	Batch 40/276 Loss:0.02003 con1:0.00004 con2: 0.00003 recon1:0.01017 recon2:0.00979
	Batch 50/276 Loss:0.01263 con1:0.00001 con2: 0.00001 recon1:0.00650 recon2:0.00612
	Batch 60/276 Loss:0.01280 con1:0.00001 con2: 0.00001 recon1:0.00624 recon2:0.00654
	Batch 70/276 Loss:0.01420 con1:0.00001 con2: 0.00001 recon1:0.00671 recon2:0.00747
	Batch 80/276 Loss:0.01519 con1:0.00001 con2: 0.00001 recon1:0.00723 recon2:0.00794
	Batch 90/276 Loss:0.00923 con1:0.00001 con2: 0.00001 recon1:0.00455 recon2:0.00467
	Batch 100/276 Loss:0.01092 con1:0.00001 con2: 0.00001 recon1:0.00540 recon2:0.00551
	Batch 110/276 Loss:0.00948 con1:0.00001 con2: 0.00001 recon1:0.00486 recon2:0.00461
	Batch 120/276 Loss:0.01627 con1:0.00001 con2: 0.00001 recon1:0.00748 recon2:0.00878
	Batch 130/276 Loss:0.01215 con1:0.00001 con2: 0.00001 recon1:0.00596 recon2:0.00618
	Batch 140/276 Loss:0.01854 con1:0.00001 con2: 0.00001 recon1:0.00922 recon2:0.00931
	Batch 150/276 Loss:0.01392 con1:0.00001 con2: 0.00001 recon1:0.00712 recon2:0.00679
	Batch 160/276 Loss:0.02081 con1:0.00001 con2: 0.00001 recon1:0.01049 recon2:0.01030
	Batch 170/276 Loss:0.01666 con1:0.00002 con2: 0.00002 recon1:0.00799 recon2:0.00863
	Batch 180/276 Loss:0.01216 con1:0.00001 con2: 0.00001 recon1:0.00596 recon2:0.00618
	Batch 190/276 Loss:0.01058 con1:0.00001 con2: 0.00001 recon1:0.00516 recon2:0.00541
	Batch 200/276 Loss:0.01602 con1:0.00001 con2: 0.00001 recon1:0.00835 recon2:0.00766
	Batch 210/276 Loss:0.01891 con1:0.00001 con2: 0.00001 recon1:0.00944 recon2:0.00945
	Batch 220/276 Loss:0.01174 con1:0.00001 con2: 0.00001 recon1:0.00603 recon2:0.00570
	Batch 230/276 Loss:0.04301 con1:0.00002 con2: 0.00002 recon1:0.01975 recon2:0.02323
	Batch 240/276 Loss:0.01040 con1:0.00001 con2: 0.00001 recon1:0.00516 recon2:0.00523
	Batch 250/276 Loss:0.01082 con1:0.00001 con2: 0.00001 recon1:0.00536 recon2:0.00545
	Batch 260/276 Loss:0.01039 con1:0.00001 con2: 0.00001 recon1:0.00509 recon2:0.00528
	Batch 270/276 Loss:0.01366 con1:0.00001 con2: 0.00001 recon1:0.00671 recon2:0.00694
Validation Epoch 2/1000 Loss:0.01467 con1:0.00001 con2:0.00001 recon1:0.00728 recon2:0.00737
Training...
	Batch 10/672 Loss:0.01332 con1:0.00001 con2:0.00001 recon1:0.00646 recon2:0.00685
	Batch 20/672 Loss:0.01385 con1:0.00001 con2:0.00001 recon1:0.00650 recon2:0.00734
	Batch 30/672 Loss:0.01484 con1:0.00001 con2:0.00001 recon1:0.00695 recon2:0.00788
	Batch 40/672 Loss:0.01710 con1:0.00001 con2:0.00001 recon1:0.00851 recon2:0.00858
	Batch 50/672 Loss:0.01582 con1:0.00001 con2:0.00001 recon1:0.00814 recon2:0.00766
	Batch 60/672 Loss:0.01297 con1:0.00001 con2:0.00001 recon1:0.00655 recon2:0.00640
	Batch 70/672 Loss:0.01507 con1:0.00001 con2:0.00001 recon1:0.00765 recon2:0.00741
	Batch 80/672 Loss:0.01512 con1:0.00001 con2:0.00001 recon1:0.00796 recon2:0.00714
	Batch 90/672 Loss:0.01298 con1:0.00001 con2:0.00001 recon1:0.00659 recon2:0.00638
	Batch 100/672 Loss:0.01271 con1:0.00001 con2:0.00001 recon1:0.00636 recon2:0.00633
	Batch 110/672 Loss:0.01817 con1:0.00001 con2:0.00001 recon1:0.00897 recon2:0.00918
	Batch 120/672 Loss:0.01324 con1:0.00001 con2:0.00001 recon1:0.00672 recon2:0.00650
	Batch 130/672 Loss:0.01597 con1:0.00001 con2:0.00001 recon1:0.00832 recon2:0.00763
	Batch 140/672 Loss:0.01346 con1:0.00001 con2:0.00001 recon1:0.00651 recon2:0.00693
	Batch 150/672 Loss:0.01572 con1:0.00001 con2:0.00001 recon1:0.00764 recon2:0.00807
	Batch 160/672 Loss:0.01585 con1:0.00001 con2:0.00001 recon1:0.00772 recon2:0.00812
	Batch 170/672 Loss:0.01266 con1:0.00001 con2:0.00001 recon1:0.00671 recon2:0.00594
	Batch 180/672 Loss:0.01407 con1:0.00001 con2:0.00001 recon1:0.00745 recon2:0.00660
	Batch 190/672 Loss:0.01394 con1:0.00000 con2:0.00001 recon1:0.00725 recon2:0.00668
	Batch 200/672 Loss:0.01419 con1:0.00001 con2:0.00001 recon1:0.00733 recon2:0.00685
	Batch 210/672 Loss:0.01307 con1:0.00000 con2:0.00000 recon1:0.00671 recon2:0.00635
	Batch 220/672 Loss:0.01601 con1:0.00001 con2:0.00001 recon1:0.00791 recon2:0.00809
	Batch 230/672 Loss:0.01847 con1:0.00001 con2:0.00001 recon1:0.00986 recon2:0.00858
	Batch 240/672 Loss:0.01329 con1:0.00001 con2:0.00001 recon1:0.00654 recon2:0.00674
	Batch 250/672 Loss:0.01394 con1:0.00002 con2:0.00001 recon1:0.00688 recon2:0.00704
	Batch 260/672 Loss:0.01818 con1:0.00001 con2:0.00001 recon1:0.00875 recon2:0.00941
	Batch 270/672 Loss:0.01913 con1:0.00001 con2:0.00001 recon1:0.00936 recon2:0.00974
	Batch 280/672 Loss:0.01424 con1:0.00001 con2:0.00001 recon1:0.00723 recon2:0.00699
	Batch 290/672 Loss:0.01468 con1:0.00001 con2:0.00001 recon1:0.00731 recon2:0.00735
	Batch 300/672 Loss:0.02029 con1:0.00001 con2:0.00001 recon1:0.01035 recon2:0.00993
	Batch 310/672 Loss:0.01813 con1:0.00001 con2:0.00001 recon1:0.01099 recon2:0.00712
	Batch 320/672 Loss:0.01324 con1:0.00001 con2:0.00001 recon1:0.00675 recon2:0.00648
	Batch 330/672 Loss:0.01332 con1:0.00001 con2:0.00001 recon1:0.00679 recon2:0.00651
	Batch 340/672 Loss:0.01793 con1:0.00001 con2:0.00001 recon1:0.00914 recon2:0.00877
	Batch 350/672 Loss:0.01644 con1:0.00001 con2:0.00001 recon1:0.00815 recon2:0.00827
	Batch 360/672 Loss:0.01815 con1:0.00001 con2:0.00001 recon1:0.00831 recon2:0.00982
	Batch 370/672 Loss:0.01353 con1:0.00001 con2:0.00001 recon1:0.00682 recon2:0.00669
	Batch 380/672 Loss:0.01471 con1:0.00001 con2:0.00001 recon1:0.00736 recon2:0.00733
	Batch 390/672 Loss:0.01398 con1:0.00001 con2:0.00001 recon1:0.00687 recon2:0.00709
	Batch 400/672 Loss:0.01485 con1:0.00001 con2:0.00001 recon1:0.00740 recon2:0.00743
	Batch 410/672 Loss:0.01362 con1:0.00001 con2:0.00001 recon1:0.00622 recon2:0.00738
	Batch 420/672 Loss:0.01279 con1:0.00000 con2:0.00001 recon1:0.00638 recon2:0.00640
	Batch 430/672 Loss:0.01580 con1:0.00001 con2:0.00001 recon1:0.00739 recon2:0.00839
	Batch 440/672 Loss:0.01623 con1:0.00001 con2:0.00001 recon1:0.00768 recon2:0.00853
	Batch 450/672 Loss:0.01361 con1:0.00001 con2:0.00000 recon1:0.00703 recon2:0.00657
	Batch 460/672 Loss:0.01474 con1:0.00001 con2:0.00000 recon1:0.00770 recon2:0.00702
	Batch 470/672 Loss:0.01268 con1:0.00001 con2:0.00001 recon1:0.00626 recon2:0.00640
	Batch 480/672 Loss:0.01691 con1:0.00001 con2:0.00001 recon1:0.00845 recon2:0.00845
	Batch 490/672 Loss:0.01494 con1:0.00000 con2:0.00000 recon1:0.00762 recon2:0.00731
	Batch 500/672 Loss:0.01338 con1:0.00001 con2:0.00001 recon1:0.00665 recon2:0.00673
	Batch 510/672 Loss:0.01305 con1:0.00001 con2:0.00001 recon1:0.00653 recon2:0.00651
	Batch 520/672 Loss:0.01328 con1:0.00001 con2:0.00001 recon1:0.00638 recon2:0.00689
	Batch 530/672 Loss:0.01507 con1:0.00001 con2:0.00001 recon1:0.00766 recon2:0.00740
	Batch 540/672 Loss:0.01702 con1:0.00001 con2:0.00001 recon1:0.00805 recon2:0.00895
	Batch 550/672 Loss:0.01882 con1:0.00001 con2:0.00001 recon1:0.00946 recon2:0.00934
	Batch 560/672 Loss:0.01112 con1:0.00001 con2:0.00001 recon1:0.00574 recon2:0.00537
	Batch 570/672 Loss:0.01458 con1:0.00002 con2:0.00001 recon1:0.00700 recon2:0.00754
	Batch 580/672 Loss:0.01562 con1:0.00001 con2:0.00001 recon1:0.00794 recon2:0.00766
	Batch 590/672 Loss:0.01408 con1:0.00001 con2:0.00001 recon1:0.00716 recon2:0.00691
	Batch 600/672 Loss:0.01390 con1:0.00000 con2:0.00000 recon1:0.00701 recon2:0.00688
	Batch 610/672 Loss:0.01627 con1:0.00001 con2:0.00001 recon1:0.00824 recon2:0.00802
	Batch 620/672 Loss:0.01608 con1:0.00001 con2:0.00001 recon1:0.00782 recon2:0.00825
	Batch 630/672 Loss:0.01336 con1:0.00001 con2:0.00001 recon1:0.00660 recon2:0.00675
	Batch 640/672 Loss:0.01512 con1:0.00001 con2:0.00001 recon1:0.00813 recon2:0.00696
	Batch 650/672 Loss:0.01387 con1:0.00001 con2:0.00001 recon1:0.00662 recon2:0.00723
	Batch 660/672 Loss:0.01944 con1:0.00002 con2:0.00002 recon1:0.00893 recon2:0.01048
	Batch 670/672 Loss:0.01427 con1:0.00001 con2:0.00001 recon1:0.00711 recon2:0.00715
Training Epoch 3/1000 Loss:0.01507 con1:0.00001 con2:0.00001 recon1:0.00754 recon2:0.00751
Validation...
	Batch 10/276 Loss:0.01052 con1:0.00000 con2: 0.00000 recon1:0.00519 recon2:0.00532
	Batch 20/276 Loss:0.00978 con1:0.00001 con2: 0.00000 recon1:0.00492 recon2:0.00486
	Batch 30/276 Loss:0.01051 con1:0.00000 con2: 0.00000 recon1:0.00537 recon2:0.00513
	Batch 40/276 Loss:0.01903 con1:0.00003 con2: 0.00003 recon1:0.00961 recon2:0.00935
	Batch 50/276 Loss:0.01132 con1:0.00000 con2: 0.00000 recon1:0.00582 recon2:0.00549
	Batch 60/276 Loss:0.01169 con1:0.00000 con2: 0.00000 recon1:0.00569 recon2:0.00599
	Batch 70/276 Loss:0.01316 con1:0.00001 con2: 0.00001 recon1:0.00620 recon2:0.00695
	Batch 80/276 Loss:0.01390 con1:0.00001 con2: 0.00001 recon1:0.00659 recon2:0.00730
	Batch 90/276 Loss:0.00804 con1:0.00000 con2: 0.00000 recon1:0.00397 recon2:0.00406
	Batch 100/276 Loss:0.00967 con1:0.00000 con2: 0.00000 recon1:0.00483 recon2:0.00483
	Batch 110/276 Loss:0.00828 con1:0.00000 con2: 0.00000 recon1:0.00425 recon2:0.00402
	Batch 120/276 Loss:0.01524 con1:0.00001 con2: 0.00001 recon1:0.00695 recon2:0.00828
	Batch 130/276 Loss:0.01104 con1:0.00001 con2: 0.00001 recon1:0.00542 recon2:0.00561
	Batch 140/276 Loss:0.01748 con1:0.00001 con2: 0.00001 recon1:0.00870 recon2:0.00877
	Batch 150/276 Loss:0.01272 con1:0.00000 con2: 0.00000 recon1:0.00651 recon2:0.00620
	Batch 160/276 Loss:0.02007 con1:0.00001 con2: 0.00001 recon1:0.01012 recon2:0.00992
	Batch 170/276 Loss:0.01573 con1:0.00001 con2: 0.00002 recon1:0.00751 recon2:0.00818
	Batch 180/276 Loss:0.01090 con1:0.00000 con2: 0.00001 recon1:0.00534 recon2:0.00556
	Batch 190/276 Loss:0.00942 con1:0.00000 con2: 0.00000 recon1:0.00462 recon2:0.00479
	Batch 200/276 Loss:0.01476 con1:0.00001 con2: 0.00000 recon1:0.00776 recon2:0.00699
	Batch 210/276 Loss:0.01753 con1:0.00001 con2: 0.00001 recon1:0.00871 recon2:0.00882
	Batch 220/276 Loss:0.01053 con1:0.00000 con2: 0.00000 recon1:0.00543 recon2:0.00510
	Batch 230/276 Loss:0.04268 con1:0.00002 con2: 0.00002 recon1:0.01962 recon2:0.02301
	Batch 240/276 Loss:0.00923 con1:0.00000 con2: 0.00000 recon1:0.00456 recon2:0.00467
	Batch 250/276 Loss:0.00974 con1:0.00001 con2: 0.00000 recon1:0.00480 recon2:0.00493
	Batch 260/276 Loss:0.00910 con1:0.00000 con2: 0.00000 recon1:0.00446 recon2:0.00463
	Batch 270/276 Loss:0.01256 con1:0.00001 con2: 0.00001 recon1:0.00617 recon2:0.00638
Validation Epoch 3/1000 Loss:0.01357 con1:0.00001 con2:0.00001 recon1:0.00673 recon2:0.00682
Training...
	Batch 10/672 Loss:0.01909 con1:0.00001 con2:0.00001 recon1:0.01053 recon2:0.00855
	Batch 20/672 Loss:0.01846 con1:0.00001 con2:0.00001 recon1:0.00915 recon2:0.00930
	Batch 30/672 Loss:0.01620 con1:0.00001 con2:0.00001 recon1:0.00826 recon2:0.00793
	Batch 40/672 Loss:0.01299 con1:0.00000 con2:0.00001 recon1:0.00647 recon2:0.00651
	Batch 50/672 Loss:0.01479 con1:0.00001 con2:0.00001 recon1:0.00724 recon2:0.00754
	Batch 60/672 Loss:0.01269 con1:0.00001 con2:0.00000 recon1:0.00654 recon2:0.00615
	Batch 70/672 Loss:0.01357 con1:0.00001 con2:0.00001 recon1:0.00635 recon2:0.00721
	Batch 80/672 Loss:0.01372 con1:0.00001 con2:0.00001 recon1:0.00676 recon2:0.00695
	Batch 90/672 Loss:0.01306 con1:0.00000 con2:0.00000 recon1:0.00647 recon2:0.00658
	Batch 100/672 Loss:0.01180 con1:0.00001 con2:0.00001 recon1:0.00606 recon2:0.00572
	Batch 110/672 Loss:0.01110 con1:0.00002 con2:0.00002 recon1:0.00559 recon2:0.00547
	Batch 120/672 Loss:0.01323 con1:0.00001 con2:0.00001 recon1:0.00637 recon2:0.00686
	Batch 130/672 Loss:0.01188 con1:0.00000 con2:0.00000 recon1:0.00601 recon2:0.00585
	Batch 140/672 Loss:0.01277 con1:0.00000 con2:0.00000 recon1:0.00649 recon2:0.00627
	Batch 150/672 Loss:0.01376 con1:0.00001 con2:0.00001 recon1:0.00647 recon2:0.00726
	Batch 160/672 Loss:0.01341 con1:0.00001 con2:0.00001 recon1:0.00657 recon2:0.00682
	Batch 170/672 Loss:0.01382 con1:0.00000 con2:0.00000 recon1:0.00717 recon2:0.00665
	Batch 180/672 Loss:0.01258 con1:0.00000 con2:0.00000 recon1:0.00624 recon2:0.00634
	Batch 190/672 Loss:0.01308 con1:0.00001 con2:0.00001 recon1:0.00685 recon2:0.00622
	Batch 200/672 Loss:0.01361 con1:0.00001 con2:0.00001 recon1:0.00687 recon2:0.00673
	Batch 210/672 Loss:0.01743 con1:0.00001 con2:0.00002 recon1:0.00850 recon2:0.00890
	Batch 220/672 Loss:0.01574 con1:0.00001 con2:0.00001 recon1:0.00808 recon2:0.00765
	Batch 230/672 Loss:0.01955 con1:0.00003 con2:0.00002 recon1:0.00951 recon2:0.00999
	Batch 240/672 Loss:0.01479 con1:0.00001 con2:0.00001 recon1:0.00719 recon2:0.00759
	Batch 250/672 Loss:0.01260 con1:0.00001 con2:0.00000 recon1:0.00642 recon2:0.00617
	Batch 260/672 Loss:0.01351 con1:0.00001 con2:0.00001 recon1:0.00686 recon2:0.00663
	Batch 270/672 Loss:0.01162 con1:0.00001 con2:0.00001 recon1:0.00604 recon2:0.00557
	Batch 280/672 Loss:0.02123 con1:0.00001 con2:0.00001 recon1:0.00985 recon2:0.01136
	Batch 290/672 Loss:0.01485 con1:0.00001 con2:0.00001 recon1:0.00697 recon2:0.00787
	Batch 300/672 Loss:0.01311 con1:0.00000 con2:0.00000 recon1:0.00620 recon2:0.00690
	Batch 310/672 Loss:0.01630 con1:0.00001 con2:0.00002 recon1:0.00815 recon2:0.00813
	Batch 320/672 Loss:0.01230 con1:0.00000 con2:0.00000 recon1:0.00612 recon2:0.00617
	Batch 330/672 Loss:0.01424 con1:0.00001 con2:0.00001 recon1:0.00674 recon2:0.00749
	Batch 340/672 Loss:0.01327 con1:0.00002 con2:0.00001 recon1:0.00674 recon2:0.00649
	Batch 350/672 Loss:0.01712 con1:0.00001 con2:0.00001 recon1:0.00848 recon2:0.00862
	Batch 360/672 Loss:0.01530 con1:0.00001 con2:0.00001 recon1:0.00807 recon2:0.00721
	Batch 370/672 Loss:0.01387 con1:0.00002 con2:0.00002 recon1:0.00671 recon2:0.00712
	Batch 380/672 Loss:0.01786 con1:0.00002 con2:0.00001 recon1:0.00866 recon2:0.00917
	Batch 390/672 Loss:0.01210 con1:0.00001 con2:0.00000 recon1:0.00598 recon2:0.00611
	Batch 400/672 Loss:0.01196 con1:0.00000 con2:0.00000 recon1:0.00590 recon2:0.00605
	Batch 410/672 Loss:0.01613 con1:0.00001 con2:0.00001 recon1:0.00797 recon2:0.00815
	Batch 420/672 Loss:0.01303 con1:0.00001 con2:0.00001 recon1:0.00653 recon2:0.00649
	Batch 430/672 Loss:0.01251 con1:0.00001 con2:0.00001 recon1:0.00602 recon2:0.00647
	Batch 440/672 Loss:0.01511 con1:0.00001 con2:0.00001 recon1:0.00780 recon2:0.00729
	Batch 450/672 Loss:0.01251 con1:0.00001 con2:0.00001 recon1:0.00637 recon2:0.00612
	Batch 460/672 Loss:0.01632 con1:0.00001 con2:0.00001 recon1:0.00793 recon2:0.00836
	Batch 470/672 Loss:0.01685 con1:0.00001 con2:0.00001 recon1:0.00871 recon2:0.00812
	Batch 480/672 Loss:0.01523 con1:0.00000 con2:0.00000 recon1:0.00756 recon2:0.00767
	Batch 490/672 Loss:0.01942 con1:0.00000 con2:0.00000 recon1:0.00829 recon2:0.01112
	Batch 500/672 Loss:0.01825 con1:0.00001 con2:0.00001 recon1:0.00901 recon2:0.00922
	Batch 510/672 Loss:0.01612 con1:0.00000 con2:0.00001 recon1:0.00763 recon2:0.00848
	Batch 520/672 Loss:0.01617 con1:0.00001 con2:0.00001 recon1:0.00840 recon2:0.00775
	Batch 530/672 Loss:0.01585 con1:0.00000 con2:0.00001 recon1:0.00811 recon2:0.00772
	Batch 540/672 Loss:0.01401 con1:0.00001 con2:0.00000 recon1:0.00727 recon2:0.00674
	Batch 550/672 Loss:0.01332 con1:0.00001 con2:0.00000 recon1:0.00675 recon2:0.00657
	Batch 560/672 Loss:0.01351 con1:0.00001 con2:0.00001 recon1:0.00655 recon2:0.00695
	Batch 570/672 Loss:0.01354 con1:0.00001 con2:0.00001 recon1:0.00668 recon2:0.00685
	Batch 580/672 Loss:0.01432 con1:0.00001 con2:0.00001 recon1:0.00744 recon2:0.00686
	Batch 590/672 Loss:0.01350 con1:0.00000 con2:0.00000 recon1:0.00636 recon2:0.00713
	Batch 600/672 Loss:0.01266 con1:0.00000 con2:0.00000 recon1:0.00638 recon2:0.00627
	Batch 610/672 Loss:0.01708 con1:0.00000 con2:0.00000 recon1:0.00935 recon2:0.00772
	Batch 620/672 Loss:0.01643 con1:0.00004 con2:0.00003 recon1:0.00858 recon2:0.00778
	Batch 630/672 Loss:0.01748 con1:0.00001 con2:0.00002 recon1:0.00870 recon2:0.00875
	Batch 640/672 Loss:0.01283 con1:0.00000 con2:0.00000 recon1:0.00619 recon2:0.00664
	Batch 650/672 Loss:0.01519 con1:0.00000 con2:0.00000 recon1:0.00749 recon2:0.00769
	Batch 660/672 Loss:0.01428 con1:0.00000 con2:0.00001 recon1:0.00750 recon2:0.00677
	Batch 670/672 Loss:0.01481 con1:0.00001 con2:0.00001 recon1:0.00762 recon2:0.00718
Training Epoch 4/1000 Loss:0.01436 con1:0.00001 con2:0.00001 recon1:0.00715 recon2:0.00719
Validation...
	Batch 10/276 Loss:0.01007 con1:0.00000 con2: 0.00000 recon1:0.00498 recon2:0.00508
	Batch 20/276 Loss:0.00939 con1:0.00000 con2: 0.00000 recon1:0.00473 recon2:0.00466
	Batch 30/276 Loss:0.01005 con1:0.00000 con2: 0.00000 recon1:0.00513 recon2:0.00491
	Batch 40/276 Loss:0.01830 con1:0.00004 con2: 0.00003 recon1:0.00922 recon2:0.00900
	Batch 50/276 Loss:0.01085 con1:0.00000 con2: 0.00000 recon1:0.00558 recon2:0.00527
	Batch 60/276 Loss:0.01121 con1:0.00000 con2: 0.00000 recon1:0.00545 recon2:0.00575
	Batch 70/276 Loss:0.01266 con1:0.00001 con2: 0.00000 recon1:0.00599 recon2:0.00667
	Batch 80/276 Loss:0.01353 con1:0.00000 con2: 0.00000 recon1:0.00642 recon2:0.00710
	Batch 90/276 Loss:0.00773 con1:0.00000 con2: 0.00000 recon1:0.00381 recon2:0.00391
	Batch 100/276 Loss:0.00929 con1:0.00000 con2: 0.00000 recon1:0.00463 recon2:0.00465
	Batch 110/276 Loss:0.00796 con1:0.00000 con2: 0.00000 recon1:0.00410 recon2:0.00386
	Batch 120/276 Loss:0.01478 con1:0.00000 con2: 0.00001 recon1:0.00673 recon2:0.00804
	Batch 130/276 Loss:0.01053 con1:0.00000 con2: 0.00001 recon1:0.00517 recon2:0.00535
	Batch 140/276 Loss:0.01705 con1:0.00000 con2: 0.00000 recon1:0.00845 recon2:0.00859
	Batch 150/276 Loss:0.01222 con1:0.00000 con2: 0.00000 recon1:0.00626 recon2:0.00595
	Batch 160/276 Loss:0.01947 con1:0.00001 con2: 0.00001 recon1:0.00981 recon2:0.00964
	Batch 170/276 Loss:0.01523 con1:0.00001 con2: 0.00002 recon1:0.00724 recon2:0.00796
	Batch 180/276 Loss:0.01045 con1:0.00000 con2: 0.00000 recon1:0.00511 recon2:0.00533
	Batch 190/276 Loss:0.00904 con1:0.00000 con2: 0.00000 recon1:0.00443 recon2:0.00461
	Batch 200/276 Loss:0.01433 con1:0.00000 con2: 0.00000 recon1:0.00753 recon2:0.00679
	Batch 210/276 Loss:0.01705 con1:0.00001 con2: 0.00001 recon1:0.00847 recon2:0.00856
	Batch 220/276 Loss:0.01013 con1:0.00000 con2: 0.00000 recon1:0.00522 recon2:0.00490
	Batch 230/276 Loss:0.04117 con1:0.00004 con2: 0.00007 recon1:0.01885 recon2:0.02221
	Batch 240/276 Loss:0.00880 con1:0.00000 con2: 0.00000 recon1:0.00435 recon2:0.00444
	Batch 250/276 Loss:0.00934 con1:0.00000 con2: 0.00000 recon1:0.00462 recon2:0.00471
	Batch 260/276 Loss:0.00870 con1:0.00000 con2: 0.00000 recon1:0.00427 recon2:0.00443
	Batch 270/276 Loss:0.01209 con1:0.00000 con2: 0.00000 recon1:0.00594 recon2:0.00615
Validation Epoch 4/1000 Loss:0.01307 con1:0.00001 con2:0.00001 recon1:0.00648 recon2:0.00657
Training...
	Batch 10/672 Loss:0.01390 con1:0.00000 con2:0.00000 recon1:0.00696 recon2:0.00693
	Batch 20/672 Loss:0.01708 con1:0.00002 con2:0.00001 recon1:0.00791 recon2:0.00914
	Batch 30/672 Loss:0.01190 con1:0.00000 con2:0.00000 recon1:0.00619 recon2:0.00570
	Batch 40/672 Loss:0.01139 con1:0.00001 con2:0.00001 recon1:0.00566 recon2:0.00572
	Batch 50/672 Loss:0.01452 con1:0.00000 con2:0.00000 recon1:0.00730 recon2:0.00722
	Batch 60/672 Loss:0.01272 con1:0.00000 con2:0.00000 recon1:0.00594 recon2:0.00678
	Batch 70/672 Loss:0.01468 con1:0.00001 con2:0.00001 recon1:0.00660 recon2:0.00806
	Batch 80/672 Loss:0.01722 con1:0.00000 con2:0.00001 recon1:0.00853 recon2:0.00869
	Batch 90/672 Loss:0.01453 con1:0.00001 con2:0.00001 recon1:0.00747 recon2:0.00705
	Batch 100/672 Loss:0.01591 con1:0.00001 con2:0.00001 recon1:0.00791 recon2:0.00799
	Batch 110/672 Loss:0.01544 con1:0.00001 con2:0.00001 recon1:0.00763 recon2:0.00780
	Batch 120/672 Loss:0.01406 con1:0.00000 con2:0.00000 recon1:0.00724 recon2:0.00681
	Batch 130/672 Loss:0.01202 con1:0.00002 con2:0.00002 recon1:0.00620 recon2:0.00578
	Batch 140/672 Loss:0.01311 con1:0.00001 con2:0.00001 recon1:0.00648 recon2:0.00661
	Batch 150/672 Loss:0.01588 con1:0.00001 con2:0.00001 recon1:0.00844 recon2:0.00742
	Batch 160/672 Loss:0.01280 con1:0.00000 con2:0.00000 recon1:0.00638 recon2:0.00641
	Batch 170/672 Loss:0.01333 con1:0.00001 con2:0.00001 recon1:0.00684 recon2:0.00647
	Batch 180/672 Loss:0.01361 con1:0.00000 con2:0.00000 recon1:0.00680 recon2:0.00680
	Batch 190/672 Loss:0.01170 con1:0.00001 con2:0.00001 recon1:0.00578 recon2:0.00591
	Batch 200/672 Loss:0.01234 con1:0.00001 con2:0.00000 recon1:0.00602 recon2:0.00631
	Batch 210/672 Loss:0.01568 con1:0.00001 con2:0.00001 recon1:0.00760 recon2:0.00806
	Batch 220/672 Loss:0.01172 con1:0.00000 con2:0.00001 recon1:0.00528 recon2:0.00643
	Batch 230/672 Loss:0.01415 con1:0.00001 con2:0.00000 recon1:0.00740 recon2:0.00673
	Batch 240/672 Loss:0.01583 con1:0.00000 con2:0.00001 recon1:0.00821 recon2:0.00761
	Batch 250/672 Loss:0.01587 con1:0.00000 con2:0.00001 recon1:0.00793 recon2:0.00793
	Batch 260/672 Loss:0.01569 con1:0.00001 con2:0.00001 recon1:0.00745 recon2:0.00822
	Batch 270/672 Loss:0.01247 con1:0.00000 con2:0.00000 recon1:0.00647 recon2:0.00599
	Batch 280/672 Loss:0.01347 con1:0.00000 con2:0.00001 recon1:0.00677 recon2:0.00669
	Batch 290/672 Loss:0.01410 con1:0.00000 con2:0.00000 recon1:0.00748 recon2:0.00662
	Batch 300/672 Loss:0.01458 con1:0.00000 con2:0.00001 recon1:0.00708 recon2:0.00749
	Batch 310/672 Loss:0.01420 con1:0.00001 con2:0.00001 recon1:0.00732 recon2:0.00685
	Batch 320/672 Loss:0.01683 con1:0.00000 con2:0.00000 recon1:0.00869 recon2:0.00814
	Batch 330/672 Loss:0.01540 con1:0.00001 con2:0.00001 recon1:0.00795 recon2:0.00744
	Batch 340/672 Loss:0.01430 con1:0.00001 con2:0.00001 recon1:0.00706 recon2:0.00722
	Batch 350/672 Loss:0.01049 con1:0.00001 con2:0.00001 recon1:0.00506 recon2:0.00541
	Batch 360/672 Loss:0.01391 con1:0.00000 con2:0.00001 recon1:0.00728 recon2:0.00662
	Batch 370/672 Loss:0.01201 con1:0.00000 con2:0.00000 recon1:0.00639 recon2:0.00562
	Batch 380/672 Loss:0.01325 con1:0.00000 con2:0.00001 recon1:0.00658 recon2:0.00667
	Batch 390/672 Loss:0.01273 con1:0.00000 con2:0.00000 recon1:0.00665 recon2:0.00607
	Batch 400/672 Loss:0.01191 con1:0.00001 con2:0.00001 recon1:0.00612 recon2:0.00578
	Batch 410/672 Loss:0.01366 con1:0.00001 con2:0.00000 recon1:0.00686 recon2:0.00679
	Batch 420/672 Loss:0.01475 con1:0.00001 con2:0.00000 recon1:0.00774 recon2:0.00700
	Batch 430/672 Loss:0.01186 con1:0.00000 con2:0.00000 recon1:0.00563 recon2:0.00622
	Batch 440/672 Loss:0.01444 con1:0.00001 con2:0.00001 recon1:0.00701 recon2:0.00741
	Batch 450/672 Loss:0.01593 con1:0.00001 con2:0.00001 recon1:0.00884 recon2:0.00707
	Batch 460/672 Loss:0.01355 con1:0.00001 con2:0.00001 recon1:0.00654 recon2:0.00699
	Batch 470/672 Loss:0.01134 con1:0.00001 con2:0.00001 recon1:0.00572 recon2:0.00560
	Batch 480/672 Loss:0.01380 con1:0.00001 con2:0.00001 recon1:0.00717 recon2:0.00663
	Batch 490/672 Loss:0.01375 con1:0.00001 con2:0.00001 recon1:0.00682 recon2:0.00691
	Batch 500/672 Loss:0.01239 con1:0.00001 con2:0.00001 recon1:0.00626 recon2:0.00611
	Batch 510/672 Loss:0.01778 con1:0.00001 con2:0.00001 recon1:0.00912 recon2:0.00864
	Batch 520/672 Loss:0.01111 con1:0.00001 con2:0.00000 recon1:0.00536 recon2:0.00574
	Batch 530/672 Loss:0.01087 con1:0.00000 con2:0.00000 recon1:0.00556 recon2:0.00530
	Batch 540/672 Loss:0.01622 con1:0.00000 con2:0.00001 recon1:0.00796 recon2:0.00824
	Batch 550/672 Loss:0.01205 con1:0.00001 con2:0.00001 recon1:0.00614 recon2:0.00590
	Batch 560/672 Loss:0.01285 con1:0.00000 con2:0.00000 recon1:0.00626 recon2:0.00659
	Batch 570/672 Loss:0.01427 con1:0.00000 con2:0.00000 recon1:0.00706 recon2:0.00720
	Batch 580/672 Loss:0.01229 con1:0.00001 con2:0.00001 recon1:0.00580 recon2:0.00648
	Batch 590/672 Loss:0.01237 con1:0.00001 con2:0.00001 recon1:0.00656 recon2:0.00579
	Batch 600/672 Loss:0.01475 con1:0.00001 con2:0.00000 recon1:0.00870 recon2:0.00604
	Batch 610/672 Loss:0.01772 con1:0.00001 con2:0.00001 recon1:0.00781 recon2:0.00989
	Batch 620/672 Loss:0.01397 con1:0.00000 con2:0.00000 recon1:0.00740 recon2:0.00656
	Batch 630/672 Loss:0.01227 con1:0.00000 con2:0.00000 recon1:0.00587 recon2:0.00639
	Batch 640/672 Loss:0.01431 con1:0.00001 con2:0.00001 recon1:0.00752 recon2:0.00677
	Batch 650/672 Loss:0.01645 con1:0.00001 con2:0.00001 recon1:0.00881 recon2:0.00763
	Batch 660/672 Loss:0.02011 con1:0.00002 con2:0.00002 recon1:0.01057 recon2:0.00950
	Batch 670/672 Loss:0.01396 con1:0.00001 con2:0.00001 recon1:0.00679 recon2:0.00715
Training Epoch 5/1000 Loss:0.01389 con1:0.00001 con2:0.00001 recon1:0.00694 recon2:0.00694
Validation...
	Batch 10/276 Loss:0.00972 con1:0.00000 con2: 0.00000 recon1:0.00479 recon2:0.00492
	Batch 20/276 Loss:0.00911 con1:0.00000 con2: 0.00000 recon1:0.00459 recon2:0.00452
	Batch 30/276 Loss:0.00974 con1:0.00000 con2: 0.00000 recon1:0.00498 recon2:0.00475
	Batch 40/276 Loss:0.01743 con1:0.00002 con2: 0.00002 recon1:0.00880 recon2:0.00860
	Batch 50/276 Loss:0.01050 con1:0.00000 con2: 0.00000 recon1:0.00540 recon2:0.00510
	Batch 60/276 Loss:0.01091 con1:0.00000 con2: 0.00000 recon1:0.00531 recon2:0.00559
	Batch 70/276 Loss:0.01237 con1:0.00001 con2: 0.00000 recon1:0.00583 recon2:0.00653
	Batch 80/276 Loss:0.01336 con1:0.00000 con2: 0.00000 recon1:0.00634 recon2:0.00701
	Batch 90/276 Loss:0.00755 con1:0.00000 con2: 0.00000 recon1:0.00373 recon2:0.00382
	Batch 100/276 Loss:0.00903 con1:0.00000 con2: 0.00000 recon1:0.00451 recon2:0.00451
	Batch 110/276 Loss:0.00776 con1:0.00000 con2: 0.00000 recon1:0.00399 recon2:0.00377
	Batch 120/276 Loss:0.01438 con1:0.00000 con2: 0.00000 recon1:0.00657 recon2:0.00781
	Batch 130/276 Loss:0.01016 con1:0.00000 con2: 0.00000 recon1:0.00499 recon2:0.00516
	Batch 140/276 Loss:0.01666 con1:0.00000 con2: 0.00000 recon1:0.00825 recon2:0.00839
	Batch 150/276 Loss:0.01179 con1:0.00000 con2: 0.00000 recon1:0.00602 recon2:0.00576
	Batch 160/276 Loss:0.01874 con1:0.00001 con2: 0.00001 recon1:0.00945 recon2:0.00928
	Batch 170/276 Loss:0.01450 con1:0.00001 con2: 0.00002 recon1:0.00689 recon2:0.00758
	Batch 180/276 Loss:0.01013 con1:0.00000 con2: 0.00000 recon1:0.00495 recon2:0.00517
	Batch 190/276 Loss:0.00880 con1:0.00000 con2: 0.00000 recon1:0.00432 recon2:0.00447
	Batch 200/276 Loss:0.01410 con1:0.00000 con2: 0.00000 recon1:0.00743 recon2:0.00666
	Batch 210/276 Loss:0.01676 con1:0.00000 con2: 0.00000 recon1:0.00833 recon2:0.00841
	Batch 220/276 Loss:0.00984 con1:0.00000 con2: 0.00000 recon1:0.00505 recon2:0.00479
	Batch 230/276 Loss:0.03971 con1:0.00005 con2: 0.00006 recon1:0.01825 recon2:0.02135
	Batch 240/276 Loss:0.00850 con1:0.00000 con2: 0.00000 recon1:0.00418 recon2:0.00431
	Batch 250/276 Loss:0.00914 con1:0.00000 con2: 0.00000 recon1:0.00451 recon2:0.00462
	Batch 260/276 Loss:0.00842 con1:0.00000 con2: 0.00000 recon1:0.00413 recon2:0.00429
	Batch 270/276 Loss:0.01179 con1:0.00000 con2: 0.00000 recon1:0.00582 recon2:0.00597
Validation Epoch 5/1000 Loss:0.01267 con1:0.00001 con2:0.00001 recon1:0.00628 recon2:0.00637
Training...
	Batch 10/672 Loss:0.01268 con1:0.00000 con2:0.00000 recon1:0.00670 recon2:0.00597
	Batch 20/672 Loss:0.01725 con1:0.00000 con2:0.00001 recon1:0.00931 recon2:0.00794
	Batch 30/672 Loss:0.01932 con1:0.00001 con2:0.00001 recon1:0.00997 recon2:0.00933
	Batch 40/672 Loss:0.01585 con1:0.00002 con2:0.00003 recon1:0.00779 recon2:0.00802
	Batch 50/672 Loss:0.01491 con1:0.00000 con2:0.00000 recon1:0.00728 recon2:0.00762
	Batch 60/672 Loss:0.01101 con1:0.00001 con2:0.00001 recon1:0.00523 recon2:0.00576
	Batch 70/672 Loss:0.01265 con1:0.00000 con2:0.00000 recon1:0.00647 recon2:0.00618
	Batch 80/672 Loss:0.01488 con1:0.00000 con2:0.00000 recon1:0.00715 recon2:0.00772
	Batch 90/672 Loss:0.01516 con1:0.00001 con2:0.00001 recon1:0.00837 recon2:0.00676
	Batch 100/672 Loss:0.01330 con1:0.00000 con2:0.00000 recon1:0.00679 recon2:0.00650
	Batch 110/672 Loss:0.01484 con1:0.00001 con2:0.00001 recon1:0.00792 recon2:0.00691
	Batch 120/672 Loss:0.01192 con1:0.00000 con2:0.00000 recon1:0.00613 recon2:0.00578
	Batch 130/672 Loss:0.01301 con1:0.00000 con2:0.00000 recon1:0.00634 recon2:0.00666
	Batch 140/672 Loss:0.01461 con1:0.00001 con2:0.00001 recon1:0.00674 recon2:0.00785
	Batch 150/672 Loss:0.01199 con1:0.00001 con2:0.00001 recon1:0.00585 recon2:0.00613
	Batch 160/672 Loss:0.01298 con1:0.00001 con2:0.00001 recon1:0.00699 recon2:0.00597
	Batch 170/672 Loss:0.01552 con1:0.00001 con2:0.00001 recon1:0.00683 recon2:0.00868
	Batch 180/672 Loss:0.01502 con1:0.00001 con2:0.00001 recon1:0.00773 recon2:0.00727
	Batch 190/672 Loss:0.01401 con1:0.00001 con2:0.00001 recon1:0.00695 recon2:0.00704
	Batch 200/672 Loss:0.01433 con1:0.00001 con2:0.00001 recon1:0.00628 recon2:0.00803
	Batch 210/672 Loss:0.01216 con1:0.00000 con2:0.00000 recon1:0.00579 recon2:0.00636
	Batch 220/672 Loss:0.01436 con1:0.00001 con2:0.00001 recon1:0.00699 recon2:0.00735
	Batch 230/672 Loss:0.01068 con1:0.00000 con2:0.00001 recon1:0.00506 recon2:0.00560
	Batch 240/672 Loss:0.01279 con1:0.00001 con2:0.00001 recon1:0.00634 recon2:0.00644
	Batch 250/672 Loss:0.01431 con1:0.00001 con2:0.00001 recon1:0.00730 recon2:0.00700
	Batch 260/672 Loss:0.01354 con1:0.00001 con2:0.00001 recon1:0.00676 recon2:0.00677
	Batch 270/672 Loss:0.01222 con1:0.00001 con2:0.00001 recon1:0.00590 recon2:0.00631
	Batch 280/672 Loss:0.01179 con1:0.00001 con2:0.00000 recon1:0.00543 recon2:0.00635
	Batch 290/672 Loss:0.01426 con1:0.00001 con2:0.00001 recon1:0.00712 recon2:0.00713
	Batch 300/672 Loss:0.01189 con1:0.00000 con2:0.00000 recon1:0.00590 recon2:0.00598
	Batch 310/672 Loss:0.01653 con1:0.00000 con2:0.00001 recon1:0.00758 recon2:0.00893
	Batch 320/672 Loss:0.01229 con1:0.00001 con2:0.00001 recon1:0.00620 recon2:0.00608
	Batch 330/672 Loss:0.01173 con1:0.00001 con2:0.00001 recon1:0.00581 recon2:0.00591
	Batch 340/672 Loss:0.01446 con1:0.00001 con2:0.00001 recon1:0.00745 recon2:0.00698
	Batch 350/672 Loss:0.01144 con1:0.00000 con2:0.00000 recon1:0.00560 recon2:0.00583
	Batch 360/672 Loss:0.01135 con1:0.00001 con2:0.00001 recon1:0.00582 recon2:0.00551
	Batch 370/672 Loss:0.00990 con1:0.00000 con2:0.00000 recon1:0.00495 recon2:0.00494
	Batch 380/672 Loss:0.01192 con1:0.00000 con2:0.00000 recon1:0.00563 recon2:0.00628
	Batch 390/672 Loss:0.01500 con1:0.00001 con2:0.00001 recon1:0.00796 recon2:0.00704
	Batch 400/672 Loss:0.01201 con1:0.00001 con2:0.00001 recon1:0.00605 recon2:0.00595
	Batch 410/672 Loss:0.01336 con1:0.00001 con2:0.00001 recon1:0.00706 recon2:0.00628
	Batch 420/672 Loss:0.01401 con1:0.00001 con2:0.00001 recon1:0.00701 recon2:0.00699
	Batch 430/672 Loss:0.01547 con1:0.00001 con2:0.00001 recon1:0.00756 recon2:0.00789
	Batch 440/672 Loss:0.01359 con1:0.00000 con2:0.00001 recon1:0.00628 recon2:0.00730
	Batch 450/672 Loss:0.01394 con1:0.00001 con2:0.00000 recon1:0.00713 recon2:0.00680
	Batch 460/672 Loss:0.01116 con1:0.00000 con2:0.00000 recon1:0.00566 recon2:0.00549
	Batch 470/672 Loss:0.01028 con1:0.00000 con2:0.00000 recon1:0.00535 recon2:0.00492
	Batch 480/672 Loss:0.01588 con1:0.00001 con2:0.00001 recon1:0.00771 recon2:0.00816
	Batch 490/672 Loss:0.01392 con1:0.00001 con2:0.00001 recon1:0.00645 recon2:0.00746
	Batch 500/672 Loss:0.01224 con1:0.00000 con2:0.00000 recon1:0.00618 recon2:0.00606
	Batch 510/672 Loss:0.01559 con1:0.00001 con2:0.00000 recon1:0.00864 recon2:0.00694
	Batch 520/672 Loss:0.01264 con1:0.00000 con2:0.00000 recon1:0.00676 recon2:0.00587
	Batch 530/672 Loss:0.01451 con1:0.00001 con2:0.00001 recon1:0.00754 recon2:0.00695
	Batch 540/672 Loss:0.01451 con1:0.00001 con2:0.00002 recon1:0.00712 recon2:0.00735
	Batch 550/672 Loss:0.01643 con1:0.00001 con2:0.00001 recon1:0.00797 recon2:0.00845
	Batch 560/672 Loss:0.01204 con1:0.00001 con2:0.00001 recon1:0.00600 recon2:0.00602
	Batch 570/672 Loss:0.01689 con1:0.00001 con2:0.00002 recon1:0.00788 recon2:0.00899
	Batch 580/672 Loss:0.01091 con1:0.00000 con2:0.00001 recon1:0.00535 recon2:0.00555
	Batch 590/672 Loss:0.01383 con1:0.00000 con2:0.00001 recon1:0.00680 recon2:0.00702
	Batch 600/672 Loss:0.01239 con1:0.00001 con2:0.00001 recon1:0.00589 recon2:0.00649
	Batch 610/672 Loss:0.01258 con1:0.00001 con2:0.00001 recon1:0.00584 recon2:0.00673
	Batch 620/672 Loss:0.01684 con1:0.00001 con2:0.00001 recon1:0.00869 recon2:0.00813
	Batch 630/672 Loss:0.01581 con1:0.00001 con2:0.00001 recon1:0.00676 recon2:0.00903
	Batch 640/672 Loss:0.01022 con1:0.00000 con2:0.00000 recon1:0.00501 recon2:0.00521
	Batch 650/672 Loss:0.01492 con1:0.00000 con2:0.00001 recon1:0.00614 recon2:0.00877
	Batch 660/672 Loss:0.01198 con1:0.00001 con2:0.00000 recon1:0.00594 recon2:0.00603
	Batch 670/672 Loss:0.01565 con1:0.00002 con2:0.00002 recon1:0.00803 recon2:0.00758
Training Epoch 6/1000 Loss:0.01341 con1:0.00001 con2:0.00001 recon1:0.00670 recon2:0.00670
Validation...
	Batch 10/276 Loss:0.00949 con1:0.00001 con2: 0.00000 recon1:0.00466 recon2:0.00482
	Batch 20/276 Loss:0.00892 con1:0.00000 con2: 0.00000 recon1:0.00447 recon2:0.00444
	Batch 30/276 Loss:0.00948 con1:0.00000 con2: 0.00000 recon1:0.00485 recon2:0.00463
	Batch 40/276 Loss:0.01675 con1:0.00002 con2: 0.00001 recon1:0.00845 recon2:0.00827
	Batch 50/276 Loss:0.01029 con1:0.00000 con2: 0.00000 recon1:0.00526 recon2:0.00502
	Batch 60/276 Loss:0.01066 con1:0.00000 con2: 0.00000 recon1:0.00518 recon2:0.00547
	Batch 70/276 Loss:0.01215 con1:0.00001 con2: 0.00001 recon1:0.00572 recon2:0.00641
	Batch 80/276 Loss:0.01321 con1:0.00000 con2: 0.00000 recon1:0.00627 recon2:0.00692
	Batch 90/276 Loss:0.00749 con1:0.00000 con2: 0.00000 recon1:0.00370 recon2:0.00378
	Batch 100/276 Loss:0.00886 con1:0.00000 con2: 0.00000 recon1:0.00444 recon2:0.00441
	Batch 110/276 Loss:0.00764 con1:0.00000 con2: 0.00000 recon1:0.00393 recon2:0.00370
	Batch 120/276 Loss:0.01404 con1:0.00000 con2: 0.00001 recon1:0.00640 recon2:0.00763
	Batch 130/276 Loss:0.00985 con1:0.00000 con2: 0.00000 recon1:0.00485 recon2:0.00500
	Batch 140/276 Loss:0.01646 con1:0.00000 con2: 0.00000 recon1:0.00814 recon2:0.00832
	Batch 150/276 Loss:0.01145 con1:0.00000 con2: 0.00000 recon1:0.00586 recon2:0.00559
	Batch 160/276 Loss:0.01832 con1:0.00001 con2: 0.00001 recon1:0.00918 recon2:0.00913
	Batch 170/276 Loss:0.01402 con1:0.00001 con2: 0.00001 recon1:0.00665 recon2:0.00735
	Batch 180/276 Loss:0.00984 con1:0.00000 con2: 0.00000 recon1:0.00481 recon2:0.00503
	Batch 190/276 Loss:0.00861 con1:0.00000 con2: 0.00000 recon1:0.00425 recon2:0.00436
	Batch 200/276 Loss:0.01393 con1:0.00001 con2: 0.00000 recon1:0.00735 recon2:0.00657
	Batch 210/276 Loss:0.01652 con1:0.00001 con2: 0.00001 recon1:0.00818 recon2:0.00834
	Batch 220/276 Loss:0.00969 con1:0.00000 con2: 0.00000 recon1:0.00495 recon2:0.00473
	Batch 230/276 Loss:0.03859 con1:0.00006 con2: 0.00008 recon1:0.01791 recon2:0.02054
	Batch 240/276 Loss:0.00826 con1:0.00000 con2: 0.00000 recon1:0.00405 recon2:0.00421
	Batch 250/276 Loss:0.00890 con1:0.00000 con2: 0.00000 recon1:0.00440 recon2:0.00449
	Batch 260/276 Loss:0.00828 con1:0.00000 con2: 0.00000 recon1:0.00405 recon2:0.00423
	Batch 270/276 Loss:0.01160 con1:0.00001 con2: 0.00001 recon1:0.00574 recon2:0.00584
Validation Epoch 6/1000 Loss:0.01238 con1:0.00001 con2:0.00001 recon1:0.00614 recon2:0.00622
Training...
