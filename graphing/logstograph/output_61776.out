Parameters for training:
Batch Size: 32
Tensor Size: (3,8,112,112)
Skip Length: 2
Precrop: False
Total Epochs: 1000
Learning Rate: 0.0001
FullNetwork(
  (vgg): VGG(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU(inplace)
      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU(inplace)
      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (6): ReLU(inplace)
      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (8): ReLU(inplace)
      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (11): ReLU(inplace)
      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (13): ReLU(inplace)
      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (15): ReLU(inplace)
      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (18): ReLU(inplace)
      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (20): ReLU(inplace)
      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (22): ReLU(inplace)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (25): ReLU(inplace)
      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (27): ReLU(inplace)
      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (29): ReLU(inplace)
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
    (classifier): Sequential(
      (0): Linear(in_features=25088, out_features=4096, bias=True)
      (1): ReLU(inplace)
      (2): Dropout(p=0.5)
      (3): Linear(in_features=4096, out_features=4096, bias=True)
      (4): ReLU(inplace)
      (5): Dropout(p=0.5)
      (6): Linear(in_features=4096, out_features=1000, bias=True)
    )
  )
  (i3d): InceptionI3d(
    (logits): Unit3D(
      (conv3d): Conv3d(1024, 157, kernel_size=[1, 1, 1], stride=(1, 1, 1))
    )
    (Conv3d_1a_7x7): Unit3D(
      (conv3d): Conv3d(3, 64, kernel_size=[7, 7, 7], stride=(2, 2, 2), bias=False)
      (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
    )
    (MaxPool3d_2a_3x3): MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)
    (Conv3d_2b_1x1): Unit3D(
      (conv3d): Conv3d(64, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
      (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
    )
    (Conv3d_2c_3x3): Unit3D(
      (conv3d): Conv3d(64, 192, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
      (bn): BatchNorm3d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
    )
    (MaxPool3d_3a_3x3): MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)
    (Mixed_3b): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(192, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(192, 96, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(96, 128, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(192, 16, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(16, 32, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(192, 32, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (Mixed_3c): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(256, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(256, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(128, 192, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(256, 32, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(32, 96, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(256, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (MaxPool3d_4a_3x3): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)
    (Mixed_4b): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(480, 192, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(480, 96, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(96, 208, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(208, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(480, 16, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(16, 48, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(480, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (Mixed_4c): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(512, 160, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(512, 112, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(112, 224, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(224, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(512, 24, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(24, 64, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(512, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (Mixed_4d): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(512, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(512, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(128, 256, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(512, 24, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(24, 64, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(512, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (Mixed_4e): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(512, 112, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(512, 144, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(144, 288, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(512, 32, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(32, 64, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(512, 64, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (Mixed_4f): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(528, 256, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(528, 160, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(160, 320, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(320, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(528, 32, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(32, 128, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(528, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (MaxPool3d_5a_1x1): MaxPool3dSamePadding(kernel_size=[2, 2, 2], stride=(2, 1, 1), padding=0, dilation=1, ceil_mode=False)
    (Mixed_5b): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(832, 256, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(832, 160, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(160, 320, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(320, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(832, 32, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(32, 128, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(832, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (Mixed_5c): InceptionModule(
      (b0): Unit3D(
        (conv3d): Conv3d(832, 384, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1a): Unit3D(
        (conv3d): Conv3d(832, 192, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b1b): Unit3D(
        (conv3d): Conv3d(192, 384, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2a): Unit3D(
        (conv3d): Conv3d(832, 48, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b2b): Unit3D(
        (conv3d): Conv3d(48, 128, kernel_size=[3, 3, 3], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (b3a): MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)
      (b3b): Unit3D(
        (conv3d): Conv3d(832, 128, kernel_size=[1, 1, 1], stride=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
  )
  (gen): Generator(
    (conv2d): Conv2d(1536, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (upsamp1): Upsample(scale_factor=2.0, mode=nearest)
    (conv3d_1a): Conv3d(1024, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (conv3d_1b): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (upsamp2): Upsample(scale_factor=2.0, mode=nearest)
    (conv3d_2a): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (conv3d_2b): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (upsamp3): Upsample(scale_factor=2.0, mode=nearest)
    (conv3d_3a): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (conv3d_3b): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    (upsamp4): Upsample(scale_factor=2.0, mode=nearest)
    (conv3d_4): Conv3d(64, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))
  )
)
Training...
	Batch 10/122 Loss:0.51879 con:0.31113 recon1:0.09941 recon2:0.10825
	Batch 20/122 Loss:0.39254 con:0.31267 recon1:0.03970 recon2:0.04017
	Batch 30/122 Loss:0.33931 con:0.27386 recon1:0.03347 recon2:0.03197
	Batch 40/122 Loss:0.31575 con:0.26590 recon1:0.02650 recon2:0.02336
	Batch 50/122 Loss:0.29191 con:0.24227 recon1:0.02206 recon2:0.02758
	Batch 60/122 Loss:0.27985 con:0.24441 recon1:0.01810 recon2:0.01734
	Batch 70/122 Loss:0.27985 con:0.24504 recon1:0.01750 recon2:0.01731
	Batch 80/122 Loss:0.28084 con:0.25090 recon1:0.01621 recon2:0.01374
	Batch 90/122 Loss:0.27571 con:0.24755 recon1:0.01395 recon2:0.01421
	Batch 100/122 Loss:0.25613 con:0.23068 recon1:0.01369 recon2:0.01177
	Batch 110/122 Loss:0.26055 con:0.23320 recon1:0.01197 recon2:0.01537
	Batch 120/122 Loss:0.26501 con:0.24039 recon1:0.01264 recon2:0.01198
Training Epoch 1/1000 Loss:0.32128 con:0.26037 recon1:0.03054 recon2:0.03037
Validation...
	Batch 10/17 Loss:0.30805 con:0.26019 recon1:0.02294 recon2:0.02493
Validation Epoch 1/1000 Loss:0.24777 con:0.22010 recon1:0.01382 recon2:0.01385
Training...
	Batch 10/122 Loss:0.26669 con:0.24209 recon1:0.01181 recon2:0.01279
	Batch 20/122 Loss:0.25233 con:0.23025 recon1:0.01118 recon2:0.01090
	Batch 30/122 Loss:0.25055 con:0.23064 recon1:0.00976 recon2:0.01014
	Batch 40/122 Loss:0.25797 con:0.23322 recon1:0.01384 recon2:0.01091
	Batch 50/122 Loss:0.25574 con:0.23302 recon1:0.01139 recon2:0.01132
	Batch 60/122 Loss:0.25673 con:0.23559 recon1:0.01052 recon2:0.01061
	Batch 70/122 Loss:0.27262 con:0.24686 recon1:0.01285 recon2:0.01291
	Batch 80/122 Loss:0.25057 con:0.23096 recon1:0.01001 recon2:0.00960
	Batch 90/122 Loss:0.24558 con:0.22428 recon1:0.00978 recon2:0.01151
	Batch 100/122 Loss:0.26166 con:0.23941 recon1:0.01134 recon2:0.01092
	Batch 110/122 Loss:0.25349 con:0.23336 recon1:0.00958 recon2:0.01055
	Batch 120/122 Loss:0.24963 con:0.23115 recon1:0.00867 recon2:0.00981
Training Epoch 2/1000 Loss:0.25604 con:0.23409 recon1:0.01095 recon2:0.01100
Validation...
	Batch 10/17 Loss:0.31366 con:0.27892 recon1:0.01594 recon2:0.01879
Validation Epoch 2/1000 Loss:0.25301 con:0.23260 recon1:0.01012 recon2:0.01028
Training...
	Batch 10/122 Loss:0.25106 con:0.22417 recon1:0.01433 recon2:0.01256
	Batch 20/122 Loss:0.24690 con:0.22865 recon1:0.00923 recon2:0.00901
	Batch 30/122 Loss:0.26078 con:0.24142 recon1:0.00934 recon2:0.01002
	Batch 40/122 Loss:0.25322 con:0.23506 recon1:0.00927 recon2:0.00888
	Batch 50/122 Loss:0.24869 con:0.23084 recon1:0.00856 recon2:0.00929
	Batch 60/122 Loss:0.23996 con:0.22096 recon1:0.00937 recon2:0.00963
	Batch 70/122 Loss:0.26342 con:0.24222 recon1:0.01229 recon2:0.00891
	Batch 80/122 Loss:0.23384 con:0.21476 recon1:0.01000 recon2:0.00908
	Batch 90/122 Loss:0.24242 con:0.22471 recon1:0.00894 recon2:0.00877
	Batch 100/122 Loss:0.24871 con:0.22924 recon1:0.00869 recon2:0.01078
	Batch 110/122 Loss:0.24433 con:0.22575 recon1:0.00904 recon2:0.00954
	Batch 120/122 Loss:0.24705 con:0.23106 recon1:0.00732 recon2:0.00867
Training Epoch 3/1000 Loss:0.25002 con:0.23090 recon1:0.00962 recon2:0.00950
Validation...
	Batch 10/17 Loss:0.30892 con:0.27690 recon1:0.01482 recon2:0.01720
Validation Epoch 3/1000 Loss:0.25293 con:0.23203 recon1:0.01043 recon2:0.01047
Training...
	Batch 10/122 Loss:0.24653 con:0.22807 recon1:0.00903 recon2:0.00944
	Batch 20/122 Loss:0.24570 con:0.22831 recon1:0.00898 recon2:0.00841
	Batch 30/122 Loss:0.23768 con:0.22102 recon1:0.00788 recon2:0.00878
	Batch 40/122 Loss:0.24081 con:0.22425 recon1:0.00849 recon2:0.00807
	Batch 50/122 Loss:0.24291 con:0.22729 recon1:0.00822 recon2:0.00740
	Batch 60/122 Loss:0.24221 con:0.22691 recon1:0.00779 recon2:0.00751
	Batch 70/122 Loss:0.24142 con:0.22282 recon1:0.00957 recon2:0.00902
	Batch 80/122 Loss:0.24092 con:0.22391 recon1:0.00830 recon2:0.00871
	Batch 90/122 Loss:0.23218 con:0.21811 recon1:0.00766 recon2:0.00641
	Batch 100/122 Loss:0.23610 con:0.22088 recon1:0.00803 recon2:0.00720
	Batch 110/122 Loss:0.25544 con:0.23856 recon1:0.00844 recon2:0.00843
	Batch 120/122 Loss:0.23171 con:0.21785 recon1:0.00698 recon2:0.00687
Training Epoch 4/1000 Loss:0.24417 con:0.22717 recon1:0.00845 recon2:0.00854
Validation...
	Batch 10/17 Loss:0.30495 con:0.27823 recon1:0.01231 recon2:0.01441
Validation Epoch 4/1000 Loss:0.24940 con:0.23257 recon1:0.00841 recon2:0.00842
Training...
	Batch 10/122 Loss:0.23701 con:0.21884 recon1:0.00892 recon2:0.00925
	Batch 20/122 Loss:0.23579 con:0.22057 recon1:0.00753 recon2:0.00769
	Batch 30/122 Loss:0.25335 con:0.23182 recon1:0.01392 recon2:0.00762
	Batch 40/122 Loss:0.23879 con:0.22194 recon1:0.00841 recon2:0.00844
	Batch 50/122 Loss:0.24511 con:0.23004 recon1:0.00723 recon2:0.00785
	Batch 60/122 Loss:0.24961 con:0.23178 recon1:0.00909 recon2:0.00874
	Batch 70/122 Loss:0.23292 con:0.21423 recon1:0.01023 recon2:0.00846
	Batch 80/122 Loss:0.22552 con:0.20983 recon1:0.00670 recon2:0.00899
	Batch 90/122 Loss:0.23735 con:0.22294 recon1:0.00798 recon2:0.00643
	Batch 100/122 Loss:0.23786 con:0.22273 recon1:0.00738 recon2:0.00775
	Batch 110/122 Loss:0.23998 con:0.22355 recon1:0.00870 recon2:0.00774
	Batch 120/122 Loss:0.23605 con:0.22081 recon1:0.00757 recon2:0.00766
Training Epoch 5/1000 Loss:0.24079 con:0.22452 recon1:0.00821 recon2:0.00805
Validation...
	Batch 10/17 Loss:0.29962 con:0.27426 recon1:0.01170 recon2:0.01366
Validation Epoch 5/1000 Loss:0.24434 con:0.22854 recon1:0.00788 recon2:0.00792
Training...
	Batch 10/122 Loss:0.24360 con:0.22493 recon1:0.00908 recon2:0.00959
	Batch 20/122 Loss:0.23475 con:0.22048 recon1:0.00705 recon2:0.00722
	Batch 30/122 Loss:0.23274 con:0.21989 recon1:0.00658 recon2:0.00627
	Batch 40/122 Loss:0.23261 con:0.21866 recon1:0.00724 recon2:0.00671
	Batch 50/122 Loss:0.24083 con:0.22735 recon1:0.00674 recon2:0.00675
	Batch 60/122 Loss:0.24004 con:0.22512 recon1:0.00754 recon2:0.00737
	Batch 70/122 Loss:0.25042 con:0.23035 recon1:0.01052 recon2:0.00955
	Batch 80/122 Loss:0.23260 con:0.21709 recon1:0.00777 recon2:0.00774
	Batch 90/122 Loss:0.23142 con:0.21635 recon1:0.00784 recon2:0.00722
	Batch 100/122 Loss:0.24375 con:0.22917 recon1:0.00720 recon2:0.00738
	Batch 110/122 Loss:0.23233 con:0.21812 recon1:0.00708 recon2:0.00712
	Batch 120/122 Loss:0.23274 con:0.21931 recon1:0.00689 recon2:0.00653
Training Epoch 6/1000 Loss:0.23751 con:0.22229 recon1:0.00764 recon2:0.00758
Validation...
	Batch 10/17 Loss:0.29667 con:0.27206 recon1:0.01140 recon2:0.01321
Validation Epoch 6/1000 Loss:0.24190 con:0.22652 recon1:0.00767 recon2:0.00772
Training...
	Batch 10/122 Loss:0.24580 con:0.22811 recon1:0.01014 recon2:0.00755
	Batch 20/122 Loss:0.23232 con:0.21947 recon1:0.00650 recon2:0.00636
	Batch 30/122 Loss:0.23791 con:0.22270 recon1:0.00658 recon2:0.00864
	Batch 40/122 Loss:0.24870 con:0.23231 recon1:0.00779 recon2:0.00860
	Batch 50/122 Loss:0.23872 con:0.22219 recon1:0.00798 recon2:0.00855
	Batch 60/122 Loss:0.23661 con:0.22094 recon1:0.00705 recon2:0.00861
	Batch 70/122 Loss:0.24363 con:0.22824 recon1:0.00762 recon2:0.00777
	Batch 80/122 Loss:0.24554 con:0.23188 recon1:0.00681 recon2:0.00685
	Batch 90/122 Loss:0.22503 con:0.21213 recon1:0.00661 recon2:0.00629
	Batch 100/122 Loss:0.23003 con:0.21720 recon1:0.00619 recon2:0.00664
	Batch 110/122 Loss:0.22539 con:0.21332 recon1:0.00622 recon2:0.00585
	Batch 120/122 Loss:0.22634 con:0.21393 recon1:0.00611 recon2:0.00631
Training Epoch 7/1000 Loss:0.23430 con:0.22018 recon1:0.00714 recon2:0.00697
Validation...
	Batch 10/17 Loss:0.29022 con:0.26695 recon1:0.01077 recon2:0.01250
Validation Epoch 7/1000 Loss:0.23649 con:0.22245 recon1:0.00700 recon2:0.00704
Training...
	Batch 10/122 Loss:0.22580 con:0.21260 recon1:0.00631 recon2:0.00689
	Batch 20/122 Loss:0.23936 con:0.22529 recon1:0.00781 recon2:0.00626
	Batch 30/122 Loss:0.23377 con:0.22150 recon1:0.00619 recon2:0.00609
	Batch 40/122 Loss:0.22625 con:0.21334 recon1:0.00670 recon2:0.00621
	Batch 50/122 Loss:0.23052 con:0.21269 recon1:0.00864 recon2:0.00920
	Batch 60/122 Loss:0.39944 con:0.20755 recon1:0.09377 recon2:0.09813
	Batch 70/122 Loss:1.96881 con:0.08285 recon1:1.06824 recon2:0.81771
	Batch 80/122 Loss:6.64761 con:0.00485 recon1:2.90700 recon2:3.73576
	Batch 90/122 Loss:0.20291 con:0.00089 recon1:0.11893 recon2:0.08309
	Batch 100/122 Loss:0.23603 con:0.00211 recon1:0.04868 recon2:0.18523
	Batch 110/122 Loss:0.16882 con:0.00031 recon1:0.07167 recon2:0.09684
	Batch 120/122 Loss:0.11080 con:0.00004 recon1:0.05459 recon2:0.05617
Training Epoch 8/1000 Loss:6.05205 con:0.12213 recon1:5.52179 recon2:0.40814
Validation...
	Batch 10/17 Loss:10.76184 con:0.00090 recon1:5.79189 recon2:4.96904
Validation Epoch 8/1000 Loss:10.73212 con:0.00073 recon1:5.45501 recon2:5.27638
Training...
	Batch 10/122 Loss:0.24257 con:0.00003 recon1:0.13536 recon2:0.10718
	Batch 20/122 Loss:4.38602 con:0.00003 recon1:3.40392 recon2:0.98207
	Batch 30/122 Loss:0.46803 con:0.00002 recon1:0.35777 recon2:0.11023
	Batch 40/122 Loss:0.23161 con:0.00002 recon1:0.12860 recon2:0.10298
	Batch 50/122 Loss:0.09718 con:0.00001 recon1:0.04734 recon2:0.04983
	Batch 60/122 Loss:0.16262 con:0.00001 recon1:0.08986 recon2:0.07275
	Batch 70/122 Loss:0.08976 con:0.00001 recon1:0.04282 recon2:0.04692
	Batch 80/122 Loss:0.07737 con:0.00000 recon1:0.04103 recon2:0.03633
	Batch 90/122 Loss:0.07044 con:0.00001 recon1:0.03578 recon2:0.03465
	Batch 100/122 Loss:0.07239 con:0.00001 recon1:0.03618 recon2:0.03620
	Batch 110/122 Loss:0.06301 con:0.00000 recon1:0.03150 recon2:0.03151
	Batch 120/122 Loss:0.06930 con:0.00001 recon1:0.03271 recon2:0.03658
Training Epoch 9/1000 Loss:2.18000 con:0.00003 recon1:0.57102 recon2:1.60896
Validation...
	Batch 10/17 Loss:0.12131 con:0.00000 recon1:0.06389 recon2:0.05743
Validation Epoch 9/1000 Loss:0.08776 con:0.00000 recon1:0.04437 recon2:0.04339
Training...
	Batch 10/122 Loss:4.70342 con:0.00000 recon1:2.15086 recon2:2.55256
	Batch 20/122 Loss:1.09436 con:0.00000 recon1:0.53849 recon2:0.55587
	Batch 30/122 Loss:78.66013 con:0.00000 recon1:38.21620 recon2:40.44393
	Batch 40/122 Loss:22.80107 con:0.00000 recon1:10.14553 recon2:12.65554
	Batch 50/122 Loss:5.93281 con:0.00000 recon1:2.61445 recon2:3.31836
	Batch 60/122 Loss:3.28844 con:0.00000 recon1:1.63510 recon2:1.65335
	Batch 70/122 Loss:2.18892 con:0.00000 recon1:1.25223 recon2:0.93668
	Batch 80/122 Loss:6.19992 con:0.00000 recon1:3.00611 recon2:3.19381
	Batch 90/122 Loss:2.36572 con:0.00000 recon1:1.22583 recon2:1.13989
	Batch 100/122 Loss:1.20351 con:0.00000 recon1:0.64415 recon2:0.55936
	Batch 110/122 Loss:0.51978 con:0.00000 recon1:0.28646 recon2:0.23332
	Batch 120/122 Loss:9.58693 con:0.00000 recon1:4.79276 recon2:4.79417
Training Epoch 10/1000 Loss:1813.15769 con:0.00002 recon1:1018.68579 recon2:794.47190
Validation...
	Batch 10/17 Loss:88.50333 con:0.00000 recon1:41.27263 recon2:47.23070
Validation Epoch 10/1000 Loss:80.45160 con:0.00000 recon1:40.24690 recon2:40.20470
Training...
	Batch 10/122 Loss:250.94131 con:0.00000 recon1:146.95183 recon2:103.98949
	Batch 20/122 Loss:154.25510 con:0.00000 recon1:68.22902 recon2:86.02608
	Batch 30/122 Loss:2361.17896 con:0.00000 recon1:1169.63501 recon2:1191.54395
	Batch 40/122 Loss:297.90430 con:0.00000 recon1:149.02475 recon2:148.87956
	Batch 50/122 Loss:188.18826 con:0.00000 recon1:72.71809 recon2:115.47018
	Batch 60/122 Loss:232.86938 con:0.00000 recon1:116.90272 recon2:115.96667
	Batch 70/122 Loss:45.88768 con:0.00000 recon1:24.98203 recon2:20.90565
	Batch 80/122 Loss:13.74547 con:0.00000 recon1:1.36944 recon2:12.37602
	Batch 90/122 Loss:45.12426 con:0.00000 recon1:22.43007 recon2:22.69419
	Batch 100/122 Loss:20.36324 con:0.00000 recon1:13.83448 recon2:6.52876
	Batch 110/122 Loss:6.21884 con:0.00000 recon1:2.97061 recon2:3.24822
	Batch 120/122 Loss:2.76190 con:0.00000 recon1:1.86325 recon2:0.89865
Training Epoch 11/1000 Loss:22761370.78076 con:0.00003 recon1:1189391.07561 recon2:21571979.18055
Validation...
	Batch 10/17 Loss:31.88846 con:0.00000 recon1:16.02788 recon2:15.86058
Validation Epoch 11/1000 Loss:31.76127 con:0.00000 recon1:15.87829 recon2:15.88299
Training...
	Batch 10/122 Loss:262038.76562 con:0.00000 recon1:129904.93750 recon2:132133.82812
	Batch 20/122 Loss:81355.40625 con:0.00000 recon1:26129.01953 recon2:55226.38281
	Batch 30/122 Loss:50254.82031 con:0.00000 recon1:24134.43164 recon2:26120.38867
	Batch 40/122 Loss:31259.38086 con:0.00000 recon1:15669.54395 recon2:15589.83691
	Batch 50/122 Loss:11682.92773 con:0.00000 recon1:5710.91309 recon2:5972.01465
	Batch 60/122 Loss:4608.89062 con:0.00000 recon1:2534.85083 recon2:2074.03955
	Batch 70/122 Loss:1547.83032 con:0.00000 recon1:779.88947 recon2:767.94086
	Batch 80/122 Loss:9027.85547 con:0.00000 recon1:2625.77808 recon2:6402.07764
	Batch 90/122 Loss:1132.03442 con:0.00000 recon1:564.80438 recon2:567.23010
	Batch 100/122 Loss:4151.17090 con:0.00000 recon1:3648.40869 recon2:502.76196
	Batch 110/122 Loss:5399.76123 con:0.00000 recon1:584.87018 recon2:4814.89111
	Batch 120/122 Loss:1007.88611 con:0.00000 recon1:711.53473 recon2:296.35141
Training Epoch 12/1000 Loss:167075626.13754 con:0.00021 recon1:136598984.07461 recon2:30476644.16137
Validation...
	Batch 10/17 Loss:143535.03125 con:0.00000 recon1:72088.87500 recon2:71446.16406
Validation Epoch 12/1000 Loss:143901.09835 con:0.00000 recon1:72186.64706 recon2:71714.44991
Training...
	Batch 10/122 Loss:396838.03125 con:0.00000 recon1:198517.73438 recon2:198320.29688
	Batch 20/122 Loss:816197.12500 con:0.00000 recon1:392033.18750 recon2:424163.96875
	Batch 30/122 Loss:656892.06250 con:0.00000 recon1:322909.50000 recon2:333982.56250
	Batch 40/122 Loss:580520.37500 con:0.00000 recon1:272129.93750 recon2:308390.46875
	Batch 50/122 Loss:486930.87500 con:0.00000 recon1:236873.46875 recon2:250057.39062
	Batch 60/122 Loss:426878.71875 con:0.00000 recon1:207416.78125 recon2:219461.93750
	Batch 70/122 Loss:397800.90625 con:0.00000 recon1:187750.09375 recon2:210050.81250
	Batch 80/122 Loss:347082.37500 con:0.00000 recon1:173319.28125 recon2:173763.10938
	Batch 90/122 Loss:412691.71875 con:0.00000 recon1:256754.06250 recon2:155937.65625
	Batch 100/122 Loss:284978.81250 con:0.00000 recon1:144137.17188 recon2:140841.65625
	Batch 110/122 Loss:282574.31250 con:0.00000 recon1:151167.60938 recon2:131406.68750
	Batch 120/122 Loss:257261.93750 con:0.00000 recon1:122395.61719 recon2:134866.31250
Training Epoch 13/1000 Loss:21986132.25945 con:0.00001 recon1:21291889.78900 recon2:694242.70100
Validation...
	Batch 10/17 Loss:19731.82422 con:0.00000 recon1:9869.30176 recon2:9862.52246
Validation Epoch 13/1000 Loss:19941.76080 con:0.00000 recon1:9989.70473 recon2:9952.05612
Training...
	Batch 10/122 Loss:3414221.00000 con:0.00001 recon1:1165446.12500 recon2:2248774.75000
	Batch 20/122 Loss:2424519.75000 con:0.00001 recon1:1277753.75000 recon2:1146766.00000
	Batch 30/122 Loss:1461254.50000 con:0.00000 recon1:736949.81250 recon2:724304.75000
	Batch 40/122 Loss:879174.62500 con:0.00000 recon1:466653.28125 recon2:412521.37500
	Batch 50/122 Loss:487753.00000 con:0.00000 recon1:274881.31250 recon2:212871.68750
	Batch 60/122 Loss:209646.84375 con:0.00000 recon1:101655.77344 recon2:107991.07812
	Batch 70/122 Loss:377008.00000 con:0.00001 recon1:56575.48047 recon2:320432.53125
	Batch 80/122 Loss:70671.75000 con:0.00001 recon1:38679.47656 recon2:31992.27539
	Batch 90/122 Loss:31087.05078 con:0.00000 recon1:14193.20410 recon2:16893.84570
	Batch 100/122 Loss:14655.99805 con:0.00001 recon1:5488.26172 recon2:9167.73633
	Batch 110/122 Loss:7589.05078 con:0.00000 recon1:4803.68848 recon2:2785.36230
	Batch 120/122 Loss:5215.17480 con:0.00000 recon1:1966.51929 recon2:3248.65527
Training Epoch 14/1000 Loss:16932468.84529 con:0.00001 recon1:10979180.99937 recon2:5953287.84700
Validation...
	Batch 10/17 Loss:2178.97314 con:0.00000 recon1:1129.02869 recon2:1049.94446
Validation Epoch 14/1000 Loss:4157.08475 con:0.00000 recon1:2242.56517 recon2:1914.51962
Training...
	Batch 10/122 Loss:63519.61719 con:0.00000 recon1:31240.74805 recon2:32278.87109
	Batch 20/122 Loss:65116.95312 con:0.00000 recon1:31668.07227 recon2:33448.87891
	Batch 30/122 Loss:65666.06250 con:0.00000 recon1:34277.61328 recon2:31388.45312
	Batch 40/122 Loss:58383.74219 con:0.00000 recon1:30161.07227 recon2:28222.66992
	Batch 50/122 Loss:52018.96094 con:0.00000 recon1:24872.63867 recon2:27146.32031
	Batch 60/122 Loss:54803.23438 con:0.00000 recon1:25973.09961 recon2:28830.13672
	Batch 70/122 Loss:85140.98438 con:0.00001 recon1:29227.02734 recon2:55913.96094
	Batch 80/122 Loss:60419.69531 con:0.00000 recon1:36245.95703 recon2:24173.73633
	Batch 90/122 Loss:282888.78125 con:0.00001 recon1:260502.53125 recon2:22386.25586
	Batch 100/122 Loss:41699.38281 con:0.00000 recon1:20590.27148 recon2:21109.10938
	Batch 110/122 Loss:40622.17578 con:0.00000 recon1:21283.72656 recon2:19338.44922
	Batch 120/122 Loss:37192.75000 con:0.00000 recon1:17969.86133 recon2:19222.88867
Training Epoch 15/1000 Loss:2971534.82660 con:0.00000 recon1:360067.14471 recon2:2611467.68207
Validation...
	Batch 10/17 Loss:35808.59375 con:0.00000 recon1:17797.26172 recon2:18011.33008
Validation Epoch 15/1000 Loss:35685.29021 con:0.00000 recon1:17784.71358 recon2:17900.57617
Training...
	Batch 10/122 Loss:28974.79688 con:0.00000 recon1:15762.50488 recon2:13212.29297
	Batch 20/122 Loss:22749.91016 con:0.00000 recon1:11338.01562 recon2:11411.89551
	Batch 30/122 Loss:20642.91016 con:0.00000 recon1:10332.56250 recon2:10310.34863
	Batch 40/122 Loss:19272.43359 con:0.00000 recon1:9962.13281 recon2:9310.30078
	Batch 50/122 Loss:19136.58203 con:0.00000 recon1:9399.02246 recon2:9737.55957
	Batch 60/122 Loss:18310.57422 con:0.00000 recon1:8957.49805 recon2:9353.07520
	Batch 70/122 Loss:18216.29297 con:0.00000 recon1:8644.53809 recon2:9571.75586
	Batch 80/122 Loss:24329.09375 con:0.00000 recon1:15704.04980 recon2:8625.04395
	Batch 90/122 Loss:17076.67969 con:0.00000 recon1:8520.33984 recon2:8556.33887
	Batch 100/122 Loss:16698.33203 con:0.00000 recon1:8628.12891 recon2:8070.20410
	Batch 110/122 Loss:16525.92188 con:0.00000 recon1:8136.12012 recon2:8389.80273
	Batch 120/122 Loss:15982.64355 con:0.00000 recon1:8176.18945 recon2:7806.45410
Training Epoch 16/1000 Loss:57498336.18434 con:0.00007 recon1:2969926.30627 recon2:54528409.09290
Validation...
	Batch 10/17 Loss:1692.11475 con:0.00000 recon1:839.66058 recon2:852.45422
Validation Epoch 16/1000 Loss:1690.61200 con:0.00000 recon1:841.51882 recon2:849.09318
Training...
	Batch 10/122 Loss:352206.62500 con:0.00000 recon1:176734.01562 recon2:175472.59375
	Batch 20/122 Loss:627650.75000 con:0.00000 recon1:318109.43750 recon2:309541.28125
	Batch 30/122 Loss:1169164.00000 con:0.00002 recon1:826231.25000 recon2:342932.71875
	Batch 40/122 Loss:538995.37500 con:0.00000 recon1:271217.87500 recon2:267777.46875
	Batch 50/122 Loss:630429.25000 con:0.00001 recon1:366808.65625 recon2:263620.56250
	Batch 60/122 Loss:326738.78125 con:0.00000 recon1:162136.85938 recon2:164601.92188
	Batch 70/122 Loss:505961.62500 con:0.00001 recon1:198951.35938 recon2:307010.25000
	Batch 80/122 Loss:945335.25000 con:0.00002 recon1:102487.45312 recon2:842847.81250
	Batch 90/122 Loss:162553.40625 con:0.00001 recon1:86553.33594 recon2:76000.07812
	Batch 100/122 Loss:127096.35938 con:0.00000 recon1:59851.55078 recon2:67244.80469
	Batch 110/122 Loss:112037.33594 con:0.00000 recon1:59457.90625 recon2:52579.42969
	Batch 120/122 Loss:95894.56250 con:0.00000 recon1:44911.59375 recon2:50982.97266
Training Epoch 17/1000 Loss:53750357.80385 con:0.00002 recon1:53343769.17558 recon2:406587.77229
Validation...
	Batch 10/17 Loss:83593.17969 con:0.00000 recon1:40405.46094 recon2:43187.71875
Validation Epoch 17/1000 Loss:83260.96645 con:0.00000 recon1:41408.21324 recon2:41852.75345
Training...
	Batch 10/122 Loss:7903.34229 con:0.00001 recon1:4303.00098 recon2:3600.34131
	Batch 20/122 Loss:179031.75000 con:0.00003 recon1:127258.38281 recon2:51773.36328
	Batch 30/122 Loss:157890.81250 con:0.00003 recon1:81020.87500 recon2:76869.92969
	Batch 40/122 Loss:171914.04688 con:0.00001 recon1:84259.77344 recon2:87654.27344
	Batch 50/122 Loss:181068.37500 con:0.00002 recon1:95447.07812 recon2:85621.29688
	Batch 60/122 Loss:179097.03125 con:0.00000 recon1:90227.04688 recon2:88869.99219
	Batch 70/122 Loss:162571.09375 con:0.00000 recon1:80973.42969 recon2:81597.65625
	Batch 80/122 Loss:152650.59375 con:0.00002 recon1:80026.04688 recon2:72624.55469
	Batch 90/122 Loss:140916.65625 con:0.00001 recon1:70686.86719 recon2:70229.78125
	Batch 100/122 Loss:132622.06250 con:0.00001 recon1:66651.86719 recon2:65970.18750
	Batch 110/122 Loss:136805.34375 con:0.00002 recon1:62518.89062 recon2:74286.46094
	Batch 120/122 Loss:121544.39062 con:0.00001 recon1:58251.39453 recon2:63293.00000
Training Epoch 18/1000 Loss:198471.97269 con:0.00001 recon1:69704.78075 recon2:128767.18997
Validation...
	Batch 10/17 Loss:117932.21875 con:0.00000 recon1:56307.29688 recon2:61624.92578
Validation Epoch 18/1000 Loss:116908.68934 con:0.00000 recon1:57720.93382 recon2:59187.75506
Training...
	Batch 10/122 Loss:129489.42188 con:0.00001 recon1:59797.03516 recon2:69692.38281
	Batch 20/122 Loss:116605.75000 con:0.00003 recon1:62943.54297 recon2:53662.20312
	Batch 30/122 Loss:117709.84375 con:0.00001 recon1:53029.98047 recon2:64679.86328
	Batch 40/122 Loss:122202.51562 con:0.00003 recon1:66011.29688 recon2:56191.22266
	Batch 50/122 Loss:103127.67188 con:0.00002 recon1:51578.86719 recon2:51548.80078
	Batch 60/122 Loss:99393.03906 con:0.00001 recon1:54102.33984 recon2:45290.69922
	Batch 70/122 Loss:89699.46875 con:0.00000 recon1:44789.35547 recon2:44910.11328
	Batch 80/122 Loss:88603.79688 con:0.00000 recon1:44045.62891 recon2:44558.17188
	Batch 90/122 Loss:149568.60938 con:0.00002 recon1:109052.97656 recon2:40515.63672
	Batch 100/122 Loss:79882.27344 con:0.00000 recon1:38338.82812 recon2:41543.44531
	Batch 110/122 Loss:73537.70312 con:0.00001 recon1:38018.38281 recon2:35519.32422
	Batch 120/122 Loss:78340.25000 con:0.00002 recon1:36998.50391 recon2:41341.74609
Training Epoch 19/1000 Loss:24333107.47515 con:0.00003 recon1:9172373.16339 recon2:15160735.36069
Validation...
	Batch 10/17 Loss:70076.39062 con:0.00000 recon1:33884.00391 recon2:36192.38672
Validation Epoch 19/1000 Loss:70278.91360 con:0.00000 recon1:34799.98598 recon2:35478.92762
Training...
	Batch 10/122 Loss:49402.52344 con:0.00003 recon1:26276.48242 recon2:23126.04297
	Batch 20/122 Loss:42168.10547 con:0.00002 recon1:18537.38086 recon2:23630.72461
	Batch 30/122 Loss:39528.23438 con:0.00003 recon1:22740.30469 recon2:16787.92773
	Batch 40/122 Loss:32625.48828 con:0.00001 recon1:16314.87109 recon2:16310.61719
	Batch 50/122 Loss:31627.65039 con:0.00001 recon1:19055.55273 recon2:12572.09766
	Batch 60/122 Loss:25879.95508 con:0.00001 recon1:12280.68750 recon2:13599.26758
	Batch 70/122 Loss:41117.78125 con:0.00005 recon1:19414.57227 recon2:21703.20703
	Batch 80/122 Loss:34150.82812 con:0.00003 recon1:15463.22656 recon2:18687.60352
	Batch 90/122 Loss:38945.85156 con:0.00001 recon1:22417.76758 recon2:16528.08203
	Batch 100/122 Loss:32702.25195 con:0.00002 recon1:17160.21094 recon2:15542.04102
	Batch 110/122 Loss:34311.26562 con:0.00001 recon1:17052.70117 recon2:17258.56641
	Batch 120/122 Loss:31066.59766 con:0.00003 recon1:17831.09961 recon2:13235.49707
Training Epoch 20/1000 Loss:39891.64027 con:0.00002 recon1:19631.41944 recon2:20260.22083
Validation...
	Batch 10/17 Loss:24200.87500 con:0.00001 recon1:11612.73633 recon2:12588.13867
Validation Epoch 20/1000 Loss:24911.68049 con:0.00001 recon1:12774.68592 recon2:12136.99446
Training...
	Batch 10/122 Loss:26820.96875 con:0.00002 recon1:11653.17090 recon2:15167.79785
	Batch 20/122 Loss:29912.42188 con:0.00001 recon1:15336.95898 recon2:14575.46289
	Batch 30/122 Loss:30590.86719 con:0.00002 recon1:18734.11914 recon2:11856.74707
	Batch 40/122 Loss:34085.39062 con:0.00002 recon1:15158.16797 recon2:18927.22461
	Batch 50/122 Loss:28369.94922 con:0.00001 recon1:15922.09570 recon2:12447.85449
	Batch 60/122 Loss:28956.44922 con:0.00002 recon1:16649.20703 recon2:12307.24121
	Batch 70/122 Loss:26924.68750 con:0.00002 recon1:12280.42969 recon2:14644.25879
	Batch 80/122 Loss:29311.58789 con:0.00001 recon1:19373.70312 recon2:9937.88477
	Batch 90/122 Loss:26868.66406 con:0.00002 recon1:11959.33887 recon2:14909.32617
	Batch 100/122 Loss:25638.87109 con:0.00001 recon1:12623.11719 recon2:13015.75488
	Batch 110/122 Loss:28548.69727 con:0.00002 recon1:15935.64258 recon2:12613.05469
	Batch 120/122 Loss:29627.94531 con:0.00001 recon1:11282.12988 recon2:18345.81641
Training Epoch 21/1000 Loss:28373.11796 con:0.00002 recon1:14205.55574 recon2:14167.56228
Validation...
	Batch 10/17 Loss:21507.04102 con:0.00001 recon1:10348.71777 recon2:11158.32324
Validation Epoch 21/1000 Loss:21981.83663 con:0.00001 recon1:11238.78070 recon2:10743.05601
Training...
	Batch 10/122 Loss:28555.33398 con:0.00002 recon1:15814.57129 recon2:12740.76270
	Batch 20/122 Loss:24621.11328 con:0.00001 recon1:12005.90918 recon2:12615.20410
	Batch 30/122 Loss:21867.10547 con:0.00001 recon1:9480.77148 recon2:12386.33301
	Batch 40/122 Loss:20351.80859 con:0.00001 recon1:10756.15430 recon2:9595.65430
	Batch 50/122 Loss:24356.82422 con:0.00004 recon1:13123.49902 recon2:11233.32422
	Batch 60/122 Loss:20811.02734 con:0.00001 recon1:10171.21094 recon2:10639.81738
	Batch 70/122 Loss:21122.64453 con:0.00001 recon1:11208.79492 recon2:9913.84961
	Batch 80/122 Loss:21191.77344 con:0.00003 recon1:10672.82324 recon2:10518.95020
	Batch 90/122 Loss:23999.73828 con:0.00003 recon1:14318.14258 recon2:9681.59473
	Batch 100/122 Loss:21204.06641 con:0.00001 recon1:10435.18164 recon2:10768.88477
	Batch 110/122 Loss:26109.31250 con:0.00001 recon1:14327.45703 recon2:11781.85645
	Batch 120/122 Loss:20939.80664 con:0.00001 recon1:10648.31348 recon2:10291.49316
Training Epoch 22/1000 Loss:26809.60715 con:0.00002 recon1:14449.61720 recon2:12359.98994
Validation...
	Batch 10/17 Loss:18975.10742 con:0.00000 recon1:9192.22559 recon2:9782.88184
Validation Epoch 22/1000 Loss:19390.91240 con:0.00000 recon1:9901.11546 recon2:9489.79682
Training...
	Batch 10/122 Loss:19590.36914 con:0.00001 recon1:9255.21094 recon2:10335.15820
	Batch 20/122 Loss:17518.70508 con:0.00000 recon1:8663.39941 recon2:8855.30566
	Batch 30/122 Loss:19626.27344 con:0.00001 recon1:9207.73926 recon2:10418.53418
	Batch 40/122 Loss:22649.13672 con:0.00003 recon1:11863.87402 recon2:10785.26172
	Batch 50/122 Loss:19958.68359 con:0.00002 recon1:8963.13379 recon2:10995.55078
	Batch 60/122 Loss:23697.85547 con:0.00004 recon1:11006.05566 recon2:12691.79980
	Batch 70/122 Loss:16470.70312 con:0.00001 recon1:8359.27344 recon2:8111.42871
	Batch 80/122 Loss:21420.79102 con:0.00001 recon1:9693.06934 recon2:11727.72168
	Batch 90/122 Loss:23135.35352 con:0.00001 recon1:11350.30371 recon2:11785.04980
	Batch 100/122 Loss:22799.35938 con:0.00001 recon1:11651.01465 recon2:11148.34473
	Batch 110/122 Loss:19962.51758 con:0.00001 recon1:10413.90527 recon2:9548.61230
	Batch 120/122 Loss:24842.19336 con:0.00001 recon1:12910.21387 recon2:11931.97949
Training Epoch 23/1000 Loss:36763.88192 con:0.00002 recon1:26102.52155 recon2:10661.35989
Validation...
	Batch 10/17 Loss:17096.80273 con:0.00000 recon1:8355.01270 recon2:8741.79004
Validation Epoch 23/1000 Loss:17643.04986 con:0.00000 recon1:9003.45827 recon2:8639.59177
Training...
	Batch 10/122 Loss:19138.94922 con:0.00002 recon1:10126.95020 recon2:9011.99902
	Batch 20/122 Loss:16538.86328 con:0.00001 recon1:7461.07129 recon2:9077.79297
	Batch 30/122 Loss:16483.27734 con:0.00002 recon1:7068.16553 recon2:9415.11133
	Batch 40/122 Loss:18994.79688 con:0.00002 recon1:9070.55859 recon2:9924.23730
	Batch 50/122 Loss:17998.28516 con:0.00002 recon1:7755.92432 recon2:10242.36133
	Batch 60/122 Loss:17105.60156 con:0.00000 recon1:8327.52051 recon2:8778.08203
	Batch 70/122 Loss:16168.12402 con:0.00001 recon1:8706.68945 recon2:7461.43457
	Batch 80/122 Loss:17433.65625 con:0.00001 recon1:8451.03418 recon2:8982.62305
	Batch 90/122 Loss:15972.85547 con:0.00000 recon1:7027.06934 recon2:8945.78613
	Batch 100/122 Loss:19359.44141 con:0.00001 recon1:7722.61035 recon2:11636.83008
	Batch 110/122 Loss:18145.51562 con:0.00001 recon1:8794.24121 recon2:9351.27539
	Batch 120/122 Loss:17344.07031 con:0.00002 recon1:7768.62012 recon2:9575.45020
Training Epoch 24/1000 Loss:59353.56878 con:0.00002 recon1:34356.56239 recon2:24997.00842
Validation...
	Batch 10/17 Loss:14639.11914 con:0.00000 recon1:7249.18213 recon2:7389.93701
Validation Epoch 24/1000 Loss:15169.45416 con:0.00000 recon1:7740.18517 recon2:7429.26881
Training...
	Batch 10/122 Loss:14568.08203 con:0.00001 recon1:7459.39697 recon2:7108.68555
	Batch 20/122 Loss:15303.88086 con:0.00001 recon1:7005.18457 recon2:8298.69629
	Batch 30/122 Loss:17599.10547 con:0.00001 recon1:8655.37207 recon2:8943.73438
	Batch 40/122 Loss:13934.47070 con:0.00002 recon1:7713.22217 recon2:6221.24854
	Batch 50/122 Loss:15093.42383 con:0.00001 recon1:8596.69238 recon2:6496.73193
	Batch 60/122 Loss:14606.55176 con:0.00001 recon1:7122.45801 recon2:7484.09375
	Batch 70/122 Loss:15266.94434 con:0.00002 recon1:7098.16650 recon2:8168.77783
	Batch 80/122 Loss:15639.29102 con:0.00004 recon1:9329.51562 recon2:6309.77490
	Batch 90/122 Loss:14634.27344 con:0.00003 recon1:7033.82324 recon2:7600.44971
	Batch 100/122 Loss:16577.66016 con:0.00001 recon1:8011.92334 recon2:8565.73633
	Batch 110/122 Loss:15837.14258 con:0.00002 recon1:8183.00830 recon2:7654.13428
	Batch 120/122 Loss:13537.12891 con:0.00001 recon1:8460.87793 recon2:5076.25146
Training Epoch 25/1000 Loss:15453.27439 con:0.00001 recon1:7948.67043 recon2:7504.60400
Validation...
	Batch 10/17 Loss:12073.19727 con:0.00000 recon1:5977.13818 recon2:6096.05859
Validation Epoch 25/1000 Loss:12439.06934 con:0.00000 recon1:6333.46364 recon2:6105.60578
Training...
	Batch 10/122 Loss:12720.19922 con:0.00002 recon1:6081.09766 recon2:6639.10107
	Batch 20/122 Loss:11306.97461 con:0.00002 recon1:5505.54541 recon2:5801.42920
	Batch 30/122 Loss:11413.35742 con:0.00001 recon1:6141.56396 recon2:5271.79297
	Batch 40/122 Loss:14651.43164 con:0.00001 recon1:7447.21191 recon2:7204.22021
	Batch 50/122 Loss:13780.90332 con:0.00000 recon1:6825.80713 recon2:6955.09619
	Batch 60/122 Loss:13367.50000 con:0.00002 recon1:6289.45898 recon2:7078.04053
	Batch 70/122 Loss:12137.01562 con:0.00001 recon1:6385.68701 recon2:5751.32812
	Batch 80/122 Loss:12858.89062 con:0.00002 recon1:7176.05518 recon2:5682.83496
	Batch 90/122 Loss:13142.32227 con:0.00001 recon1:6213.59863 recon2:6928.72412
	Batch 100/122 Loss:11435.19043 con:0.00000 recon1:5865.05811 recon2:5570.13232
	Batch 110/122 Loss:13467.79590 con:0.00001 recon1:6439.54932 recon2:7028.24658
	Batch 120/122 Loss:11393.90527 con:0.00001 recon1:5950.07861 recon2:5443.82666
Training Epoch 26/1000 Loss:49502.77809 con:0.00002 recon1:42816.10891 recon2:6686.66736
Validation...
	Batch 10/17 Loss:10826.17383 con:0.00000 recon1:5338.58691 recon2:5487.58740
Validation Epoch 26/1000 Loss:11083.81434 con:0.00000 recon1:5624.60168 recon2:5459.21263
Training...
	Batch 10/122 Loss:13746.71094 con:0.00001 recon1:7193.76025 recon2:6552.95068
	Batch 20/122 Loss:10769.52051 con:0.00001 recon1:5351.60791 recon2:5417.91260
	Batch 30/122 Loss:14783.04199 con:0.00002 recon1:6236.39453 recon2:8546.64746
	Batch 40/122 Loss:10217.16602 con:0.00001 recon1:5539.20410 recon2:4677.96143
	Batch 50/122 Loss:10040.61621 con:0.00001 recon1:4532.86621 recon2:5507.75000
	Batch 60/122 Loss:10418.85059 con:0.00001 recon1:5593.41113 recon2:4825.43945
	Batch 70/122 Loss:11107.43066 con:0.00001 recon1:5730.20508 recon2:5377.22559
	Batch 80/122 Loss:11603.89453 con:0.00001 recon1:5164.13672 recon2:6439.75830
	Batch 90/122 Loss:9858.24609 con:0.00002 recon1:5195.17920 recon2:4663.06738
	Batch 100/122 Loss:10866.34473 con:0.00003 recon1:6551.77588 recon2:4314.56885
	Batch 110/122 Loss:11058.50977 con:0.00002 recon1:4289.44971 recon2:6769.06055
	Batch 120/122 Loss:10421.04297 con:0.00001 recon1:4775.61035 recon2:5645.43213
Training Epoch 27/1000 Loss:10792.27799 con:0.00001 recon1:5381.32480 recon2:5410.95317
Validation...
	Batch 10/17 Loss:8826.00000 con:0.00000 recon1:4348.93604 recon2:4477.06396
Validation Epoch 27/1000 Loss:9003.64037 con:0.00000 recon1:4555.14611 recon2:4448.49423
Training...
	Batch 10/122 Loss:9157.95508 con:0.00001 recon1:5085.41602 recon2:4072.53857
	Batch 20/122 Loss:8266.31055 con:0.00001 recon1:4364.59277 recon2:3901.71777
	Batch 30/122 Loss:10646.27734 con:0.00001 recon1:4295.20068 recon2:6351.07715
	Batch 40/122 Loss:10835.45020 con:0.00001 recon1:6695.83691 recon2:4139.61328
	Batch 50/122 Loss:10555.89551 con:0.00001 recon1:4887.20166 recon2:5668.69385
	Batch 60/122 Loss:8624.60059 con:0.00001 recon1:4225.70752 recon2:4398.89307
	Batch 70/122 Loss:12542.57812 con:0.00001 recon1:7610.92578 recon2:4931.65234
	Batch 80/122 Loss:8083.51074 con:0.00000 recon1:4279.75977 recon2:3803.75073
	Batch 90/122 Loss:8667.51562 con:0.00001 recon1:4110.00830 recon2:4557.50781
	Batch 100/122 Loss:9436.46973 con:0.00001 recon1:5200.69238 recon2:4235.77734
	Batch 110/122 Loss:10695.97461 con:0.00001 recon1:5331.17627 recon2:5364.79785
	Batch 120/122 Loss:10558.33203 con:0.00001 recon1:4955.98047 recon2:5602.35107
Training Epoch 28/1000 Loss:33201.74797 con:0.00002 recon1:28152.26707 recon2:5049.47990
Validation...
	Batch 10/17 Loss:8108.09668 con:0.00000 recon1:4046.45532 recon2:4061.64111
Validation Epoch 28/1000 Loss:8339.64416 con:0.00000 recon1:4228.90988 recon2:4110.73430
Training...
	Batch 10/122 Loss:9227.74707 con:0.00003 recon1:4756.50781 recon2:4471.23926
	Batch 20/122 Loss:8072.90430 con:0.00002 recon1:4570.51514 recon2:3502.38892
	Batch 30/122 Loss:7780.63037 con:0.00000 recon1:4332.02637 recon2:3448.60400
	Batch 40/122 Loss:7415.72070 con:0.00000 recon1:3635.80688 recon2:3779.91406
	Batch 50/122 Loss:8572.89160 con:0.00000 recon1:4815.62109 recon2:3757.27075
	Batch 60/122 Loss:9430.84668 con:0.00002 recon1:6115.23242 recon2:3315.61450
	Batch 70/122 Loss:7864.44531 con:0.00001 recon1:4138.42969 recon2:3726.01562
	Batch 80/122 Loss:8186.20996 con:0.00001 recon1:4602.52393 recon2:3583.68628
	Batch 90/122 Loss:8154.43848 con:0.00001 recon1:3941.22461 recon2:4213.21387
	Batch 100/122 Loss:7719.17480 con:0.00002 recon1:3312.52319 recon2:4406.65186
	Batch 110/122 Loss:7834.18311 con:0.00000 recon1:4527.61035 recon2:3306.57275
	Batch 120/122 Loss:7157.01270 con:0.00000 recon1:3570.98047 recon2:3586.03223
Training Epoch 29/1000 Loss:8622.22104 con:0.00001 recon1:4249.48053 recon2:4372.74052
Validation...
	Batch 10/17 Loss:6833.04199 con:0.00000 recon1:3398.68457 recon2:3434.35718
Validation Epoch 29/1000 Loss:7024.98242 con:0.00000 recon1:3553.30897 recon2:3471.67345
Training...
	Batch 10/122 Loss:6495.20020 con:0.00000 recon1:3281.62329 recon2:3213.57715
	Batch 20/122 Loss:8047.42871 con:0.00002 recon1:3782.74243 recon2:4264.68652
	Batch 30/122 Loss:8837.41016 con:0.00001 recon1:3560.51489 recon2:5276.89551
	Batch 40/122 Loss:6195.10596 con:0.00001 recon1:2879.98975 recon2:3315.11621
	Batch 50/122 Loss:7838.87305 con:0.00001 recon1:3642.72607 recon2:4196.14697
	Batch 60/122 Loss:7534.72314 con:0.00000 recon1:3873.52637 recon2:3661.19678
	Batch 70/122 Loss:7810.24658 con:0.00000 recon1:3690.33496 recon2:4119.91162
	Batch 80/122 Loss:7042.19727 con:0.00001 recon1:3349.63892 recon2:3692.55835
	Batch 90/122 Loss:7544.61523 con:0.00001 recon1:4092.13135 recon2:3452.48389
	Batch 100/122 Loss:7435.05811 con:0.00002 recon1:4055.72778 recon2:3379.33032
	Batch 110/122 Loss:7424.95996 con:0.00001 recon1:3281.58813 recon2:4143.37158
	Batch 120/122 Loss:7542.03271 con:0.00001 recon1:3296.83105 recon2:4245.20166
Training Epoch 30/1000 Loss:7629.13220 con:0.00001 recon1:3804.57409 recon2:3824.55810
Validation...
	Batch 10/17 Loss:6269.63965 con:0.00000 recon1:3087.62158 recon2:3182.01831
Validation Epoch 30/1000 Loss:6400.88856 con:0.00000 recon1:3225.77677 recon2:3175.11183
Training...
	Batch 10/122 Loss:8150.75684 con:0.00001 recon1:4274.45020 recon2:3876.30688
	Batch 20/122 Loss:7392.28076 con:0.00002 recon1:3712.76709 recon2:3679.51367
	Batch 30/122 Loss:6555.59375 con:0.00001 recon1:3248.05835 recon2:3307.53540
	Batch 40/122 Loss:12567.90137 con:0.00002 recon1:3687.59009 recon2:8880.31152
	Batch 50/122 Loss:7415.28809 con:0.00001 recon1:4178.35889 recon2:3236.92896
	Batch 60/122 Loss:7849.82129 con:0.00002 recon1:4012.97925 recon2:3836.84180
	Batch 70/122 Loss:6832.43652 con:0.00001 recon1:3555.87646 recon2:3276.56006
	Batch 80/122 Loss:6411.51172 con:0.00001 recon1:3391.18335 recon2:3020.32812
	Batch 90/122 Loss:6294.34424 con:0.00000 recon1:2811.28906 recon2:3483.05518
	Batch 100/122 Loss:6723.06641 con:0.00001 recon1:3341.87622 recon2:3381.19043
	Batch 110/122 Loss:8569.70508 con:0.00002 recon1:3667.74023 recon2:4901.96484
	Batch 120/122 Loss:6297.48926 con:0.00001 recon1:3037.45117 recon2:3260.03784
Training Epoch 31/1000 Loss:7471.73652 con:0.00001 recon1:3906.84665 recon2:3564.88989
Validation...
	Batch 10/17 Loss:5898.16846 con:0.00000 recon1:2930.47803 recon2:2967.69043
Validation Epoch 31/1000 Loss:6043.31261 con:0.00000 recon1:3053.21800 recon2:2990.09458
Training...
	Batch 10/122 Loss:5377.25684 con:0.00001 recon1:2668.35791 recon2:2708.89893
	Batch 20/122 Loss:5995.87305 con:0.00002 recon1:3083.01880 recon2:2912.85425
	Batch 30/122 Loss:5945.31982 con:0.00001 recon1:3187.20459 recon2:2758.11523
	Batch 40/122 Loss:6604.33594 con:0.00001 recon1:3173.49561 recon2:3430.84058
	Batch 50/122 Loss:5844.20996 con:0.00003 recon1:2946.16602 recon2:2898.04419
	Batch 60/122 Loss:5861.01514 con:0.00000 recon1:2931.20410 recon2:2929.81104
	Batch 70/122 Loss:7955.16797 con:0.00001 recon1:5108.11279 recon2:2847.05518
	Batch 80/122 Loss:5695.66406 con:0.00002 recon1:2702.61768 recon2:2993.04614
	Batch 90/122 Loss:6441.00635 con:0.00001 recon1:3214.02271 recon2:3226.98364
	Batch 100/122 Loss:6421.35547 con:0.00001 recon1:3046.26636 recon2:3375.08887
	Batch 110/122 Loss:5301.16211 con:0.00001 recon1:2597.04468 recon2:2704.11719
	Batch 120/122 Loss:6547.77637 con:0.00001 recon1:3462.95972 recon2:3084.81665
Training Epoch 32/1000 Loss:9013.95487 con:0.00001 recon1:4571.53925 recon2:4442.41573
Validation...
	Batch 10/17 Loss:5426.09131 con:0.00000 recon1:2704.40820 recon2:2721.68311
Validation Epoch 32/1000 Loss:5569.23271 con:0.00000 recon1:2812.30973 recon2:2756.92308
Training...
	Batch 10/122 Loss:5044.86914 con:0.00000 recon1:2513.22021 recon2:2531.64868
	Batch 20/122 Loss:5784.55859 con:0.00002 recon1:2763.54517 recon2:3021.01318
	Batch 30/122 Loss:5283.19434 con:0.00001 recon1:2815.56543 recon2:2467.62866
	Batch 40/122 Loss:5269.99707 con:0.00002 recon1:2837.81470 recon2:2432.18237
	Batch 50/122 Loss:6142.73096 con:0.00001 recon1:2570.82031 recon2:3571.91064
	Batch 60/122 Loss:5406.21484 con:0.00001 recon1:2380.33545 recon2:3025.87915
	Batch 70/122 Loss:13571.25977 con:0.00002 recon1:3005.28833 recon2:10565.97168
	Batch 80/122 Loss:6127.41504 con:0.00001 recon1:2768.00269 recon2:3359.41260
	Batch 90/122 Loss:6332.66748 con:0.00001 recon1:2812.71387 recon2:3519.95361
	Batch 100/122 Loss:5172.25781 con:0.00000 recon1:2490.31958 recon2:2681.93799
	Batch 110/122 Loss:6898.05566 con:0.00001 recon1:3081.31323 recon2:3816.74268
	Batch 120/122 Loss:5424.45996 con:0.00001 recon1:2704.70337 recon2:2719.75635
Training Epoch 33/1000 Loss:5976.11813 con:0.00001 recon1:2986.87960 recon2:2989.23854
Validation...
	Batch 10/17 Loss:4924.34375 con:0.00000 recon1:2466.82520 recon2:2457.51855
Validation Epoch 33/1000 Loss:5073.59056 con:0.00000 recon1:2563.46658 recon2:2510.12397
Training...
	Batch 10/122 Loss:5251.94629 con:0.00001 recon1:2810.31665 recon2:2441.62939
	Batch 20/122 Loss:5503.60840 con:0.00001 recon1:2523.92017 recon2:2979.68848
	Batch 30/122 Loss:5601.64258 con:0.00003 recon1:2866.59253 recon2:2735.05029
	Batch 40/122 Loss:5890.90381 con:0.00002 recon1:2853.51489 recon2:3037.38892
	Batch 50/122 Loss:6224.24023 con:0.00000 recon1:3075.98926 recon2:3148.25098
	Batch 60/122 Loss:4787.01025 con:0.00001 recon1:2559.86938 recon2:2227.14087
	Batch 70/122 Loss:5552.26172 con:0.00000 recon1:2402.91431 recon2:3149.34741
	Batch 80/122 Loss:4725.72607 con:0.00000 recon1:2217.25000 recon2:2508.47607
	Batch 90/122 Loss:5404.48535 con:0.00000 recon1:2552.00269 recon2:2852.48291
	Batch 100/122 Loss:4400.39160 con:0.00000 recon1:2243.94507 recon2:2156.44629
	Batch 110/122 Loss:4620.25781 con:0.00000 recon1:2286.47583 recon2:2333.78198
	Batch 120/122 Loss:6357.04199 con:0.00001 recon1:2419.34497 recon2:3937.69727
Training Epoch 34/1000 Loss:10218.66080 con:0.00001 recon1:2711.68197 recon2:7506.97900
Validation...
	Batch 10/17 Loss:4546.11230 con:0.00000 recon1:2269.63867 recon2:2276.47339
Validation Epoch 34/1000 Loss:4682.90042 con:0.00000 recon1:2362.78381 recon2:2320.11657
Training...
	Batch 10/122 Loss:4302.04199 con:0.00001 recon1:2178.45923 recon2:2123.58301
	Batch 20/122 Loss:4497.89258 con:0.00001 recon1:2225.87598 recon2:2272.01636
	Batch 30/122 Loss:4845.53906 con:0.00000 recon1:2749.17529 recon2:2096.36401
	Batch 40/122 Loss:4217.82861 con:0.00000 recon1:1958.95605 recon2:2258.87256
	Batch 50/122 Loss:4425.12305 con:0.00001 recon1:2231.07178 recon2:2194.05103
	Batch 60/122 Loss:5075.73730 con:0.00002 recon1:2082.73438 recon2:2993.00317
	Batch 70/122 Loss:4498.90137 con:0.00000 recon1:2270.25195 recon2:2228.64917
	Batch 80/122 Loss:4615.02246 con:0.00000 recon1:2016.64673 recon2:2598.37573
	Batch 90/122 Loss:4260.12988 con:0.00001 recon1:2123.75244 recon2:2136.37744
	Batch 100/122 Loss:4344.14453 con:0.00001 recon1:2017.42688 recon2:2326.71753
	Batch 110/122 Loss:4219.87695 con:0.00001 recon1:2180.93750 recon2:2038.93921
	Batch 120/122 Loss:4431.66553 con:0.00001 recon1:2149.25854 recon2:2282.40698
Training Epoch 35/1000 Loss:4864.87296 con:0.00001 recon1:2425.87070 recon2:2439.00226
Validation...
	Batch 10/17 Loss:4031.35645 con:0.00000 recon1:2012.26660 recon2:2019.08997
Validation Epoch 35/1000 Loss:4169.61251 con:0.00000 recon1:2100.66505 recon2:2068.94747
Training...
	Batch 10/122 Loss:4119.07129 con:0.00001 recon1:2105.47852 recon2:2013.59253
	Batch 20/122 Loss:4354.70996 con:0.00000 recon1:2403.11450 recon2:1951.59570
	Batch 30/122 Loss:4719.81152 con:0.00001 recon1:2321.49023 recon2:2398.32153
	Batch 40/122 Loss:4211.48828 con:0.00001 recon1:2042.08337 recon2:2169.40479
	Batch 50/122 Loss:4485.44824 con:0.00000 recon1:1918.96936 recon2:2566.47900
	Batch 60/122 Loss:3612.38232 con:0.00001 recon1:1808.65259 recon2:1803.72986
	Batch 70/122 Loss:4063.54834 con:0.00002 recon1:1816.08020 recon2:2247.46802
	Batch 80/122 Loss:4113.79395 con:0.00002 recon1:2023.98047 recon2:2089.81372
	Batch 90/122 Loss:4045.12744 con:0.00001 recon1:1868.41736 recon2:2176.71021
	Batch 100/122 Loss:4106.13184 con:0.00001 recon1:2054.87646 recon2:2051.25513
	Batch 110/122 Loss:4973.67480 con:0.00000 recon1:2768.88623 recon2:2204.78857
	Batch 120/122 Loss:4192.83057 con:0.00000 recon1:2143.35693 recon2:2049.47363
Training Epoch 36/1000 Loss:4493.02248 con:0.00001 recon1:2280.89966 recon2:2212.12281
Validation...
	Batch 10/17 Loss:3746.36914 con:0.00000 recon1:1878.72327 recon2:1867.64600
Validation Epoch 36/1000 Loss:3866.57367 con:0.00000 recon1:1949.84073 recon2:1916.73296
Training...
	Batch 10/122 Loss:4340.09863 con:0.00001 recon1:1826.59741 recon2:2513.50122
	Batch 20/122 Loss:3896.39209 con:0.00001 recon1:1985.65259 recon2:1910.73962
	Batch 30/122 Loss:3729.61572 con:0.00000 recon1:1940.32849 recon2:1789.28723
	Batch 40/122 Loss:4956.70947 con:0.00002 recon1:2860.07886 recon2:2096.63062
	Batch 50/122 Loss:3885.17285 con:0.00001 recon1:2190.82031 recon2:1694.35266
	Batch 60/122 Loss:4502.86670 con:0.00002 recon1:2490.11743 recon2:2012.74915
	Batch 70/122 Loss:4085.12939 con:0.00000 recon1:1880.43457 recon2:2204.69482
	Batch 80/122 Loss:3928.41235 con:0.00001 recon1:2136.73926 recon2:1791.67310
	Batch 90/122 Loss:3560.59766 con:0.00001 recon1:1828.78320 recon2:1731.81458
	Batch 100/122 Loss:3762.16284 con:0.00001 recon1:1824.40857 recon2:1937.75427
	Batch 110/122 Loss:3602.54761 con:0.00002 recon1:1855.73914 recon2:1746.80847
	Batch 120/122 Loss:4175.86768 con:0.00001 recon1:2132.83423 recon2:2043.03357
Training Epoch 37/1000 Loss:15506.56459 con:0.00002 recon1:2111.14219 recon2:13395.42254
Validation...
	Batch 10/17 Loss:3433.71484 con:0.00000 recon1:1721.45032 recon2:1712.26440
Validation Epoch 37/1000 Loss:3539.56452 con:0.00000 recon1:1785.14310 recon2:1754.42144
Training...
	Batch 10/122 Loss:3044.37598 con:0.00001 recon1:1428.74170 recon2:1615.63440
	Batch 20/122 Loss:3379.29688 con:0.00000 recon1:1660.47961 recon2:1718.81726
	Batch 30/122 Loss:3180.20947 con:0.00003 recon1:1513.19373 recon2:1667.01562
	Batch 40/122 Loss:3128.08252 con:0.00000 recon1:1576.87073 recon2:1551.21179
	Batch 50/122 Loss:3635.62891 con:0.00001 recon1:1907.32190 recon2:1728.30713
	Batch 60/122 Loss:3357.18018 con:0.00000 recon1:1665.23462 recon2:1691.94556
	Batch 70/122 Loss:3142.52441 con:0.00003 recon1:1531.07886 recon2:1611.44568
	Batch 80/122 Loss:3717.78223 con:0.00001 recon1:1762.12646 recon2:1955.65564
	Batch 90/122 Loss:3057.64307 con:0.00000 recon1:1308.29492 recon2:1749.34827
	Batch 100/122 Loss:3374.77686 con:0.00000 recon1:1789.53955 recon2:1585.23718
	Batch 110/122 Loss:2611.27100 con:0.00001 recon1:1307.65637 recon2:1303.61450
	Batch 120/122 Loss:3630.95361 con:0.00001 recon1:1991.87793 recon2:1639.07581
Training Epoch 38/1000 Loss:8319.39319 con:0.00001 recon1:1688.39656 recon2:6630.99668
Validation...
	Batch 10/17 Loss:2804.30103 con:0.00000 recon1:1409.62695 recon2:1394.67407
Validation Epoch 38/1000 Loss:2893.51073 con:0.00000 recon1:1458.75046 recon2:1434.76029
Training...
	Batch 10/122 Loss:2790.32983 con:0.00001 recon1:1377.08765 recon2:1413.24219
	Batch 20/122 Loss:3044.28906 con:0.00001 recon1:1609.26184 recon2:1435.02710
	Batch 30/122 Loss:2524.90039 con:0.00001 recon1:1206.34351 recon2:1318.55676
	Batch 40/122 Loss:3092.53369 con:0.00000 recon1:1662.57520 recon2:1429.95850
	Batch 50/122 Loss:3140.46777 con:0.00001 recon1:1589.34204 recon2:1551.12561
	Batch 60/122 Loss:2899.23145 con:0.00002 recon1:1386.60522 recon2:1512.62622
	Batch 70/122 Loss:2868.33789 con:0.00001 recon1:1505.33374 recon2:1363.00415
	Batch 80/122 Loss:2904.53369 con:0.00002 recon1:1289.84717 recon2:1614.68640
	Batch 90/122 Loss:2659.83008 con:0.00002 recon1:1454.37207 recon2:1205.45813
	Batch 100/122 Loss:2834.35352 con:0.00001 recon1:1267.58167 recon2:1566.77185
	Batch 110/122 Loss:2908.23413 con:0.00001 recon1:1494.60632 recon2:1413.62781
	Batch 120/122 Loss:2563.07422 con:0.00001 recon1:1225.99158 recon2:1337.08264
Training Epoch 39/1000 Loss:4123.59051 con:0.00001 recon1:2690.88680 recon2:1432.70371
Validation...
	Batch 10/17 Loss:2407.06543 con:0.00000 recon1:1201.17688 recon2:1205.88843
Validation Epoch 39/1000 Loss:2478.66326 con:0.00000 recon1:1245.49541 recon2:1233.16785
Training...
	Batch 10/122 Loss:3213.71777 con:0.00000 recon1:2001.89246 recon2:1211.82532
	Batch 20/122 Loss:2832.99121 con:0.00000 recon1:1355.30530 recon2:1477.68579
	Batch 30/122 Loss:2478.92139 con:0.00001 recon1:1165.28491 recon2:1313.63647
	Batch 40/122 Loss:2685.68677 con:0.00001 recon1:1360.78247 recon2:1324.90430
	Batch 50/122 Loss:3078.20728 con:0.00001 recon1:1618.69373 recon2:1459.51355
	Batch 60/122 Loss:2532.91357 con:0.00001 recon1:1298.90259 recon2:1234.01111
	Batch 70/122 Loss:2939.21167 con:0.00000 recon1:1609.70898 recon2:1329.50269
	Batch 80/122 Loss:2766.17358 con:0.00001 recon1:1456.67151 recon2:1309.50208
	Batch 90/122 Loss:2481.61841 con:0.00000 recon1:1308.07361 recon2:1173.54480
	Batch 100/122 Loss:2399.08789 con:0.00002 recon1:1120.28772 recon2:1278.80005
	Batch 110/122 Loss:2395.44116 con:0.00000 recon1:1213.88843 recon2:1181.55273
	Batch 120/122 Loss:2361.55737 con:0.00001 recon1:1066.03967 recon2:1295.51770
Training Epoch 40/1000 Loss:3550.38145 con:0.00001 recon1:1364.29006 recon2:2186.09138
Validation...
	Batch 10/17 Loss:2208.53369 con:0.00000 recon1:1110.78589 recon2:1097.74792
Validation Epoch 40/1000 Loss:2280.07086 con:0.00000 recon1:1148.05526 recon2:1132.01564
Training...
	Batch 10/122 Loss:2599.30615 con:0.00000 recon1:1115.97412 recon2:1483.33191
	Batch 20/122 Loss:2063.98584 con:0.00000 recon1:976.69312 recon2:1087.29272
	Batch 30/122 Loss:2473.88086 con:0.00001 recon1:1230.59229 recon2:1243.28870
	Batch 40/122 Loss:2306.75195 con:0.00001 recon1:1218.83582 recon2:1087.91602
	Batch 50/122 Loss:2757.29248 con:0.00000 recon1:1114.94202 recon2:1642.35034
	Batch 60/122 Loss:1928.00854 con:0.00001 recon1:997.18335 recon2:930.82526
	Batch 70/122 Loss:2768.47852 con:0.00003 recon1:1309.47888 recon2:1458.99963
	Batch 80/122 Loss:2075.64648 con:0.00001 recon1:1025.40002 recon2:1050.24634
	Batch 90/122 Loss:2451.23975 con:0.00001 recon1:1011.82312 recon2:1439.41650
	Batch 100/122 Loss:2177.78247 con:0.00001 recon1:1185.15601 recon2:992.62640
	Batch 110/122 Loss:2033.87622 con:0.00001 recon1:998.04089 recon2:1035.83533
	Batch 120/122 Loss:2080.09106 con:0.00000 recon1:1109.85425 recon2:970.23682
Training Epoch 41/1000 Loss:13385.31219 con:0.00002 recon1:2583.39874 recon2:10801.91395
Validation...
	Batch 10/17 Loss:1967.18542 con:0.00000 recon1:987.76581 recon2:979.41962
Validation Epoch 41/1000 Loss:2034.57615 con:0.00000 recon1:1023.67147 recon2:1010.90470
Training...
	Batch 10/122 Loss:1791.78992 con:0.00000 recon1:952.15039 recon2:839.63953
	Batch 20/122 Loss:1813.10229 con:0.00000 recon1:871.12128 recon2:941.98108
	Batch 30/122 Loss:2099.55542 con:0.00000 recon1:1310.70190 recon2:788.85345
	Batch 40/122 Loss:1763.55713 con:0.00000 recon1:776.55304 recon2:987.00415
	Batch 50/122 Loss:1507.48254 con:0.00000 recon1:771.51086 recon2:735.97168
	Batch 60/122 Loss:1494.24719 con:0.00001 recon1:787.85919 recon2:706.38800
	Batch 70/122 Loss:1493.71814 con:0.00001 recon1:711.82300 recon2:781.89514
	Batch 80/122 Loss:1498.98828 con:0.00000 recon1:706.03271 recon2:792.95557
	Batch 90/122 Loss:1686.02771 con:0.00001 recon1:793.96222 recon2:892.06549
	Batch 100/122 Loss:1652.52173 con:0.00000 recon1:778.80286 recon2:873.71893
	Batch 110/122 Loss:1611.25757 con:0.00001 recon1:739.30682 recon2:871.95074
	Batch 120/122 Loss:1782.62341 con:0.00002 recon1:1138.47888 recon2:644.14453
Training Epoch 42/1000 Loss:1847.94264 con:0.00001 recon1:972.22521 recon2:875.71742
Validation...
	Batch 10/17 Loss:1435.09644 con:0.00000 recon1:723.54773 recon2:711.54871
Validation Epoch 42/1000 Loss:1483.91708 con:0.00000 recon1:746.30028 recon2:737.61681
Training...
	Batch 10/122 Loss:1576.10303 con:0.00002 recon1:780.19507 recon2:795.90790
	Batch 20/122 Loss:1400.48291 con:0.00002 recon1:758.21503 recon2:642.26788
	Batch 30/122 Loss:1552.89600 con:0.00001 recon1:893.63171 recon2:659.26428
	Batch 40/122 Loss:1809.49194 con:0.00001 recon1:1091.59668 recon2:717.89532
	Batch 50/122 Loss:1497.66528 con:0.00001 recon1:709.67413 recon2:787.99109
	Batch 60/122 Loss:1344.25366 con:0.00001 recon1:724.15723 recon2:620.09644
	Batch 70/122 Loss:1546.70312 con:0.00000 recon1:767.26746 recon2:779.43567
	Batch 80/122 Loss:1590.74292 con:0.00001 recon1:783.66174 recon2:807.08124
	Batch 90/122 Loss:1464.33374 con:0.00000 recon1:733.99304 recon2:730.34064
	Batch 100/122 Loss:1529.29883 con:0.00000 recon1:927.83472 recon2:601.46405
	Batch 110/122 Loss:1315.44360 con:0.00001 recon1:697.59204 recon2:617.85150
	Batch 120/122 Loss:1467.38965 con:0.00000 recon1:793.77191 recon2:673.61768
Training Epoch 43/1000 Loss:1572.46519 con:0.00001 recon1:779.90283 recon2:792.56235
Validation...
	Batch 10/17 Loss:1318.32947 con:0.00000 recon1:661.52496 recon2:656.80450
Validation Epoch 43/1000 Loss:1358.83524 con:0.00000 recon1:682.47018 recon2:676.36506
Training...
	Batch 10/122 Loss:1483.61792 con:0.00001 recon1:814.86316 recon2:668.75470
	Batch 20/122 Loss:1911.15991 con:0.00002 recon1:1188.67102 recon2:722.48883
	Batch 30/122 Loss:1662.11377 con:0.00001 recon1:779.95544 recon2:882.15826
	Batch 40/122 Loss:1643.31763 con:0.00001 recon1:1046.27271 recon2:597.04498
	Batch 50/122 Loss:1232.43579 con:0.00000 recon1:638.42657 recon2:594.00916
	Batch 60/122 Loss:1235.81433 con:0.00001 recon1:617.52734 recon2:618.28699
	Batch 70/122 Loss:1381.76392 con:0.00000 recon1:657.99713 recon2:723.76678
	Batch 80/122 Loss:1387.06665 con:0.00001 recon1:618.64209 recon2:768.42450
	Batch 90/122 Loss:1494.96326 con:0.00003 recon1:837.12921 recon2:657.83405
	Batch 100/122 Loss:1598.42871 con:0.00001 recon1:910.75488 recon2:687.67383
	Batch 110/122 Loss:1290.39209 con:0.00000 recon1:659.45831 recon2:630.93378
	Batch 120/122 Loss:1445.25732 con:0.00000 recon1:633.59540 recon2:811.66193
Training Epoch 44/1000 Loss:1690.50378 con:0.00001 recon1:881.95566 recon2:808.54811
Validation...
	Batch 10/17 Loss:1232.24609 con:0.00000 recon1:623.82587 recon2:608.42023
Validation Epoch 44/1000 Loss:1277.07163 con:0.00000 recon1:642.49740 recon2:634.57423
Training...
	Batch 10/122 Loss:1170.44214 con:0.00000 recon1:579.18787 recon2:591.25427
	Batch 20/122 Loss:1400.79858 con:0.00001 recon1:703.69855 recon2:697.10010
	Batch 30/122 Loss:1179.37720 con:0.00000 recon1:631.40015 recon2:547.97705
	Batch 40/122 Loss:1289.32251 con:0.00000 recon1:721.28998 recon2:568.03253
	Batch 50/122 Loss:1369.39404 con:0.00001 recon1:709.74756 recon2:659.64642
	Batch 60/122 Loss:1130.35083 con:0.00001 recon1:558.82178 recon2:571.52911
	Batch 70/122 Loss:1248.91455 con:0.00001 recon1:688.94904 recon2:559.96545
	Batch 80/122 Loss:1153.45068 con:0.00000 recon1:577.47974 recon2:575.97095
	Batch 90/122 Loss:1190.12012 con:0.00000 recon1:638.92542 recon2:551.19470
	Batch 100/122 Loss:1236.90625 con:0.00000 recon1:559.71606 recon2:677.19025
	Batch 110/122 Loss:1231.45728 con:0.00000 recon1:568.34320 recon2:663.11401
	Batch 120/122 Loss:1525.53101 con:0.00002 recon1:895.07538 recon2:630.45557
Training Epoch 45/1000 Loss:1325.52194 con:0.00001 recon1:658.92660 recon2:666.59534
Validation...
	Batch 10/17 Loss:1116.89185 con:0.00000 recon1:559.62323 recon2:557.26862
Validation Epoch 45/1000 Loss:1150.73593 con:0.00000 recon1:577.41279 recon2:573.32315
Training...
	Batch 10/122 Loss:1112.04395 con:0.00001 recon1:523.88794 recon2:588.15601
	Batch 20/122 Loss:1151.09717 con:0.00001 recon1:552.11755 recon2:598.97961
	Batch 30/122 Loss:1218.07959 con:0.00000 recon1:620.06598 recon2:598.01367
	Batch 40/122 Loss:1079.98877 con:0.00000 recon1:532.81445 recon2:547.17426
	Batch 50/122 Loss:1102.60852 con:0.00001 recon1:519.33472 recon2:583.27380
	Batch 60/122 Loss:1150.76416 con:0.00001 recon1:544.24786 recon2:606.51630
	Batch 70/122 Loss:1241.07861 con:0.00001 recon1:662.15436 recon2:578.92419
	Batch 80/122 Loss:1148.83899 con:0.00001 recon1:573.45221 recon2:575.38678
	Batch 90/122 Loss:1055.47168 con:0.00000 recon1:520.67365 recon2:534.79797
	Batch 100/122 Loss:1175.96497 con:0.00000 recon1:493.96353 recon2:682.00140
	Batch 110/122 Loss:1118.68286 con:0.00002 recon1:562.24194 recon2:556.44086
	Batch 120/122 Loss:1076.43323 con:0.00000 recon1:573.41382 recon2:503.01944
Training Epoch 46/1000 Loss:1207.73644 con:0.00001 recon1:606.10992 recon2:601.62652
Validation...
	Batch 10/17 Loss:1032.03101 con:0.00000 recon1:521.13177 recon2:510.89923
Validation Epoch 46/1000 Loss:1068.98199 con:0.00000 recon1:536.33201 recon2:532.65001
Training...
	Batch 10/122 Loss:1350.78821 con:0.00000 recon1:677.64929 recon2:673.13892
	Batch 20/122 Loss:1098.41199 con:0.00001 recon1:589.30865 recon2:509.10330
	Batch 30/122 Loss:1118.28735 con:0.00000 recon1:636.01929 recon2:482.26813
	Batch 40/122 Loss:1120.98694 con:0.00001 recon1:611.20746 recon2:509.77945
	Batch 50/122 Loss:1128.18591 con:0.00001 recon1:562.74371 recon2:565.44220
	Batch 60/122 Loss:1071.84998 con:0.00000 recon1:565.22803 recon2:506.62192
	Batch 70/122 Loss:993.48279 con:0.00001 recon1:476.53040 recon2:516.95239
	Batch 80/122 Loss:1029.41980 con:0.00000 recon1:534.33978 recon2:495.08005
	Batch 90/122 Loss:1250.28687 con:0.00000 recon1:634.70068 recon2:615.58612
	Batch 100/122 Loss:1197.54907 con:0.00001 recon1:544.51996 recon2:653.02917
	Batch 110/122 Loss:1195.58154 con:0.00002 recon1:533.60590 recon2:661.97565
	Batch 120/122 Loss:1051.03381 con:0.00000 recon1:586.08875 recon2:464.94510
Training Epoch 47/1000 Loss:1332.27654 con:0.00001 recon1:742.75127 recon2:589.52527
Validation...
	Batch 10/17 Loss:957.52863 con:0.00000 recon1:484.16565 recon2:473.36298
Validation Epoch 47/1000 Loss:991.04177 con:0.00000 recon1:497.68115 recon2:493.36062
Training...
	Batch 10/122 Loss:1135.18762 con:0.00000 recon1:676.38129 recon2:458.80634
	Batch 20/122 Loss:936.58289 con:0.00000 recon1:455.44089 recon2:481.14200
	Batch 30/122 Loss:998.23230 con:0.00001 recon1:520.82367 recon2:477.40863
	Batch 40/122 Loss:1039.69067 con:0.00000 recon1:483.74298 recon2:555.94769
	Batch 50/122 Loss:1031.45789 con:0.00002 recon1:545.31177 recon2:486.14615
	Batch 60/122 Loss:1085.28394 con:0.00001 recon1:443.75009 recon2:641.53381
	Batch 70/122 Loss:1005.22058 con:0.00001 recon1:574.85681 recon2:430.36374
	Batch 80/122 Loss:971.66467 con:0.00001 recon1:531.03839 recon2:440.62631
	Batch 90/122 Loss:1010.42224 con:0.00002 recon1:420.57727 recon2:589.84497
	Batch 100/122 Loss:1070.37036 con:0.00000 recon1:528.59338 recon2:541.77692
	Batch 110/122 Loss:922.14142 con:0.00000 recon1:443.21375 recon2:478.92767
	Batch 120/122 Loss:1007.38690 con:0.00000 recon1:502.40366 recon2:504.98325
Training Epoch 48/1000 Loss:1151.90015 con:0.00001 recon1:606.70222 recon2:545.19792
Validation...
	Batch 10/17 Loss:874.81403 con:0.00000 recon1:442.33795 recon2:432.47607
Validation Epoch 48/1000 Loss:907.68752 con:0.00000 recon1:455.51101 recon2:452.17652
Training...
	Batch 10/122 Loss:886.17316 con:0.00001 recon1:402.80954 recon2:483.36362
	Batch 20/122 Loss:807.53699 con:0.00000 recon1:405.94266 recon2:401.59436
	Batch 30/122 Loss:1033.92554 con:0.00000 recon1:544.57416 recon2:489.35135
	Batch 40/122 Loss:890.85931 con:0.00001 recon1:441.21960 recon2:449.63971
	Batch 50/122 Loss:805.26611 con:0.00001 recon1:403.54224 recon2:401.72391
	Batch 60/122 Loss:921.45581 con:0.00002 recon1:436.86151 recon2:484.59427
	Batch 70/122 Loss:883.76611 con:0.00001 recon1:458.02725 recon2:425.73889
	Batch 80/122 Loss:1051.51147 con:0.00001 recon1:546.31677 recon2:505.19473
	Batch 90/122 Loss:842.11206 con:0.00000 recon1:451.75668 recon2:390.35538
	Batch 100/122 Loss:885.16772 con:0.00002 recon1:428.64618 recon2:456.52148
	Batch 110/122 Loss:843.99060 con:0.00001 recon1:413.15417 recon2:430.83646
	Batch 120/122 Loss:988.48315 con:0.00000 recon1:424.00998 recon2:564.47321
Training Epoch 49/1000 Loss:1638.57232 con:0.00001 recon1:834.28159 recon2:804.29070
Validation...
	Batch 10/17 Loss:798.90271 con:0.00000 recon1:405.98315 recon2:392.91953
Validation Epoch 49/1000 Loss:830.07373 con:0.00000 recon1:417.04903 recon2:413.02469
Training...
	Batch 10/122 Loss:792.88153 con:0.00000 recon1:404.14258 recon2:388.73895
	Batch 20/122 Loss:847.38477 con:0.00000 recon1:452.61102 recon2:394.77371
	Batch 30/122 Loss:714.04272 con:0.00001 recon1:367.94357 recon2:346.09915
	Batch 40/122 Loss:751.70410 con:0.00002 recon1:352.25186 recon2:399.45227
	Batch 50/122 Loss:766.91003 con:0.00000 recon1:411.55759 recon2:355.35245
	Batch 60/122 Loss:753.20569 con:0.00000 recon1:354.36456 recon2:398.84116
	Batch 70/122 Loss:730.67993 con:0.00001 recon1:392.77655 recon2:337.90341
	Batch 80/122 Loss:706.08826 con:0.00000 recon1:332.98389 recon2:373.10437
	Batch 90/122 Loss:797.67261 con:0.00000 recon1:375.64581 recon2:422.02679
	Batch 100/122 Loss:690.56128 con:0.00001 recon1:339.48938 recon2:351.07187
	Batch 110/122 Loss:752.58722 con:0.00001 recon1:386.91281 recon2:365.67441
	Batch 120/122 Loss:1032.66675 con:0.00001 recon1:329.28555 recon2:703.38123
Training Epoch 50/1000 Loss:864.07637 con:0.00001 recon1:416.16322 recon2:447.91315
Validation...
	Batch 10/17 Loss:697.45508 con:0.00000 recon1:351.31927 recon2:346.13580
Validation Epoch 50/1000 Loss:722.88586 con:0.00000 recon1:362.21168 recon2:360.67418
Training...
	Batch 10/122 Loss:674.54626 con:0.00001 recon1:336.53094 recon2:338.01532
	Batch 20/122 Loss:763.57745 con:0.00001 recon1:430.63904 recon2:332.93842
	Batch 30/122 Loss:892.74493 con:0.00000 recon1:371.17853 recon2:521.56641
	Batch 40/122 Loss:716.56952 con:0.00001 recon1:365.11237 recon2:351.45715
	Batch 50/122 Loss:784.17468 con:0.00001 recon1:346.10632 recon2:438.06836
	Batch 60/122 Loss:658.66821 con:0.00001 recon1:314.66443 recon2:344.00378
	Batch 70/122 Loss:893.54077 con:0.00000 recon1:460.31952 recon2:433.22128
	Batch 80/122 Loss:701.50153 con:0.00000 recon1:297.79236 recon2:403.70917
	Batch 90/122 Loss:669.12311 con:0.00001 recon1:355.37369 recon2:313.74942
	Batch 100/122 Loss:808.26910 con:0.00001 recon1:452.23529 recon2:356.03381
	Batch 110/122 Loss:709.19000 con:0.00001 recon1:333.97046 recon2:375.21954
	Batch 120/122 Loss:714.96082 con:0.00002 recon1:322.92557 recon2:392.03522
Training Epoch 51/1000 Loss:813.65533 con:0.00001 recon1:386.12726 recon2:427.52807
Validation...
	Batch 10/17 Loss:639.48792 con:0.00000 recon1:322.44516 recon2:317.04272
Validation Epoch 51/1000 Loss:664.16749 con:0.00000 recon1:332.67459 recon2:331.49291
Training...
	Batch 10/122 Loss:683.56787 con:0.00000 recon1:379.07190 recon2:304.49597
	Batch 20/122 Loss:668.76294 con:0.00001 recon1:331.75571 recon2:337.00726
	Batch 30/122 Loss:789.16602 con:0.00001 recon1:462.24820 recon2:326.91785
	Batch 40/122 Loss:649.70081 con:0.00000 recon1:357.07382 recon2:292.62695
	Batch 50/122 Loss:668.81152 con:0.00000 recon1:314.23077 recon2:354.58078
	Batch 60/122 Loss:631.12158 con:0.00002 recon1:314.98181 recon2:316.13977
	Batch 70/122 Loss:895.66931 con:0.00001 recon1:404.56219 recon2:491.10709
	Batch 80/122 Loss:802.30652 con:0.00003 recon1:485.74557 recon2:316.56094
	Batch 90/122 Loss:596.24835 con:0.00001 recon1:257.75916 recon2:338.48920
	Batch 100/122 Loss:606.54834 con:0.00000 recon1:316.40433 recon2:290.14404
	Batch 110/122 Loss:652.55017 con:0.00002 recon1:329.93091 recon2:322.61926
	Batch 120/122 Loss:669.72681 con:0.00001 recon1:394.46579 recon2:275.26105
Training Epoch 52/1000 Loss:755.24445 con:0.00001 recon1:363.88176 recon2:391.36269
Validation...
	Batch 10/17 Loss:584.75391 con:0.00000 recon1:297.37613 recon2:287.37778
Validation Epoch 52/1000 Loss:608.98073 con:0.00000 recon1:305.32916 recon2:303.65157
Training...
	Batch 10/122 Loss:614.12091 con:0.00000 recon1:308.48694 recon2:305.63397
	Batch 20/122 Loss:572.87390 con:0.00001 recon1:285.48648 recon2:287.38742
	Batch 30/122 Loss:647.44788 con:0.00000 recon1:315.94550 recon2:331.50235
	Batch 40/122 Loss:638.79492 con:0.00000 recon1:297.75931 recon2:341.03561
	Batch 50/122 Loss:942.14636 con:0.00001 recon1:673.31238 recon2:268.83401
	Batch 60/122 Loss:573.87646 con:0.00000 recon1:273.40921 recon2:300.46729
	Batch 70/122 Loss:569.11517 con:0.00000 recon1:283.35611 recon2:285.75906
	Batch 80/122 Loss:763.99304 con:0.00000 recon1:285.61957 recon2:478.37350
	Batch 90/122 Loss:564.98364 con:0.00000 recon1:302.32587 recon2:262.65775
	Batch 100/122 Loss:689.25000 con:0.00000 recon1:334.31302 recon2:354.93701
	Batch 110/122 Loss:653.02655 con:0.00001 recon1:313.98621 recon2:339.04034
	Batch 120/122 Loss:600.41052 con:0.00001 recon1:320.69879 recon2:279.71170
Training Epoch 53/1000 Loss:685.05387 con:0.00001 recon1:349.18515 recon2:335.86872
Validation...
	Batch 10/17 Loss:525.59943 con:0.00000 recon1:266.04565 recon2:259.55377
Validation Epoch 53/1000 Loss:546.96351 con:0.00000 recon1:273.78683 recon2:273.17668
Training...
	Batch 10/122 Loss:500.77069 con:0.00001 recon1:247.66937 recon2:253.10132
	Batch 20/122 Loss:645.28259 con:0.00000 recon1:323.84805 recon2:321.43457
	Batch 30/122 Loss:602.51678 con:0.00001 recon1:249.79193 recon2:352.72485
	Batch 40/122 Loss:542.40698 con:0.00001 recon1:251.68646 recon2:290.72049
	Batch 50/122 Loss:533.88666 con:0.00000 recon1:233.87181 recon2:300.01483
	Batch 60/122 Loss:538.48840 con:0.00000 recon1:289.71375 recon2:248.77469
	Batch 70/122 Loss:1083.89697 con:0.00001 recon1:339.58121 recon2:744.31573
	Batch 80/122 Loss:481.54810 con:0.00000 recon1:249.42105 recon2:232.12704
	Batch 90/122 Loss:526.48828 con:0.00001 recon1:243.84225 recon2:282.64606
	Batch 100/122 Loss:600.01611 con:0.00001 recon1:293.60669 recon2:306.40939
	Batch 110/122 Loss:484.93933 con:0.00000 recon1:225.04921 recon2:259.89011
	Batch 120/122 Loss:521.80481 con:0.00001 recon1:260.80200 recon2:261.00281
Training Epoch 54/1000 Loss:611.01359 con:0.00001 recon1:303.82957 recon2:307.18402
Validation...
	Batch 10/17 Loss:481.64026 con:0.00000 recon1:244.33780 recon2:237.30247
Validation Epoch 54/1000 Loss:501.22436 con:0.00000 recon1:250.85164 recon2:250.37271
Training...
	Batch 10/122 Loss:634.09100 con:0.00003 recon1:297.28748 recon2:336.80350
	Batch 20/122 Loss:525.46515 con:0.00001 recon1:273.73740 recon2:251.72775
	Batch 30/122 Loss:517.39600 con:0.00001 recon1:296.07962 recon2:221.31641
	Batch 40/122 Loss:554.61200 con:0.00001 recon1:235.02583 recon2:319.58615
	Batch 50/122 Loss:447.43628 con:0.00001 recon1:208.90578 recon2:238.53050
	Batch 60/122 Loss:480.25433 con:0.00002 recon1:239.28975 recon2:240.96455
	Batch 70/122 Loss:595.17883 con:0.00001 recon1:309.82346 recon2:285.35538
	Batch 80/122 Loss:438.77280 con:0.00000 recon1:208.63214 recon2:230.14066
	Batch 90/122 Loss:457.26740 con:0.00000 recon1:247.08580 recon2:210.18158
	Batch 100/122 Loss:431.81247 con:0.00000 recon1:209.79886 recon2:222.01361
	Batch 110/122 Loss:426.26025 con:0.00000 recon1:206.17624 recon2:220.08401
	Batch 120/122 Loss:444.03235 con:0.00000 recon1:227.42921 recon2:216.60315
Training Epoch 55/1000 Loss:6112.53361 con:0.00002 recon1:264.73573 recon2:5847.79766
Validation...
	Batch 10/17 Loss:401.16849 con:0.00000 recon1:203.71890 recon2:197.44958
Validation Epoch 55/1000 Loss:419.23911 con:0.00000 recon1:209.55795 recon2:209.68116
Training...
	Batch 10/122 Loss:246.57950 con:0.00001 recon1:120.39175 recon2:126.18773
	Batch 20/122 Loss:216.05841 con:0.00000 recon1:131.59966 recon2:84.45875
	Batch 30/122 Loss:224.30064 con:0.00002 recon1:81.19829 recon2:143.10234
	Batch 40/122 Loss:144.79607 con:0.00000 recon1:72.04501 recon2:72.75105
	Batch 50/122 Loss:163.66592 con:0.00000 recon1:89.82080 recon2:73.84513
	Batch 60/122 Loss:162.32465 con:0.00001 recon1:85.65140 recon2:76.67323
	Batch 70/122 Loss:152.89737 con:0.00000 recon1:71.39004 recon2:81.50733
	Batch 80/122 Loss:167.32607 con:0.00001 recon1:90.82852 recon2:76.49754
	Batch 90/122 Loss:180.95001 con:0.00001 recon1:112.30014 recon2:68.64987
	Batch 100/122 Loss:159.69644 con:0.00000 recon1:87.06120 recon2:72.63525
	Batch 110/122 Loss:255.72842 con:0.00001 recon1:163.45567 recon2:92.27274
	Batch 120/122 Loss:201.66946 con:0.00001 recon1:109.07256 recon2:92.59689
Training Epoch 56/1000 Loss:219.75330 con:0.00001 recon1:112.01508 recon2:107.73822
Validation...
	Batch 10/17 Loss:147.06046 con:0.00000 recon1:75.67453 recon2:71.38593
Validation Epoch 56/1000 Loss:157.04484 con:0.00000 recon1:77.64860 recon2:79.39624
Training...
	Batch 10/122 Loss:155.06842 con:0.00000 recon1:78.69512 recon2:76.37330
	Batch 20/122 Loss:178.99123 con:0.00000 recon1:87.76682 recon2:91.22441
	Batch 30/122 Loss:153.41370 con:0.00000 recon1:83.49631 recon2:69.91738
	Batch 40/122 Loss:296.66406 con:0.00003 recon1:142.27380 recon2:154.39024
	Batch 50/122 Loss:137.85022 con:0.00000 recon1:69.13078 recon2:68.71945
	Batch 60/122 Loss:171.51584 con:0.00001 recon1:75.85694 recon2:95.65890
	Batch 70/122 Loss:159.42760 con:0.00001 recon1:89.19981 recon2:70.22777
	Batch 80/122 Loss:152.77341 con:0.00000 recon1:83.86538 recon2:68.90804
	Batch 90/122 Loss:139.72731 con:0.00000 recon1:64.78284 recon2:74.94447
	Batch 100/122 Loss:168.61584 con:0.00000 recon1:92.39957 recon2:76.21626
	Batch 110/122 Loss:166.12299 con:0.00001 recon1:63.54758 recon2:102.57539
	Batch 120/122 Loss:162.04776 con:0.00000 recon1:85.79224 recon2:76.25551
Training Epoch 57/1000 Loss:324.86163 con:0.00001 recon1:93.46314 recon2:231.39848
Validation...
	Batch 10/17 Loss:136.33569 con:0.00000 recon1:69.88157 recon2:66.45412
Validation Epoch 57/1000 Loss:145.52906 con:0.00000 recon1:71.96414 recon2:73.56492
Training...
	Batch 10/122 Loss:180.57187 con:0.00002 recon1:78.37153 recon2:102.20033
	Batch 20/122 Loss:128.33640 con:0.00000 recon1:65.65539 recon2:62.68100
	Batch 30/122 Loss:126.98668 con:0.00000 recon1:61.31223 recon2:65.67444
	Batch 40/122 Loss:128.62564 con:0.00000 recon1:66.78640 recon2:61.83924
	Batch 50/122 Loss:136.45027 con:0.00000 recon1:70.27858 recon2:66.17169
	Batch 60/122 Loss:156.77786 con:0.00000 recon1:95.64681 recon2:61.13105
	Batch 70/122 Loss:145.71924 con:0.00000 recon1:69.52696 recon2:76.19227
	Batch 80/122 Loss:145.61530 con:0.00001 recon1:79.37389 recon2:66.24139
	Batch 90/122 Loss:140.83403 con:0.00000 recon1:74.03458 recon2:66.79945
	Batch 100/122 Loss:164.24918 con:0.00001 recon1:94.44702 recon2:69.80215
	Batch 110/122 Loss:156.33420 con:0.00000 recon1:93.39061 recon2:62.94357
	Batch 120/122 Loss:132.66473 con:0.00000 recon1:77.43875 recon2:55.22598
Training Epoch 58/1000 Loss:211.01433 con:0.00001 recon1:84.41741 recon2:126.59691
Validation...
	Batch 10/17 Loss:125.80413 con:0.00000 recon1:65.29211 recon2:60.51202
Validation Epoch 58/1000 Loss:135.38142 con:0.00000 recon1:67.05529 recon2:68.32614
Training...
	Batch 10/122 Loss:144.66740 con:0.00000 recon1:70.35178 recon2:74.31563
	Batch 20/122 Loss:133.25241 con:0.00001 recon1:56.92377 recon2:76.32862
	Batch 30/122 Loss:142.78024 con:0.00001 recon1:64.64430 recon2:78.13593
	Batch 40/122 Loss:118.99233 con:0.00000 recon1:61.61028 recon2:57.38205
	Batch 50/122 Loss:158.52777 con:0.00000 recon1:63.39466 recon2:95.13309
	Batch 60/122 Loss:165.78897 con:0.00001 recon1:98.97911 recon2:66.80984
	Batch 70/122 Loss:141.25299 con:0.00000 recon1:68.85551 recon2:72.39748
	Batch 80/122 Loss:162.52301 con:0.00001 recon1:73.81108 recon2:88.71192
	Batch 90/122 Loss:151.25012 con:0.00001 recon1:79.64200 recon2:71.60812
	Batch 100/122 Loss:130.26373 con:0.00001 recon1:61.28596 recon2:68.97775
	Batch 110/122 Loss:139.03729 con:0.00000 recon1:82.41383 recon2:56.62345
	Batch 120/122 Loss:132.77496 con:0.00001 recon1:59.82125 recon2:72.95371
Training Epoch 59/1000 Loss:157.62142 con:0.00001 recon1:78.04495 recon2:79.57646
Validation...
	Batch 10/17 Loss:115.33844 con:0.00000 recon1:59.01432 recon2:56.32412
Validation Epoch 59/1000 Loss:123.38683 con:0.00000 recon1:60.65455 recon2:62.73228
Training...
	Batch 10/122 Loss:120.24358 con:0.00001 recon1:65.24876 recon2:54.99481
	Batch 20/122 Loss:121.71079 con:0.00000 recon1:61.27051 recon2:60.44028
	Batch 30/122 Loss:156.89044 con:0.00000 recon1:61.56470 recon2:95.32574
	Batch 40/122 Loss:182.36090 con:0.00001 recon1:67.64221 recon2:114.71868
	Batch 50/122 Loss:127.30041 con:0.00000 recon1:67.68038 recon2:59.62003
	Batch 60/122 Loss:154.63152 con:0.00001 recon1:66.68553 recon2:87.94598
	Batch 70/122 Loss:134.21466 con:0.00000 recon1:60.70506 recon2:73.50960
	Batch 80/122 Loss:113.77250 con:0.00000 recon1:52.11675 recon2:61.65575
	Batch 90/122 Loss:111.30735 con:0.00000 recon1:48.58382 recon2:62.72353
	Batch 100/122 Loss:119.29335 con:0.00000 recon1:53.55735 recon2:65.73599
	Batch 110/122 Loss:125.93816 con:0.00001 recon1:66.10625 recon2:59.83190
	Batch 120/122 Loss:143.76062 con:0.00000 recon1:79.23201 recon2:64.52860
Training Epoch 60/1000 Loss:285.27206 con:0.00001 recon1:209.96855 recon2:75.30350
Validation...
	Batch 10/17 Loss:108.21387 con:0.00000 recon1:55.60751 recon2:52.60635
Validation Epoch 60/1000 Loss:115.95564 con:0.00000 recon1:57.13571 recon2:58.81993
Training...
	Batch 10/122 Loss:123.96004 con:0.00000 recon1:67.77990 recon2:56.18013
	Batch 20/122 Loss:139.02748 con:0.00000 recon1:69.59460 recon2:69.43288
	Batch 30/122 Loss:130.68263 con:0.00001 recon1:79.65544 recon2:51.02718
	Batch 40/122 Loss:103.78413 con:0.00000 recon1:56.67524 recon2:47.10889
	Batch 50/122 Loss:115.64073 con:0.00000 recon1:65.19928 recon2:50.44145
	Batch 60/122 Loss:107.77388 con:0.00000 recon1:46.41446 recon2:61.35941
	Batch 70/122 Loss:143.54971 con:0.00001 recon1:65.36316 recon2:78.18653
	Batch 80/122 Loss:133.35765 con:0.00000 recon1:75.12025 recon2:58.23740
	Batch 90/122 Loss:138.51202 con:0.00001 recon1:53.93575 recon2:84.57626
	Batch 100/122 Loss:134.79637 con:0.00001 recon1:49.28225 recon2:85.51411
	Batch 110/122 Loss:140.91978 con:0.00000 recon1:59.42121 recon2:81.49857
	Batch 120/122 Loss:131.14806 con:0.00001 recon1:58.57994 recon2:72.56811
Training Epoch 61/1000 Loss:510.92333 con:0.00001 recon1:67.19280 recon2:443.73052
Validation...
	Batch 10/17 Loss:97.40302 con:0.00000 recon1:50.23598 recon2:47.16704
Validation Epoch 61/1000 Loss:104.74806 con:0.00000 recon1:51.51781 recon2:53.23025
Training...
	Batch 10/122 Loss:103.11395 con:0.00000 recon1:54.88099 recon2:48.23296
	Batch 20/122 Loss:100.69438 con:0.00000 recon1:60.07141 recon2:40.62296
	Batch 30/122 Loss:99.18031 con:0.00001 recon1:43.53162 recon2:55.64869
	Batch 40/122 Loss:188.40097 con:0.00002 recon1:96.90340 recon2:91.49756
	Batch 50/122 Loss:89.13821 con:0.00000 recon1:42.90274 recon2:46.23547
	Batch 60/122 Loss:86.72803 con:0.00000 recon1:39.55490 recon2:47.17312
	Batch 70/122 Loss:107.63676 con:0.00000 recon1:52.19607 recon2:55.44070
	Batch 80/122 Loss:106.45573 con:0.00001 recon1:48.42081 recon2:58.03492
	Batch 90/122 Loss:81.10120 con:0.00000 recon1:40.92111 recon2:40.18008
	Batch 100/122 Loss:90.21589 con:0.00000 recon1:49.36426 recon2:40.85163
	Batch 110/122 Loss:90.43090 con:0.00000 recon1:43.81968 recon2:46.61121
	Batch 120/122 Loss:143.46957 con:0.00001 recon1:47.18379 recon2:96.28577
Training Epoch 62/1000 Loss:150.28954 con:0.00001 recon1:63.91676 recon2:86.37277
Validation...
	Batch 10/17 Loss:82.80265 con:0.00000 recon1:42.58964 recon2:40.21301
Validation Epoch 62/1000 Loss:89.09961 con:0.00000 recon1:43.61062 recon2:45.48899
Training...
	Batch 10/122 Loss:132.98579 con:0.00001 recon1:44.30829 recon2:88.67750
	Batch 20/122 Loss:114.51179 con:0.00001 recon1:56.44525 recon2:58.06652
	Batch 30/122 Loss:100.06728 con:0.00000 recon1:40.16446 recon2:59.90282
	Batch 40/122 Loss:143.33940 con:0.00000 recon1:56.10381 recon2:87.23559
	Batch 50/122 Loss:221.18088 con:0.00002 recon1:170.57231 recon2:50.60854
	Batch 60/122 Loss:83.32625 con:0.00000 recon1:44.73987 recon2:38.58638
	Batch 70/122 Loss:78.72662 con:0.00000 recon1:41.39535 recon2:37.33127
	Batch 80/122 Loss:83.35124 con:0.00000 recon1:43.36517 recon2:39.98607
	Batch 90/122 Loss:108.57973 con:0.00000 recon1:69.88891 recon2:38.69083
	Batch 100/122 Loss:102.94852 con:0.00001 recon1:62.39166 recon2:40.55685
	Batch 110/122 Loss:81.34847 con:0.00000 recon1:42.17284 recon2:39.17563
	Batch 120/122 Loss:133.25011 con:0.00002 recon1:78.92641 recon2:54.32369
Training Epoch 63/1000 Loss:137.98777 con:0.00001 recon1:57.70522 recon2:80.28254
Validation...
	Batch 10/17 Loss:78.21559 con:0.00000 recon1:40.24331 recon2:37.97228
Validation Epoch 63/1000 Loss:84.34906 con:0.00000 recon1:41.32151 recon2:43.02755
Training...
	Batch 10/122 Loss:96.50484 con:0.00001 recon1:53.84195 recon2:42.66289
	Batch 20/122 Loss:134.09935 con:0.00002 recon1:75.74398 recon2:58.35535
	Batch 30/122 Loss:115.86609 con:0.00001 recon1:61.20126 recon2:54.66481
	Batch 40/122 Loss:74.79259 con:0.00000 recon1:39.32994 recon2:35.46264
	Batch 50/122 Loss:97.60974 con:0.00001 recon1:50.70705 recon2:46.90268
	Batch 60/122 Loss:90.38917 con:0.00001 recon1:52.11769 recon2:38.27147
	Batch 70/122 Loss:80.28651 con:0.00000 recon1:39.04138 recon2:41.24513
	Batch 80/122 Loss:104.73053 con:0.00001 recon1:60.76561 recon2:43.96492
	Batch 90/122 Loss:83.52568 con:0.00001 recon1:40.54149 recon2:42.98418
	Batch 100/122 Loss:118.30917 con:0.00001 recon1:62.56544 recon2:55.74373
	Batch 110/122 Loss:74.73687 con:0.00000 recon1:34.94029 recon2:39.79658
	Batch 120/122 Loss:101.87260 con:0.00001 recon1:33.04201 recon2:68.83058
Training Epoch 64/1000 Loss:194.61815 con:0.00001 recon1:61.42829 recon2:133.18985
Validation...
	Batch 10/17 Loss:72.78645 con:0.00000 recon1:37.52910 recon2:35.25736
Validation Epoch 64/1000 Loss:78.77483 con:0.00000 recon1:38.46378 recon2:40.31106
Training...
	Batch 10/122 Loss:108.97177 con:0.00000 recon1:64.54629 recon2:44.42549
	Batch 20/122 Loss:76.35826 con:0.00001 recon1:41.08161 recon2:35.27665
	Batch 30/122 Loss:197.75851 con:0.00002 recon1:156.90640 recon2:40.85209
	Batch 40/122 Loss:78.19298 con:0.00000 recon1:41.43250 recon2:36.76048
	Batch 50/122 Loss:101.80354 con:0.00001 recon1:54.34286 recon2:47.46067
	Batch 60/122 Loss:113.08562 con:0.00001 recon1:49.19296 recon2:63.89265
	Batch 70/122 Loss:69.86116 con:0.00000 recon1:34.45025 recon2:35.41092
	Batch 80/122 Loss:87.05655 con:0.00001 recon1:47.94712 recon2:39.10941
	Batch 90/122 Loss:71.99800 con:0.00000 recon1:32.67410 recon2:39.32391
	Batch 100/122 Loss:78.21832 con:0.00000 recon1:40.10213 recon2:38.11618
	Batch 110/122 Loss:165.14409 con:0.00001 recon1:108.43337 recon2:56.71070
	Batch 120/122 Loss:89.36740 con:0.00000 recon1:45.59361 recon2:43.77378
Training Epoch 65/1000 Loss:141.63132 con:0.00001 recon1:87.96236 recon2:53.66895
Validation...
	Batch 10/17 Loss:68.15887 con:0.00000 recon1:35.17534 recon2:32.98352
Validation Epoch 65/1000 Loss:73.79267 con:0.00000 recon1:36.02016 recon2:37.77251
Training...
	Batch 10/122 Loss:125.05040 con:0.00002 recon1:71.33083 recon2:53.71955
	Batch 20/122 Loss:63.96928 con:0.00000 recon1:33.97786 recon2:29.99143
	Batch 30/122 Loss:81.20083 con:0.00000 recon1:34.85325 recon2:46.34757
	Batch 40/122 Loss:73.92398 con:0.00000 recon1:33.29523 recon2:40.62874
	Batch 50/122 Loss:76.17462 con:0.00000 recon1:40.85668 recon2:35.31794
	Batch 60/122 Loss:149.31717 con:0.00001 recon1:54.05082 recon2:95.26634
	Batch 70/122 Loss:98.41470 con:0.00000 recon1:40.50200 recon2:57.91271
	Batch 80/122 Loss:68.12350 con:0.00000 recon1:35.27299 recon2:32.85051
	Batch 90/122 Loss:87.76830 con:0.00001 recon1:39.96873 recon2:47.79956
	Batch 100/122 Loss:103.80873 con:0.00001 recon1:71.03523 recon2:32.77348
	Batch 110/122 Loss:91.29826 con:0.00001 recon1:47.52936 recon2:43.76889
	Batch 120/122 Loss:79.26211 con:0.00001 recon1:36.74550 recon2:42.51661
Training Epoch 66/1000 Loss:510.66552 con:0.00002 recon1:73.90407 recon2:436.76143
Validation...
	Batch 10/17 Loss:62.15014 con:0.00000 recon1:31.98838 recon2:30.16176
Validation Epoch 66/1000 Loss:67.95170 con:0.00000 recon1:32.82281 recon2:35.12889
Training...
	Batch 10/122 Loss:59.66073 con:0.00000 recon1:31.06442 recon2:28.59631
	Batch 20/122 Loss:69.80908 con:0.00000 recon1:29.11676 recon2:40.69233
	Batch 30/122 Loss:70.57938 con:0.00001 recon1:23.81382 recon2:46.76555
	Batch 40/122 Loss:55.50879 con:0.00000 recon1:31.90903 recon2:23.59975
	Batch 50/122 Loss:79.65366 con:0.00001 recon1:29.23995 recon2:50.41370
	Batch 60/122 Loss:131.78165 con:0.00001 recon1:103.60434 recon2:28.17730
	Batch 70/122 Loss:60.55386 con:0.00001 recon1:29.09862 recon2:31.45524
	Batch 80/122 Loss:79.52213 con:0.00001 recon1:45.17197 recon2:34.35014
	Batch 90/122 Loss:91.65350 con:0.00001 recon1:27.34524 recon2:64.30825
	Batch 100/122 Loss:61.77118 con:0.00001 recon1:37.69079 recon2:24.08039
	Batch 110/122 Loss:55.58416 con:0.00000 recon1:25.28052 recon2:30.30364
	Batch 120/122 Loss:93.55612 con:0.00001 recon1:64.68793 recon2:28.86818
Training Epoch 67/1000 Loss:109.19340 con:0.00001 recon1:41.93568 recon2:67.25771
Validation...
	Batch 10/17 Loss:48.53443 con:0.00000 recon1:25.50725 recon2:23.02718
Validation Epoch 67/1000 Loss:53.97751 con:0.00000 recon1:26.07962 recon2:27.89789
Training...
	Batch 10/122 Loss:66.06329 con:0.00001 recon1:29.71636 recon2:36.34692
	Batch 20/122 Loss:60.60165 con:0.00001 recon1:31.46324 recon2:29.13841
	Batch 30/122 Loss:78.17538 con:0.00001 recon1:27.86991 recon2:50.30547
	Batch 40/122 Loss:59.18477 con:0.00000 recon1:30.09861 recon2:29.08616
	Batch 50/122 Loss:80.39523 con:0.00001 recon1:51.01088 recon2:29.38435
	Batch 60/122 Loss:49.73350 con:0.00000 recon1:23.30364 recon2:26.42985
	Batch 70/122 Loss:58.81767 con:0.00000 recon1:34.37183 recon2:24.44583
	Batch 80/122 Loss:74.39264 con:0.00001 recon1:49.35729 recon2:25.03534
	Batch 90/122 Loss:67.43881 con:0.00001 recon1:40.04315 recon2:27.39565
	Batch 100/122 Loss:56.44689 con:0.00000 recon1:24.92547 recon2:31.52142
	Batch 110/122 Loss:82.18584 con:0.00001 recon1:52.47686 recon2:29.70897
	Batch 120/122 Loss:167.82127 con:0.00002 recon1:51.35893 recon2:116.46232
Training Epoch 68/1000 Loss:1665.98730 con:0.00002 recon1:1622.97227 recon2:43.01502
Validation...
	Batch 10/17 Loss:48.23619 con:0.00000 recon1:25.19714 recon2:23.03905
Validation Epoch 68/1000 Loss:53.48919 con:0.00000 recon1:25.86804 recon2:27.62115
Training...
	Batch 10/122 Loss:67.24954 con:0.00001 recon1:30.53463 recon2:36.71491
	Batch 20/122 Loss:86.34856 con:0.00000 recon1:44.55563 recon2:41.79293
	Batch 30/122 Loss:82.37117 con:0.00000 recon1:44.94962 recon2:37.42155
	Batch 40/122 Loss:95.01366 con:0.00001 recon1:48.51759 recon2:46.49606
	Batch 50/122 Loss:144.50073 con:0.00002 recon1:46.49328 recon2:98.00742
	Batch 60/122 Loss:86.43929 con:0.00001 recon1:48.49715 recon2:37.94212
	Batch 70/122 Loss:104.00334 con:0.00001 recon1:54.36017 recon2:49.64317
	Batch 80/122 Loss:71.97411 con:0.00001 recon1:35.86541 recon2:36.10869
	Batch 90/122 Loss:165.31689 con:0.00003 recon1:80.89646 recon2:84.42041
	Batch 100/122 Loss:136.61331 con:0.00003 recon1:73.53524 recon2:63.07804
	Batch 110/122 Loss:92.87610 con:0.00000 recon1:40.93763 recon2:51.93847
	Batch 120/122 Loss:96.11658 con:0.00001 recon1:60.32348 recon2:35.79308
Training Epoch 69/1000 Loss:751.81739 con:0.00002 recon1:55.03168 recon2:696.78568
Validation...
	Batch 10/17 Loss:65.17569 con:0.00000 recon1:33.85926 recon2:31.31643
Validation Epoch 69/1000 Loss:71.28889 con:0.00000 recon1:34.92481 recon2:36.36408
Training...
	Batch 10/122 Loss:87.30472 con:0.00001 recon1:51.38775 recon2:35.91696
	Batch 20/122 Loss:68.42562 con:0.00001 recon1:26.69485 recon2:41.73076
	Batch 30/122 Loss:65.65319 con:0.00000 recon1:37.86324 recon2:27.78995
	Batch 40/122 Loss:52.56655 con:0.00000 recon1:30.12772 recon2:22.43883
	Batch 50/122 Loss:109.66785 con:0.00002 recon1:62.09938 recon2:47.56844
	Batch 60/122 Loss:45.32761 con:0.00000 recon1:20.63087 recon2:24.69674
	Batch 70/122 Loss:51.47695 con:0.00000 recon1:23.17291 recon2:28.30404
	Batch 80/122 Loss:72.32457 con:0.00001 recon1:24.71474 recon2:47.60982
	Batch 90/122 Loss:49.74014 con:0.00000 recon1:23.84779 recon2:25.89235
	Batch 100/122 Loss:47.33766 con:0.00000 recon1:23.00689 recon2:24.33076
	Batch 110/122 Loss:52.36453 con:0.00000 recon1:21.66049 recon2:30.70403
	Batch 120/122 Loss:51.44148 con:0.00000 recon1:25.38382 recon2:26.05766
Training Epoch 70/1000 Loss:82.15447 con:0.00001 recon1:40.54528 recon2:41.60918
Validation...
	Batch 10/17 Loss:44.11196 con:0.00000 recon1:22.71239 recon2:21.39957
Validation Epoch 70/1000 Loss:47.47089 con:0.00000 recon1:23.10963 recon2:24.36126
Training...
	Batch 10/122 Loss:90.09110 con:0.00001 recon1:22.04029 recon2:68.05080
	Batch 20/122 Loss:87.72578 con:0.00001 recon1:20.85481 recon2:66.87096
	Batch 30/122 Loss:46.65347 con:0.00000 recon1:21.40348 recon2:25.24998
	Batch 40/122 Loss:92.99406 con:0.00001 recon1:60.77913 recon2:32.21492
	Batch 50/122 Loss:48.53926 con:0.00000 recon1:25.25359 recon2:23.28567
	Batch 60/122 Loss:62.64608 con:0.00001 recon1:23.88327 recon2:38.76280
	Batch 70/122 Loss:111.42450 con:0.00002 recon1:20.87277 recon2:90.55171
	Batch 80/122 Loss:51.28065 con:0.00000 recon1:22.23100 recon2:29.04964
	Batch 90/122 Loss:52.24427 con:0.00000 recon1:31.46022 recon2:20.78405
	Batch 100/122 Loss:45.00398 con:0.00000 recon1:23.03454 recon2:21.96944
	Batch 110/122 Loss:88.25703 con:0.00002 recon1:43.43620 recon2:44.82081
	Batch 120/122 Loss:100.47048 con:0.00002 recon1:65.38175 recon2:35.08871
Training Epoch 71/1000 Loss:121.49535 con:0.00001 recon1:81.68636 recon2:39.80899
Validation...
	Batch 10/17 Loss:40.16948 con:0.00000 recon1:20.73516 recon2:19.43432
Validation Epoch 71/1000 Loss:43.91308 con:0.00000 recon1:21.12327 recon2:22.78982
Training...
	Batch 10/122 Loss:51.75082 con:0.00000 recon1:20.23706 recon2:31.51376
	Batch 20/122 Loss:87.38206 con:0.00001 recon1:27.62839 recon2:59.75365
	Batch 30/122 Loss:168.45937 con:0.00002 recon1:135.90135 recon2:32.55800
	Batch 40/122 Loss:90.52897 con:0.00001 recon1:71.46369 recon2:19.06527
	Batch 50/122 Loss:61.56658 con:0.00001 recon1:39.04408 recon2:22.52249
	Batch 60/122 Loss:98.91096 con:0.00002 recon1:27.09685 recon2:71.81409
	Batch 70/122 Loss:49.84541 con:0.00000 recon1:22.19706 recon2:27.64836
	Batch 80/122 Loss:128.24860 con:0.00002 recon1:31.54229 recon2:96.70628
	Batch 90/122 Loss:76.59457 con:0.00001 recon1:51.94772 recon2:24.64684
	Batch 100/122 Loss:48.31553 con:0.00000 recon1:27.24172 recon2:21.07381
	Batch 110/122 Loss:106.17090 con:0.00001 recon1:82.71612 recon2:23.45477
	Batch 120/122 Loss:76.96465 con:0.00001 recon1:21.02383 recon2:55.94081
Training Epoch 72/1000 Loss:383.66743 con:0.00001 recon1:210.41456 recon2:173.25286
Validation...
	Batch 10/17 Loss:37.59870 con:0.00000 recon1:19.32256 recon2:18.27613
Validation Epoch 72/1000 Loss:40.93113 con:0.00000 recon1:19.61781 recon2:21.31332
Training...
	Batch 10/122 Loss:69.27118 con:0.00001 recon1:32.70450 recon2:36.56667
	Batch 20/122 Loss:46.61037 con:0.00001 recon1:22.11552 recon2:24.49484
	Batch 30/122 Loss:38.85606 con:0.00000 recon1:19.77769 recon2:19.07837
	Batch 40/122 Loss:57.98656 con:0.00001 recon1:39.39027 recon2:18.59628
	Batch 50/122 Loss:51.98015 con:0.00001 recon1:31.43006 recon2:20.55008
	Batch 60/122 Loss:45.94627 con:0.00000 recon1:18.04442 recon2:27.90185
	Batch 70/122 Loss:52.81545 con:0.00001 recon1:19.33814 recon2:33.47731
	Batch 80/122 Loss:43.81843 con:0.00001 recon1:17.03761 recon2:26.78081
	Batch 90/122 Loss:119.00441 con:0.00001 recon1:19.36796 recon2:99.63644
	Batch 100/122 Loss:58.12637 con:0.00001 recon1:28.76538 recon2:29.36099
	Batch 110/122 Loss:59.62990 con:0.00001 recon1:18.88954 recon2:40.74035
	Batch 120/122 Loss:55.12634 con:0.00001 recon1:33.14214 recon2:21.98419
Training Epoch 73/1000 Loss:257.03767 con:0.00001 recon1:145.22278 recon2:111.81488
Validation...
	Batch 10/17 Loss:34.58641 con:0.00000 recon1:17.94235 recon2:16.64405
Validation Epoch 73/1000 Loss:38.14323 con:0.00000 recon1:18.27400 recon2:19.86923
Training...
